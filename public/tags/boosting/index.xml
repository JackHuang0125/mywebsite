<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Boosting on Yang&#39;s World</title>
    <link>http://localhost:1313/tags/boosting/</link>
    <description>Recent content in Boosting on Yang&#39;s World</description>
    <generator>Hugo -- 0.140.2</generator>
    <language>zh-TW</language>
    <lastBuildDate>Sun, 30 Mar 2025 08:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/boosting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>XGBoost Learning</title>
      <link>http://localhost:1313/posts/xgboost/</link>
      <pubDate>Sun, 30 Mar 2025 08:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/xgboost/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h1 id=&#34;-xgboost-boost&#34;&gt;ğŸ¦¹ XGBoost Boost&lt;/h1&gt;
&lt;h3 id=&#34;what-is-xgboost&#34;&gt;What is XGBoost?&lt;/h3&gt;
&lt;p&gt;Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-key-concepts-in-xgboost-tree-building&#34;&gt;ğŸ— Key Concepts in XGBoost Tree Building&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Start with an initial guess (e.g., average score).&lt;/li&gt;
&lt;li&gt;Measure how far off the prediction is from the real answer (this is called the &lt;strong&gt;residual&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;The next tree learns how to &lt;strong&gt;fix these errors&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Every new tree improves on the mistakes of the previous trees.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-how-to-divide-the-data-not-randomly&#34;&gt;ğŸ¥¢ How to Divide the Data (Not Randomly)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;XGBoost doesnâ€™t split data based on traditional methods like information gain.&lt;/li&gt;
&lt;li&gt;It uses a formula called &lt;strong&gt;Gain&lt;/strong&gt;, which measures how much a split improves prediction.&lt;/li&gt;
&lt;li&gt;A split only happens if:&lt;br&gt;
&lt;strong&gt;(Left + Right Score) &amp;gt; (Parent Score + Penalty)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-how-do-we-know-if-a-split-is-good&#34;&gt;â“ How do we know if a split is good?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Use a value called &lt;strong&gt;Similarity Score&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The higher the score, the more consistent (similar) the residuals are in that group&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-two-ways-to-find-splits-accurate--exact-greedy-algorithm&#34;&gt;ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Try &lt;strong&gt;all&lt;/strong&gt; possible features and split points&lt;/li&gt;
&lt;li&gt;Very accurate but &lt;strong&gt;very slow&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-two-ways-to-find-splits-fast--approximate-algorithm&#34;&gt;ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Uses &lt;strong&gt;feature quantiles&lt;/strong&gt; (e.g., median) to propose a few split points&lt;/li&gt;
&lt;li&gt;Group the data based on these splits and evaluate the best one&lt;/li&gt;
&lt;li&gt;Two options:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Global Proposal&lt;/strong&gt;: use global info to suggest splits&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local Proposal&lt;/strong&gt;: use local (node-specific) info&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-weighted-quantile-sketch&#34;&gt;ğŸ‹ Weighted Quantile Sketch&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Some data points are more important (like how teachers focus more on students who struggle)&lt;/li&gt;
&lt;li&gt;Each data point has a &lt;strong&gt;weight&lt;/strong&gt; based on how wrong it was (second-order gradient)&lt;/li&gt;
&lt;li&gt;Use these weights to suggest better and more meaningful split points&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-handling-missing-values&#34;&gt;ğŸ•³ Handling Missing Values&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What if some feature values are &lt;strong&gt;missing&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;XGBoost learns a &lt;strong&gt;default path&lt;/strong&gt; for missing data&lt;/li&gt;
&lt;li&gt;This makes the model more robust even when the data isnâ€™t complete&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-controlling-model-complexity-regularization&#34;&gt;ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Gamma (Î³)&lt;/p&gt;</description>
    </item>
    <item>
      <title>GradientBoost Learning</title>
      <link>http://localhost:1313/posts/gradientboost/</link>
      <pubDate>Thu, 20 Mar 2025 08:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/gradientboost/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h1 id=&#34;-gradient-boost&#34;&gt;ğŸ² Gradient Boost&lt;/h1&gt;</description>
    </item>
    <item>
      <title>AdaBoost Learning</title>
      <link>http://localhost:1313/posts/adaboost/</link>
      <pubDate>Fri, 07 Mar 2025 08:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/adaboost/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h3 id=&#34;adaboosting&#34;&gt;ğŸª´AdaBoosting&lt;/h3&gt;
&lt;p&gt;a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
