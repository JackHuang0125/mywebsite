<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Learninig Record on Yang&#39;s World</title>
    <link>http://localhost:1313/tags/learninig-record/</link>
    <description>Recent content in Learninig Record on Yang&#39;s World</description>
    <generator>Hugo -- 0.140.2</generator>
    <language>zh-TW</language>
    <lastBuildDate>Tue, 18 Mar 2025 16:20:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/learninig-record/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Decision &amp; Classification Tree Learning</title>
      <link>http://localhost:1313/posts/decisionclassificationtrees/</link>
      <pubDate>Tue, 18 Mar 2025 16:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/decisionclassificationtrees/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h3 id=&#34;-what-is-decision-tree&#34;&gt;ğŸ¤” What is decision tree?&lt;/h3&gt;
&lt;p&gt;Decision tree is a system that relies on evaluating conditions as &lt;em&gt;True&lt;/em&gt; or &lt;em&gt;False&lt;/em&gt; to make decisions, such as in classification or regression.&lt;br&gt;
When the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call
it a &lt;strong&gt;classification tree&lt;/strong&gt;; On the other hand, when the tree performs regression to predict a numerical value, we call it a &lt;strong&gt;regression tree&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression Learning</title>
      <link>http://localhost:1313/posts/logisticregression/</link>
      <pubDate>Tue, 11 Mar 2025 09:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/logisticregression/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;p&gt;Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,
maximum-entropy classification(MaxEnt) or the log-linear classifier.&lt;br&gt;
In this model, the probabilities from the outcome of predictions is using a logistic function.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;and-what-is-logistic-function&#34;&gt;And what is logistic function?&lt;/h3&gt;
&lt;h3 id=&#34;let-talk-about-it&#34;&gt;Let talk about it.&lt;/h3&gt;
&lt;p&gt;Here comes from &lt;strong&gt;Wikipedia&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A logistic function or a logistic curve is a commond S-shaped curve (&lt;strong&gt;sigmoid curve&lt;/strong&gt;) with the equation:
$$ f(x) = \frac{L}{1+e^{-k(x-x_o)}}$$
where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaBoost Learning</title>
      <link>http://localhost:1313/posts/adaboost/</link>
      <pubDate>Fri, 07 Mar 2025 08:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/adaboost/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;p&gt;AdaBoosting, a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RandomForest Learning</title>
      <link>http://localhost:1313/posts/randomforest/</link>
      <pubDate>Wed, 05 Mar 2025 11:30:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/randomforest/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;p&gt;éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦&lt;br&gt;
ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—&lt;br&gt;
æ–¹æ³•:&lt;br&gt;
å¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)&lt;br&gt;
å› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)&lt;br&gt;
ä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°&lt;br&gt;
è€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)&lt;br&gt;
é™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤&lt;br&gt;
éš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)&lt;br&gt;
é€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ&lt;br&gt;
å„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦&lt;br&gt;
æ¼”ç®—æ³•:
éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)&lt;br&gt;
æ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š&lt;br&gt;
ç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Principal Component Analysis(PCA) Learning</title>
      <link>http://localhost:1313/posts/pca/</link>
      <pubDate>Tue, 04 Feb 2025 13:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/pca/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h4 id=&#34;é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd&#34;&gt;é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD&lt;/h4&gt;
&lt;p&gt;We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically
follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:
$$
\left[
\lambda_\alpha \left(
1 - 1.96 \sqrt{\frac{2}{n-1}}
\right); \lambda_\alpha \left(1 + 1.96 \sqrt{\frac{2}{n-1}}
\right)
\right]
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda_\alpha$ represents the $\alpha$-th eigenvalue&lt;/li&gt;
&lt;li&gt;$n$ denotes the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By caculating the 95%
confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.
This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original
information as possible.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Text Mining with R: Chapter4</title>
      <link>http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/</link>
      <pubDate>Sun, 26 Jan 2025 13:45:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;tokenizing-by-n-gram-n-gram-åˆ†è©&#34;&gt;Tokenizing by n-gram (n-gram åˆ†è©)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š&lt;br&gt;
[1] &amp;ldquo;The Bank is a place where you put your money&amp;rdquo;&lt;br&gt;
[2] The Bee is an insect that gathers honey&amp;quot;&lt;br&gt;
æˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º&lt;br&gt;
[1] &amp;ldquo;the bank&amp;rdquo;ã€&amp;ldquo;bank is&amp;rdquo;ã€&amp;ldquo;is a&amp;rdquo;ã€&amp;ldquo;a place&amp;rdquo;ã€&amp;ldquo;place where&amp;rdquo;ã€&amp;ldquo;where you&amp;rdquo;ã€&amp;ldquo;you put&amp;rdquo;ã€&amp;ldquo;put your&amp;rdquo;ã€&amp;ldquo;your money&amp;rdquo;&lt;br&gt;
[2] &amp;ldquo;the bee&amp;rdquo;ã€&amp;ldquo;bee is&amp;rdquo;ã€&amp;ldquo;is an&amp;rdquo;ã€&amp;ldquo;an insect&amp;rdquo;ã€&amp;ldquo;insect that&amp;rdquo;ã€&amp;ldquo;that gathers&amp;rdquo;ã€&amp;ldquo;gathers honey&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Text Mining with R: Chapter3</title>
      <link>http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/</link>
      <pubDate>Fri, 24 Jan 2025 15:02:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;analyzing-word-and-document-frequency-tf-idf&#34;&gt;Analyzing word and document frequency: tf-idf&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Term Frequency(tf): How frequently a word occurs in a document&lt;br&gt;
è©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents&lt;br&gt;
é€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚
ä¸¦å®šç¾©ç‚ºï¼š
$$idf(term) = ln\left(\frac{n_{documents}}{n_{documents containing term}}\right)$$
å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ
è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚
å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;zipfs-law--é½Šå¤«å®šå¾‹&#34;&gt;Zipf&amp;rsquo;s law ( é½Šå¤«å®šå¾‹)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \propto \frac{1}{rank} $$&lt;/li&gt;
&lt;li&gt;å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€&amp;hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€&lt;/li&gt;
&lt;li&gt;è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf&amp;rsquo;s law&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Hirarchical bayesian modeling</title>
      <link>http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/</link>
      <pubDate>Mon, 13 Jan 2025 15:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€&lt;br&gt;
é‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;ã€ŒHierarchical modeling is used with information is available on &lt;strong&gt;several different levels&lt;/strong&gt; of observational &lt;strong&gt;units&lt;/strong&gt;ã€&lt;br&gt;
å¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š&lt;br&gt;
&amp;ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶&lt;br&gt;
&amp;ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ã€€&lt;strong&gt;å¼•è‡ªç¶­åŸºç™¾ç§‘&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;å…¬å¼ç†è«–&#34;&gt;å…¬å¼ç†è«–&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Bayes&amp;rsquo; theorem:&lt;br&gt;
the updated probability statements about $\theta_j$, given the occurence of event $y$,&lt;br&gt;
using the basic property of conditional probability, the posterior distribution will yield:
$$P(\theta \mid y) = \frac{P(\theta, y)}{P(y)} = \frac{P(y \mid \theta)P(\theta)}{P(y)}$$
so, we can say that:
$$P(\theta \mid y) \propto P(y \mid \theta)P(\theta)$$&lt;/li&gt;
&lt;li&gt;Hierarchical models:&lt;br&gt;
Bayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:&lt;br&gt;
(1) &lt;strong&gt;Hyperparameters&lt;/strong&gt;: parameters of prior distribution&lt;br&gt;
ã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰&lt;br&gt;
(2) &lt;strong&gt;Hyperdisrtibution&lt;/strong&gt;: distribution of hyperparameters&lt;br&gt;
ã€€ è¶…åƒæ•¸çš„åˆ†é…&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾&lt;/p&gt;</description>
    </item>
    <item>
      <title>Expectation maximization algorithm</title>
      <link>http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sun, 12 Jan 2025 11:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Expectation-maximization algorithm&lt;br&gt;
åˆç¿»è­¯ç‚ºã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€&lt;/li&gt;
&lt;li&gt;ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š&lt;br&gt;
ç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼&lt;br&gt;
ç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼
Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ&lt;br&gt;
&lt;strong&gt;å¼•è‡ªç¶­åŸºç™¾ç§‘&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
  </channel>
</rss>
