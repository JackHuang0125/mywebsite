<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Dimensionality Reduction on Yang&#39;s World</title>
    <link>http://localhost:1313/tags/dimensionality-reduction/</link>
    <description>Recent content in Dimensionality Reduction on Yang&#39;s World</description>
    <generator>Hugo -- 0.140.2</generator>
    <language>zh-TW</language>
    <lastBuildDate>Fri, 21 Mar 2025 16:40:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/dimensionality-reduction/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Support Vector Machine Learning</title>
      <link>http://localhost:1313/posts/suportvectormachine/</link>
      <pubDate>Fri, 21 Mar 2025 16:40:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/suportvectormachine/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;h3 id=&#34;-support-vector-machine&#34;&gt;🪡 Support Vector Machine&lt;/h3&gt;</description>
    </item>
    <item>
      <title>Principal Component Analysis(PCA) Learning</title>
      <link>http://localhost:1313/posts/pca/</link>
      <pubDate>Tue, 04 Feb 2025 13:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/pca/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;h4 id=&#34;這篇文章利用-chat-gpt-翻譯成英文邊唸邊打純手打無複製貼上順便練英文-xd&#34;&gt;這篇文章利用 Chat Gpt 翻譯成英文，邊唸邊打（純手打，無複製貼上），順便練英文 XD&lt;/h4&gt;
&lt;p&gt;We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically
follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:
$$
\left[
\lambda_\alpha \left(
1 - 1.96 \sqrt{\frac{2}{n-1}}
\right); \lambda_\alpha \left(1 + 1.96 \sqrt{\frac{2}{n-1}}
\right)
\right]
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda_\alpha$ represents the $\alpha$-th eigenvalue&lt;/li&gt;
&lt;li&gt;$n$ denotes the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By caculating the 95%
confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.
This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original
information as possible.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
