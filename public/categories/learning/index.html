<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Learning | Yang&#39;s World</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/categories/learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9de45e225101e4f99701d2b68fc6b8a1ef6027928be6391fa15bf7f56326c909.css" integrity="sha256-neReIlEB5PmXAdK2j8a4oe9gJ5KL5jkfoVv39WMmyQk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/categories/learning/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/categories/learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Yang&#39;s World (Alt + H)">Yang&#39;s World</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tw.stock.yahoo.com/" title="YahooStock">
                    <span>YahooStock</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header"><div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/categories/">Categories</a></div>
  <h1>
    Learning
  </h1>
</header>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/EM.jpg" alt="20250112">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">20250710 Meeting
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD
</p>
  </div>
  <footer class="entry-footer"><span title='2025-07-07 11:00:00 +0800 CST'>July 7, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 20250710 Meeting" href="http://localhost:1313/posts/20250710_meeting/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/EM.jpg" alt="20250112">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Expectation maximization algorithm
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD
Expectation-maximization algorithm -「最大期望值演算法」 經過兩個步驟交替進行計算：
第一步是計算期望值（E）：利用對隱藏變量的現有估計值，計算其最大概似估計值
第二步是最大化（M）：最大化在E步上求得的最大概似值來計算參數的值 M步上找到的參數估計值被用於下一個E步計算中，這個過程不斷交替進行
引自維基百科 Example from finalterm Assume that $Y_1, Y_2, …, Y_n ～ exp(\theta)$
Consider the MLE of $\theta$ based on $Y_1, Y_2, …, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4&gt;1.2$, $Z_5&gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-07-03 11:00:00 +0800 CST'>July 3, 2025</span>&nbsp;·&nbsp;2 min</footer>
  <a class="entry-link" aria-label="post link to Expectation maximization algorithm" href="http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/XGBoost.webp" alt="20250330">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">XGBoost Learning
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 🦹 XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.
🗝 Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. 🥢 How to Divide the Data (Not Randomly) XGBoost doesn’t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:
(Left &#43; Right Score) &gt; (Parent Score &#43; Penalty) ❓ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group 🐢 Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow 🐇 Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info 🏋 Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points 🕳 Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isn’t complete 🧚‍♀️ Controlling Model Complexity: Regularization Gamma (γ)
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-30 08:00:00 +0800 CST'>March 30, 2025</span>&nbsp;·&nbsp;2 min</footer>
  <a class="entry-link" aria-label="post link to XGBoost Learning" href="http://localhost:1313/posts/20250330_extremegradientboost/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/Bayes.webp" alt="20250322">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Naive &amp; Gaussian Bayes Learning
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 👶 Naive Bayes By definition of Bayes’ theorem $$ P(y \mid x_1, x_2, …, x_n) = \frac{P(y)P(x_1, x_2, …, x_n \mid y)}{P(x_1, x_2, …, x_n)} $$
where
$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, …, x_n \mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, …, x_n$ given class $y$ $P(x_1, x_2, …, x_n)$ represents the marginal probability of the feature set $x_1, x_2, …, x_n$ With the assumption of Naive Bayes - Conditional Independence
$$ P(x_i \mid y, x_1, …, x_{i-1}, x_{i&#43;1}, …, x_n) = P(x_i \mid y) $$
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-21 16:40:00 +0800 CST'>March 21, 2025</span>&nbsp;·&nbsp;3 min</footer>
  <a class="entry-link" aria-label="post link to Naive & Gaussian Bayes Learning" href="http://localhost:1313/posts/20250321_naivebayes/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/decision.webp" alt="20250318">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Decision &amp; Classification Tree Learning
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 🤔 What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.
When the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-18 16:20:00 +0800 CST'>March 18, 2025</span>&nbsp;·&nbsp;3 min</footer>
  <a class="entry-link" aria-label="post link to Decision & Classification Tree Learning" href="http://localhost:1313/posts/20250318_decisiontrees/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/LogisticRegression.webp" alt="20250311">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Logistic Regression Learning
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.
In this model, the probabilities from the outcome of predictions is using a logistic function.
And what is logistic function? Let talk about it. Here comes from Wikipedia:
A logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \frac{L}{1&#43;e^{-k(x-x_o)}}$$ where:
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-11 09:20:00 +0800 CST'>March 11, 2025</span>&nbsp;·&nbsp;5 min</footer>
  <a class="entry-link" aria-label="post link to Logistic Regression Learning" href="http://localhost:1313/posts/20250311_logisticregression/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/RF.png" alt="20250305">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">RandomForest Learning
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 🌳 隨機森林基本概要 由多棵決策樹聚集而成的森林
方法:
從原始資料中以取後放回的方式抽取資料，建立每棵決策樹的訓練資料(training datasets)
因此有些樣本會被重複選中，這樣的抽樣法又稱為Boostraping(拔靴法)
但是當原始資料的數據龐大時，會發現在抽樣完畢後，有些樣本並沒有被抽取到
而這些樣本就被稱為Out of Bag(OOB)資料(袋外)
除了上述訓練資料是被重複抽取之外，特徵也是如此
隨機森林並不會將所有特徵一起考慮，而是會隨機抽取特徵(可設定參數max_features)
進行每棵樹的訓練，以上述兩種方式來達到每棵樹近乎獨立的情況，目的是降低每棵樹之間的高度相關性，
優點是提高模型的泛化能力，防止過擬合(Overfitting)的情況發生、增進預測穩定性與準確度
演算法: 隨機森林的演算法與決策樹的演算法的核心概念是一樣的，差別只是在建立樹的方法不同而已(如上述)
意即當應用在分類問題時，則採用吉尼不純度(Gini Impurity)或是鏑(Entropy)演算法作為分類依據
當應用在迴歸問題時，通常採用最小平方誤差(MSE)(其他還有卜氏Possion)
</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-05 11:30:00 +0800 CST'>March 5, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to RandomForest Learning" href="http://localhost:1313/posts/20250305_randomforest/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/PCA.png" alt="20250204">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Principal Component Analysis(PCA) Learning
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 這篇文章利用 Chat Gpt 翻譯成英文，邊唸邊打（純手打，無複製貼上），順便練英文 XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \left[ \lambda_\alpha \left( 1 - 1.96 \sqrt{\frac{2}{n-1}} \right); \lambda_\alpha \left(1 &#43; 1.96 \sqrt{\frac{2}{n-1}} \right) \right] $$ where:
$\lambda_\alpha$ represents the $\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-02-04 13:20:00 +0800 CST'>February 4, 2025</span>&nbsp;·&nbsp;6 min</footer>
  <a class="entry-link" aria-label="post link to Principal Component Analysis(PCA) Learning" href="http://localhost:1313/posts/20250204_principalcomponentanalysis/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/wordcloud.png" alt="20250124">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Text Mining with R: Chapter4
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD
Tokenizing by n-gram (n-gram 分詞) 將文檔的內容依照 n 個單詞作分類，例如：
[1] “The Bank is a place where you put your money”
[2] The Bee is an insect that gathers honey&#34;
我們可以利用函式 tokenize_ngrams() 依照 n = 2 分類為
[1] “the bank”、“bank is”、“is a”、“a place”、“place where”、“where you”、“you put”、“put your”、“your money”
[2] “the bee”、“bee is”、“is an”、“an insect”、“insect that”、“that gathers”、“gathers honey” </p>
  </div>
  <footer class="entry-footer"><span title='2025-01-26 13:45:00 +0800 CST'>January 26, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to Text Mining with R: Chapter4" href="http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/wordcloud.png" alt="20250124">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Text Mining with R: Chapter3
    </h2>
  </header>
  <div class="entry-content">
    <p>這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD
Analyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document
詞頻: 單詞出現在文檔的頻率 Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents
逆文檔頻率: 文檔中出現愈多次的單詞，其權重愈低；反之則愈低。即衡量某詞在所有文檔中分佈的稀疏程度，是一種懲罰項 。 並定義為： $$idf(term) = ln\left(\frac{n_{documents}}{n_{documents containing term}}\right)$$ 其中分子$n_{documents}$是文檔總數，分母$n_{documents containing term}$是包含該詞的文檔總數， 若某單詞出現在文檔的次數少，表示分母小，其 IDF 大，重要性高，權重高；反之出現的次數多，表示分母大，其 IDF 小，重要性低，權重低。 取對數則是為了減少極端數值，使 IDF 分佈更加平滑。 tf-idf的目標是用於識別在單一文檔中有價值、但不常見的單詞（詞彙）
Zipf’s law ( 齊夫定律) 一種描述自然語言中單詞使用頻率分佈的統計規律，其宣稱單詞的頻率與其在文檔裡的頻率排名成反比，即：$$frequency \propto \frac{1}{rank} $$ 出現頻率第一名的單詞的次數會是出現頻率第二名的單詞的次數的兩倍，會是出現頻率第三名的單詞的次數的三倍…依此類推，會是出現頻率第 N 名的單詞的次數的 N 倍 若將排名 x 與出現頻率 y 取對數，即 logx 、 logy ，若整個文檔的坐標點標出來接近一直線，則該文檔符合 Zipf’s law </p>
  </div>
  <footer class="entry-footer"><span title='2025-01-24 15:02:00 +0800 CST'>January 24, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to Text Mining with R: Chapter3" href="http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="http://localhost:1313/categories/learning/page/2/">Next&nbsp;2/2&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Yang&#39;s World</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
