<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yang&#39;s World</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Yang&#39;s World</description>
    <generator>Hugo -- 0.140.2</generator>
    <language>zh-TW</language>
    <lastBuildDate>Sat, 22 Mar 2025 23:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Discriminant Analysis Learning</title>
      <link>http://localhost:1313/posts/lineardiscriminantanalysis/</link>
      <pubDate>Sat, 22 Mar 2025 23:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/lineardiscriminantanalysis/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;</description>
    </item>
    <item>
      <title>Naive &amp; Gaussian Bayes Learning</title>
      <link>http://localhost:1313/posts/naivebayes/</link>
      <pubDate>Fri, 21 Mar 2025 16:40:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/naivebayes/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h3 id=&#34;-naive-bayes&#34;&gt;ğŸ‘¶ Naive Bayes&lt;/h3&gt;
&lt;p&gt;By definition of Bayes&amp;rsquo; theorem
$$
P(y \mid x_1, x_2, &amp;hellip;, x_n) = \frac{P(y)P(x_1, x_2, &amp;hellip;, x_n \mid y)}{P(x_1, x_2, &amp;hellip;, x_n)}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(y)$ represents the prior probability of class $y$&lt;/li&gt;
&lt;li&gt;$P(x_1, x_2, &amp;hellip;, x_n \mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, &amp;hellip;, x_n$ given class $y$&lt;/li&gt;
&lt;li&gt;$P(x_1, x_2, &amp;hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, &amp;hellip;, x_n$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the assumption of Naive Bayes - &lt;strong&gt;Conditional Independence&lt;/strong&gt;&lt;br&gt;
$$
P(x_i \mid y, x_1, &amp;hellip;, x_{i-1}, x_{i+1}, &amp;hellip;, x_n) = P(x_i \mid y)
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>GradientBoost Learning</title>
      <link>http://localhost:1313/posts/gradientboost/</link>
      <pubDate>Thu, 20 Mar 2025 08:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/gradientboost/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h1 id=&#34;gradient-boost&#34;&gt;Gradient Boost&lt;/h1&gt;</description>
    </item>
    <item>
      <title>Decision &amp; Classification Tree Learning</title>
      <link>http://localhost:1313/posts/decisionclassificationtrees/</link>
      <pubDate>Tue, 18 Mar 2025 16:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/decisionclassificationtrees/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h3 id=&#34;-what-is-decision-tree&#34;&gt;ğŸ¤” What is decision tree?&lt;/h3&gt;
&lt;p&gt;Decision tree is a system that relies on evaluating conditions as &lt;em&gt;True&lt;/em&gt; or &lt;em&gt;False&lt;/em&gt; to make decisions, such as in classification or regression.&lt;br&gt;
When the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call
it a &lt;strong&gt;classification tree&lt;/strong&gt;; On the other hand, when the tree performs regression to predict a numerical value, we call it a &lt;strong&gt;regression tree&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical Computing HW_0320</title>
      <link>http://localhost:1313/posts/%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/</link>
      <pubDate>Tue, 18 Mar 2025 11:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/</guid>
      <description>&lt;h4 id=&#34;this-is-homework&#34;&gt;This is homework&lt;/h4&gt;
&lt;h1 id=&#34;1&#34;&gt;(1)&lt;/h1&gt;
&lt;p&gt;å·²çŸ¥ï¼š
$$X_1, X_2, &amp;hellip;, X_n \overset{\text{iid}}{\sim}p(x)$$&lt;/p&gt;
&lt;p&gt;è¨ˆç®—ï¼š&lt;/p&gt;
&lt;p&gt;$$ E( \hat{I}_M)=E\left[\frac{1}{n} \sum^n_{i=1} \frac{f(X_i)}{p(X_i)} \right]=\frac{1}{n}E\left[ \sum^n_{i=1} \frac{f(X_i)}{p(X_i)} \right] $$&lt;/p&gt;
&lt;p&gt;å°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š
$$E\left[\frac{f(X_i)}{p(X_i)} \right]$$&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼š
$$
E\left[\frac{f(X)}{p(X)} \right] = \int^b_a\frac{f(x)}{p(x)}p(x)dx =\int^b_af(x)dx = I
$$&lt;/p&gt;
&lt;p&gt;å¯çŸ¥
$$E\left[\frac{f(X_i)}{p(X_i)} \right] =I,ã€€\forall i
$$&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥
$$
E(\hat{I}_M) =\frac{1}{n}\sum^n_{i=1}I=I
$$&lt;/p&gt;
&lt;h1 id=&#34;2&#34;&gt;(2)&lt;/h1&gt;
&lt;p&gt;è¨ˆç®—è®Šç•°æ•¸
$$Var(\hat{I}_M)=E\left[(\hat{I}_M-I)^2\right]$$&lt;/p&gt;
&lt;p&gt;å› ç‚º
$$
\begin{aligned}
Var(\widehat{I}_M)
&amp;amp;= Var\left(\frac{1}{n} \sum_{i=1}^{n} \frac{f(X_i)}{p(X_i)}\right)
= \frac{1}{n}Var\left(\frac{f(X)}{p(X)}\right) \\
&amp;amp;= \frac{1}{n}\left(E\left[\left(\frac{f(X)}{p(X)}\right)^2\right]-I^2\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;å·²çŸ¥
$$E\left[\left(\frac{f(X)}{p(X)}\right)^2\right] &amp;lt; \infty$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression Learning</title>
      <link>http://localhost:1313/posts/logisticregression/</link>
      <pubDate>Tue, 11 Mar 2025 09:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/logisticregression/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;p&gt;Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,
maximum-entropy classification(MaxEnt) or the log-linear classifier.&lt;br&gt;
In this model, the probabilities from the outcome of predictions is using a logistic function.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;and-what-is-logistic-function&#34;&gt;And what is logistic function?&lt;/h3&gt;
&lt;h3 id=&#34;let-talk-about-it&#34;&gt;Let talk about it.&lt;/h3&gt;
&lt;p&gt;Here comes from &lt;strong&gt;Wikipedia&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A logistic function or a logistic curve is a commond S-shaped curve (&lt;strong&gt;sigmoid curve&lt;/strong&gt;) with the equation:
$$ f(x) = \frac{L}{1+e^{-k(x-x_o)}}$$
where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaBoost Learning</title>
      <link>http://localhost:1313/posts/adaboost/</link>
      <pubDate>Fri, 07 Mar 2025 08:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/adaboost/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;p&gt;AdaBoosting, a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RandomForest Learning</title>
      <link>http://localhost:1313/posts/randomforest/</link>
      <pubDate>Wed, 05 Mar 2025 11:30:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/randomforest/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;p&gt;éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦&lt;br&gt;
ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—&lt;br&gt;
æ–¹æ³•:&lt;br&gt;
å¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)&lt;br&gt;
å› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)&lt;br&gt;
ä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°&lt;br&gt;
è€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)&lt;br&gt;
é™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤&lt;br&gt;
éš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)&lt;br&gt;
é€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ&lt;br&gt;
å„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦&lt;br&gt;
æ¼”ç®—æ³•:
éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)&lt;br&gt;
æ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š&lt;br&gt;
ç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Principal Component Analysis(PCA) Learning</title>
      <link>http://localhost:1313/posts/pca/</link>
      <pubDate>Tue, 04 Feb 2025 13:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/pca/</guid>
      <description>&lt;h4 id=&#34;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd&#34;&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/h4&gt;
&lt;h4 id=&#34;é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd&#34;&gt;é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD&lt;/h4&gt;
&lt;p&gt;We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically
follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:
$$
\left[
\lambda_\alpha \left(
1 - 1.96 \sqrt{\frac{2}{n-1}}
\right); \lambda_\alpha \left(1 + 1.96 \sqrt{\frac{2}{n-1}}
\right)
\right]
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda_\alpha$ represents the $\alpha$-th eigenvalue&lt;/li&gt;
&lt;li&gt;$n$ denotes the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By caculating the 95%
confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.
This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original
information as possible.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Text Mining with R: Chapter4</title>
      <link>http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/</link>
      <pubDate>Sun, 26 Jan 2025 13:45:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;tokenizing-by-n-gram-n-gram-åˆ†è©&#34;&gt;Tokenizing by n-gram (n-gram åˆ†è©)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š&lt;br&gt;
[1] &amp;ldquo;The Bank is a place where you put your money&amp;rdquo;&lt;br&gt;
[2] The Bee is an insect that gathers honey&amp;quot;&lt;br&gt;
æˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º&lt;br&gt;
[1] &amp;ldquo;the bank&amp;rdquo;ã€&amp;ldquo;bank is&amp;rdquo;ã€&amp;ldquo;is a&amp;rdquo;ã€&amp;ldquo;a place&amp;rdquo;ã€&amp;ldquo;place where&amp;rdquo;ã€&amp;ldquo;where you&amp;rdquo;ã€&amp;ldquo;you put&amp;rdquo;ã€&amp;ldquo;put your&amp;rdquo;ã€&amp;ldquo;your money&amp;rdquo;&lt;br&gt;
[2] &amp;ldquo;the bee&amp;rdquo;ã€&amp;ldquo;bee is&amp;rdquo;ã€&amp;ldquo;is an&amp;rdquo;ã€&amp;ldquo;an insect&amp;rdquo;ã€&amp;ldquo;insect that&amp;rdquo;ã€&amp;ldquo;that gathers&amp;rdquo;ã€&amp;ldquo;gathers honey&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Text Mining with R: Chapter3</title>
      <link>http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/</link>
      <pubDate>Fri, 24 Jan 2025 15:02:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;analyzing-word-and-document-frequency-tf-idf&#34;&gt;Analyzing word and document frequency: tf-idf&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Term Frequency(tf): How frequently a word occurs in a document&lt;br&gt;
è©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents&lt;br&gt;
é€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚
ä¸¦å®šç¾©ç‚ºï¼š
$$idf(term) = ln\left(\frac{n_{documents}}{n_{documents containing term}}\right)$$
å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ
è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚
å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;zipfs-law--é½Šå¤«å®šå¾‹&#34;&gt;Zipf&amp;rsquo;s law ( é½Šå¤«å®šå¾‹)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \propto \frac{1}{rank} $$&lt;/li&gt;
&lt;li&gt;å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€&amp;hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€&lt;/li&gt;
&lt;li&gt;è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf&amp;rsquo;s law&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Hirarchical bayesian modeling</title>
      <link>http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/</link>
      <pubDate>Mon, 13 Jan 2025 15:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€&lt;br&gt;
é‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;ã€ŒHierarchical modeling is used with information is available on &lt;strong&gt;several different levels&lt;/strong&gt; of observational &lt;strong&gt;units&lt;/strong&gt;ã€&lt;br&gt;
å¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š&lt;br&gt;
&amp;ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶&lt;br&gt;
&amp;ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ã€€&lt;strong&gt;å¼•è‡ªç¶­åŸºç™¾ç§‘&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;å…¬å¼ç†è«–&#34;&gt;å…¬å¼ç†è«–&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Bayes&amp;rsquo; theorem:&lt;br&gt;
the updated probability statements about $\theta_j$, given the occurence of event $y$,&lt;br&gt;
using the basic property of conditional probability, the posterior distribution will yield:
$$P(\theta \mid y) = \frac{P(\theta, y)}{P(y)} = \frac{P(y \mid \theta)P(\theta)}{P(y)}$$
so, we can say that:
$$P(\theta \mid y) \propto P(y \mid \theta)P(\theta)$$&lt;/li&gt;
&lt;li&gt;Hierarchical models:&lt;br&gt;
Bayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:&lt;br&gt;
(1) &lt;strong&gt;Hyperparameters&lt;/strong&gt;: parameters of prior distribution&lt;br&gt;
ã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰&lt;br&gt;
(2) &lt;strong&gt;Hyperdisrtibution&lt;/strong&gt;: distribution of hyperparameters&lt;br&gt;
ã€€ è¶…åƒæ•¸çš„åˆ†é…&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾&lt;/p&gt;</description>
    </item>
    <item>
      <title>Expectation maximization algorithm</title>
      <link>http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sun, 12 Jan 2025 11:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Expectation-maximization algorithm&lt;br&gt;
åˆç¿»è­¯ç‚ºã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€&lt;/li&gt;
&lt;li&gt;ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š&lt;br&gt;
ç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼&lt;br&gt;
ç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼
Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ&lt;br&gt;
&lt;strong&gt;å¼•è‡ªç¶­åŸºç™¾ç§‘&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>cocktail webcrawler</title>
      <link>http://localhost:1313/posts/%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/</link>
      <pubDate>Fri, 10 Jan 2025 13:30:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/</guid>
      <description>&lt;p&gt;&lt;strong&gt;é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;æ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;æˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„&lt;/strong&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import requests as req
# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  
from bs4 import BeautifulSoup as B
# ç´¢å–ç¶²å€å…§å®¹  
import pandas as pd
# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”
import time
# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡
import random
# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„
builder_url = &amp;#39;https://www.theeducatedbarfly.com/cocktail-builder/&amp;#39;
# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  
header = {&amp;#39;User-Agent&amp;#39;: &amp;#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36&amp;#39;}
# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸
resp = req.get(builder_url, headers=header)
# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸
cocktail_name_list = []
# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe
if resp.status_code == 200:
# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„
    soup = B(resp.text, &amp;#39;html.parser&amp;#39;)
    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½
    for cocktail_name in soup.find_all(&amp;#39;div&amp;#39;, class_=&amp;#34;wpupg-item-title wpupg-block-text-bold&amp;#34;):
    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block
    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨&amp;#39;div&amp;#39;, class_=&amp;#34;wpupg-item-title wpupg-block-text-bold&amp;#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤
        name = cocktail_name.text.strip()
        # èª¿é…’åç¨±éƒ½åœ¨&amp;#39;div&amp;#39;, class_=&amp;#34;wpupg-item-title wpupg-block-text-bold&amp;#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼
        if name:
        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“
            cocktail_name_list.append(name)
            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list
    time.sleep(random.uniform(1,3))
    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’

cocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[&amp;#39;Name&amp;#39;])
# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±

cocktail_data = []
# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®

for name in cocktail_name_df[&amp;#39;Name&amp;#39;]:
    urls = f&amp;#39;https://www.theeducatedbarfly.com/{name.replace(&amp;#34; &amp;#34;, &amp;#34;-&amp;#34;).lower()}/&amp;#39;
    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/
    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«
    resp = req.get(urls, headers=header)
    if resp.status_code == 200:
        soup = B(resp.text, &amp;#39;html.parser&amp;#39;)
        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ &amp;lt;span&amp;gt; å…ƒç´ 
        ingredients = soup.find_all(&amp;#39;span&amp;#39;, class_=&amp;#39;wprm-recipe-ingredient-name&amp;#39;)
        ingredient_name_list = []
        for ingredient in ingredients:
        # æå–&amp;lt;a&amp;gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹
            ingredient_name = ingredient.find(&amp;#39;a&amp;#39;)
            if ingredient_name:  
            # ç¢ºä¿&amp;lt;a&amp;gt;å­˜åœ¨
                ingredient_name_list.append(ingredient_name.text.strip())

        cocktail_data.append({&amp;#39;Cocktail Name&amp;#39;: name, &amp;#39;Ingredients&amp;#39;: &amp;#34;, &amp;#34;.join(ingredient_name_list)})
        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­
    time.sleep(random.uniform(1,3))

cocktail_df = pd.DataFrame(cocktail_data)
# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;æœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º&lt;/strong&gt;&lt;br&gt;
&lt;del&gt;é›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†&lt;/del&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Least square estimator of Î² in linear regression</title>
      <link>http://localhost:1313/posts/lseofbeta/</link>
      <pubDate>Fri, 04 Oct 2024 16:02:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/lseofbeta/</guid>
      <description>&lt;h3 id=&#34;assume&#34;&gt;Assume&lt;/h3&gt;
&lt;p&gt;$$ Y_i = \beta_o + \beta_1X_i+\varepsilon_i $$
, for given $n$ observerd data $(x_i, Y_i)$, $\forall i=1ï½n$&lt;/p&gt;
&lt;p&gt;Note that:
$$ Y_i \mid X_i=x_i ~ N(\beta_o+\beta_1X_1, \sigma^2) $$&lt;br&gt;
$\therefore$
$$ E_{Y \mid X}[Y_i \mid X_i =x_i]=\beta_o+\beta_1X_1$$
In vector notation:
$$ Y_i = x^T_i\beta + \varepsilon_i $$
where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ x_i=(1, X_1, &amp;hellip;, X_n)^T $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And for $ Y = (Y_1, &amp;hellip;, Y_n)^T $, We have:
$$
Y = X\beta + \varepsilon
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>My 2nd post</title>
      <link>http://localhost:1313/posts/02/</link>
      <pubDate>Tue, 17 Sep 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/02/</guid>
      <description>&lt;p&gt;ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•&lt;/p&gt;
&lt;p&gt;åˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML&lt;/p&gt;
&lt;p&gt;å› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„&lt;/p&gt;</description>
    </item>
    <item>
      <title>My 1st post</title>
      <link>http://localhost:1313/posts/01/</link>
      <pubDate>Mon, 16 Sep 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/01/</guid>
      <description>&lt;p&gt;é–‹å­¸äº†!&lt;/p&gt;
&lt;h1 id=&#34;é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ&#34;&gt;é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ&lt;/h1&gt;
&lt;h2 id=&#34;é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ&#34;&gt;é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ&lt;/h2&gt;
&lt;h3 id=&#34;é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ&#34;&gt;é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ&lt;/h3&gt;
&lt;h4 id=&#34;é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ&#34;&gt;é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ&lt;/h4&gt;
&lt;h5 id=&#34;é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ&#34;&gt;é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ&lt;/h5&gt;
&lt;h6 id=&#34;é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°&#34;&gt;é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°&lt;/h6&gt;
&lt;p&gt;&lt;em&gt;é€™æ˜¯æ–œé«”å­—&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;é€™æ˜¯ç²—é«”å­—&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;é€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;é€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \alpha b \beta \Sigma \sigma $&lt;/p&gt;
&lt;p&gt;é€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \alpha b \beta \Sigma \sigma $$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;é€™æ˜¯ç·¨è™Ÿ1è™Ÿ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;é€™æ˜¯ç·¨è™Ÿ2è™Ÿ&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;é€™æ˜¯é …ç›®ç·¨è™Ÿ
&lt;ul&gt;
&lt;li&gt;é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ
&lt;ul&gt;
&lt;li&gt;é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ
&lt;ul&gt;
&lt;li&gt;æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ
&lt;ul&gt;
&lt;li&gt;çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ&lt;/th&gt;
          &lt;th&gt;é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ&lt;/td&gt;
          &lt;td&gt;é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ&lt;/td&gt;
          &lt;td&gt;é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/posts/suportvectormachine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/suportvectormachine/</guid>
      <description></description>
    </item>
  </channel>
</rss>
