<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yang&#39;s World</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Yang&#39;s World</description>
    <generator>Hugo -- 0.140.2</generator>
    <language>zh-TW</language>
    <lastBuildDate>Sat, 22 Mar 2025 23:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Discriminant Analysis Learning</title>
      <link>http://localhost:1313/posts/lineardiscriminantanalysis/</link>
      <pubDate>Sat, 22 Mar 2025 23:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/lineardiscriminantanalysis/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;</description>
    </item>
    <item>
      <title>Naive &amp; Gaussian Bayes Learning</title>
      <link>http://localhost:1313/posts/naivebayes/</link>
      <pubDate>Fri, 21 Mar 2025 16:40:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/naivebayes/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;h3 id=&#34;-naive-bayes&#34;&gt;👶 Naive Bayes&lt;/h3&gt;
&lt;p&gt;By definition of Bayes&amp;rsquo; theorem
$$
P(y \mid x_1, x_2, &amp;hellip;, x_n) = \frac{P(y)P(x_1, x_2, &amp;hellip;, x_n \mid y)}{P(x_1, x_2, &amp;hellip;, x_n)}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(y)$ represents the prior probability of class $y$&lt;/li&gt;
&lt;li&gt;$P(x_1, x_2, &amp;hellip;, x_n \mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, &amp;hellip;, x_n$ given class $y$&lt;/li&gt;
&lt;li&gt;$P(x_1, x_2, &amp;hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, &amp;hellip;, x_n$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the assumption of Naive Bayes - &lt;strong&gt;Conditional Independence&lt;/strong&gt;&lt;br&gt;
$$
P(x_i \mid y, x_1, &amp;hellip;, x_{i-1}, x_{i+1}, &amp;hellip;, x_n) = P(x_i \mid y)
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>GradientBoost Learning</title>
      <link>http://localhost:1313/posts/gradientboost/</link>
      <pubDate>Thu, 20 Mar 2025 08:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/gradientboost/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;h1 id=&#34;gradient-boost&#34;&gt;Gradient Boost&lt;/h1&gt;</description>
    </item>
    <item>
      <title>Decision &amp; Classification Tree Learning</title>
      <link>http://localhost:1313/posts/decisionclassificationtrees/</link>
      <pubDate>Tue, 18 Mar 2025 16:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/decisionclassificationtrees/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;h3 id=&#34;-what-is-decision-tree&#34;&gt;🤔 What is decision tree?&lt;/h3&gt;
&lt;p&gt;Decision tree is a system that relies on evaluating conditions as &lt;em&gt;True&lt;/em&gt; or &lt;em&gt;False&lt;/em&gt; to make decisions, such as in classification or regression.&lt;br&gt;
When the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call
it a &lt;strong&gt;classification tree&lt;/strong&gt;; On the other hand, when the tree performs regression to predict a numerical value, we call it a &lt;strong&gt;regression tree&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical Computing HW_0320</title>
      <link>http://localhost:1313/posts/%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/</link>
      <pubDate>Tue, 18 Mar 2025 11:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/</guid>
      <description>&lt;h4 id=&#34;this-is-homework&#34;&gt;This is homework&lt;/h4&gt;
&lt;h1 id=&#34;1&#34;&gt;(1)&lt;/h1&gt;
&lt;p&gt;已知：
$$X_1, X_2, &amp;hellip;, X_n \overset{\text{iid}}{\sim}p(x)$$&lt;/p&gt;
&lt;p&gt;計算：&lt;/p&gt;
&lt;p&gt;$$ E( \hat{I}_M)=E\left[\frac{1}{n} \sum^n_{i=1} \frac{f(X_i)}{p(X_i)} \right]=\frac{1}{n}E\left[ \sum^n_{i=1} \frac{f(X_i)}{p(X_i)} \right] $$&lt;/p&gt;
&lt;p&gt;對於每個獨立的 $X_i$ ，我們只要計算：
$$E\left[\frac{f(X_i)}{p(X_i)} \right]$$&lt;/p&gt;
&lt;p&gt;因此：
$$
E\left[\frac{f(X)}{p(X)} \right] = \int^b_a\frac{f(x)}{p(x)}p(x)dx =\int^b_af(x)dx = I
$$&lt;/p&gt;
&lt;p&gt;可知
$$E\left[\frac{f(X_i)}{p(X_i)} \right] =I,　\forall i
$$&lt;/p&gt;
&lt;p&gt;所以
$$
E(\hat{I}_M) =\frac{1}{n}\sum^n_{i=1}I=I
$$&lt;/p&gt;
&lt;h1 id=&#34;2&#34;&gt;(2)&lt;/h1&gt;
&lt;p&gt;計算變異數
$$Var(\hat{I}_M)=E\left[(\hat{I}_M-I)^2\right]$$&lt;/p&gt;
&lt;p&gt;因為
$$
\begin{aligned}
Var(\widehat{I}_M)
&amp;amp;= Var\left(\frac{1}{n} \sum_{i=1}^{n} \frac{f(X_i)}{p(X_i)}\right)
= \frac{1}{n}Var\left(\frac{f(X)}{p(X)}\right) \\
&amp;amp;= \frac{1}{n}\left(E\left[\left(\frac{f(X)}{p(X)}\right)^2\right]-I^2\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;已知
$$E\left[\left(\frac{f(X)}{p(X)}\right)^2\right] &amp;lt; \infty$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression Learning</title>
      <link>http://localhost:1313/posts/logisticregression/</link>
      <pubDate>Tue, 11 Mar 2025 09:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/logisticregression/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;p&gt;Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,
maximum-entropy classification(MaxEnt) or the log-linear classifier.&lt;br&gt;
In this model, the probabilities from the outcome of predictions is using a logistic function.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;and-what-is-logistic-function&#34;&gt;And what is logistic function?&lt;/h3&gt;
&lt;h3 id=&#34;let-talk-about-it&#34;&gt;Let talk about it.&lt;/h3&gt;
&lt;p&gt;Here comes from &lt;strong&gt;Wikipedia&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A logistic function or a logistic curve is a commond S-shaped curve (&lt;strong&gt;sigmoid curve&lt;/strong&gt;) with the equation:
$$ f(x) = \frac{L}{1+e^{-k(x-x_o)}}$$
where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaBoost Learning</title>
      <link>http://localhost:1313/posts/adaboost/</link>
      <pubDate>Fri, 07 Mar 2025 08:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/adaboost/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;p&gt;AdaBoosting, a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RandomForest Learning</title>
      <link>http://localhost:1313/posts/randomforest/</link>
      <pubDate>Wed, 05 Mar 2025 11:30:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/randomforest/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;p&gt;隨機森林基本概要&lt;br&gt;
由多棵決策樹聚集而成的森林&lt;br&gt;
方法:&lt;br&gt;
從原始資料中以取後放回的方式抽取資料，建立每棵決策樹的訓練資料(training datasets)&lt;br&gt;
因此有些樣本會被重複選中，這樣的抽樣法又稱為Boostraping(拔靴法)&lt;br&gt;
但是當原始資料的數據龐大時，會發現在抽樣完畢後，有些樣本並沒有被抽取到&lt;br&gt;
而這些樣本就被稱為Out of Bag(OOB)資料(袋外)&lt;br&gt;
除了上述訓練資料是被重複抽取之外，特徵也是如此&lt;br&gt;
隨機森林並不會將所有特徵一起考慮，而是會隨機抽取特徵(可設定參數max_features)&lt;br&gt;
進行每棵樹的訓練，以上述兩種方式來達到每棵樹近乎獨立的情況，目的是降低每棵樹之間的高度相關性，&lt;br&gt;
優點是提高模型的泛化能力，防止過擬合(Overfitting)的情況發生、增進預測穩定性與準確度&lt;br&gt;
演算法:
隨機森林的演算法與決策樹的演算法的核心概念是一樣的，差別只是在建立樹的方法不同而已(如上述)&lt;br&gt;
意即當應用在分類問題時，則採用吉尼不純度(Gini Impurity)或是鏑(Entropy)演算法作為分類依據&lt;br&gt;
當應用在迴歸問題時，通常採用最小平方誤差(MSE)(其他還有卜氏Possion)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Principal Component Analysis(PCA) Learning</title>
      <link>http://localhost:1313/posts/pca/</link>
      <pubDate>Tue, 04 Feb 2025 13:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/pca/</guid>
      <description>&lt;h4 id=&#34;這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd&#34;&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/h4&gt;
&lt;h4 id=&#34;這篇文章利用-chat-gpt-翻譯成英文邊唸邊打純手打無複製貼上順便練英文-xd&#34;&gt;這篇文章利用 Chat Gpt 翻譯成英文，邊唸邊打（純手打，無複製貼上），順便練英文 XD&lt;/h4&gt;
&lt;p&gt;We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically
follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:
$$
\left[
\lambda_\alpha \left(
1 - 1.96 \sqrt{\frac{2}{n-1}}
\right); \lambda_\alpha \left(1 + 1.96 \sqrt{\frac{2}{n-1}}
\right)
\right]
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda_\alpha$ represents the $\alpha$-th eigenvalue&lt;/li&gt;
&lt;li&gt;$n$ denotes the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By caculating the 95%
confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.
This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original
information as possible.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Text Mining with R: Chapter4</title>
      <link>http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/</link>
      <pubDate>Sun, 26 Jan 2025 13:45:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/</guid>
      <description>&lt;p&gt;&lt;strong&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;tokenizing-by-n-gram-n-gram-分詞&#34;&gt;Tokenizing by n-gram (n-gram 分詞)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;將文檔的內容依照 n 個單詞作分類，例如：&lt;br&gt;
[1] &amp;ldquo;The Bank is a place where you put your money&amp;rdquo;&lt;br&gt;
[2] The Bee is an insect that gathers honey&amp;quot;&lt;br&gt;
我們可以利用函式 tokenize_ngrams() 依照 n = 2 分類為&lt;br&gt;
[1] &amp;ldquo;the bank&amp;rdquo;、&amp;ldquo;bank is&amp;rdquo;、&amp;ldquo;is a&amp;rdquo;、&amp;ldquo;a place&amp;rdquo;、&amp;ldquo;place where&amp;rdquo;、&amp;ldquo;where you&amp;rdquo;、&amp;ldquo;you put&amp;rdquo;、&amp;ldquo;put your&amp;rdquo;、&amp;ldquo;your money&amp;rdquo;&lt;br&gt;
[2] &amp;ldquo;the bee&amp;rdquo;、&amp;ldquo;bee is&amp;rdquo;、&amp;ldquo;is an&amp;rdquo;、&amp;ldquo;an insect&amp;rdquo;、&amp;ldquo;insect that&amp;rdquo;、&amp;ldquo;that gathers&amp;rdquo;、&amp;ldquo;gathers honey&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Text Mining with R: Chapter3</title>
      <link>http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/</link>
      <pubDate>Fri, 24 Jan 2025 15:02:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/</guid>
      <description>&lt;p&gt;&lt;strong&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;analyzing-word-and-document-frequency-tf-idf&#34;&gt;Analyzing word and document frequency: tf-idf&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Term Frequency(tf): How frequently a word occurs in a document&lt;br&gt;
詞頻: 單詞出現在文檔的頻率&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents&lt;br&gt;
逆文檔頻率: 文檔中出現愈多次的單詞，其權重愈低；反之則愈低。即衡量某詞在所有文檔中分佈的稀疏程度，是一種懲罰項 。
並定義為：
$$idf(term) = ln\left(\frac{n_{documents}}{n_{documents containing term}}\right)$$
其中分子$n_{documents}$是文檔總數，分母$n_{documents containing term}$是包含該詞的文檔總數，
若某單詞出現在文檔的次數少，表示分母小，其 IDF 大，重要性高，權重高；反之出現的次數多，表示分母大，其 IDF 小，重要性低，權重低。
取對數則是為了減少極端數值，使 IDF 分佈更加平滑。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;tf-idf的目標是用於識別在單一文檔中有價值、但不常見的單詞（詞彙）&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;zipfs-law--齊夫定律&#34;&gt;Zipf&amp;rsquo;s law ( 齊夫定律)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;一種描述自然語言中單詞使用頻率分佈的統計規律，其宣稱單詞的頻率與其在文檔裡的頻率排名成反比，即：$$frequency \propto \frac{1}{rank} $$&lt;/li&gt;
&lt;li&gt;出現頻率第一名的單詞的次數會是出現頻率第二名的單詞的次數的兩倍，會是出現頻率第三名的單詞的次數的三倍&amp;hellip;依此類推，會是出現頻率第 N 名的單詞的次數的 N 倍&lt;/li&gt;
&lt;li&gt;若將排名 x 與出現頻率 y 取對數，即 logx 、 logy ，若整個文檔的坐標點標出來接近一直線，則該文檔符合 Zipf&amp;rsquo;s law&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Hirarchical bayesian modeling</title>
      <link>http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/</link>
      <pubDate>Mon, 13 Jan 2025 15:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/</guid>
      <description>&lt;p&gt;&lt;strong&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian hierarchical modeling，譯為「貝氏分層建模」&lt;br&gt;
針對多個層級分析的一套統計方法&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;「Hierarchical modeling is used with information is available on &lt;strong&gt;several different levels&lt;/strong&gt; of observational &lt;strong&gt;units&lt;/strong&gt;」&lt;br&gt;
可以對多個不同層級的觀測單位的資訊進行分析，例如：&lt;br&gt;
&amp;ndash; 每個國家的每日感染病例的時間概況，單位是國家&lt;br&gt;
&amp;ndash; 多個油井產量的遞減曲線分析（油氣產量），單位是油藏區的油井&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;　&lt;strong&gt;引自維基百科&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;公式理論&#34;&gt;公式理論&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Bayes&amp;rsquo; theorem:&lt;br&gt;
the updated probability statements about $\theta_j$, given the occurence of event $y$,&lt;br&gt;
using the basic property of conditional probability, the posterior distribution will yield:
$$P(\theta \mid y) = \frac{P(\theta, y)}{P(y)} = \frac{P(y \mid \theta)P(\theta)}{P(y)}$$
so, we can say that:
$$P(\theta \mid y) \propto P(y \mid \theta)P(\theta)$$&lt;/li&gt;
&lt;li&gt;Hierarchical models:&lt;br&gt;
Bayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:&lt;br&gt;
(1) &lt;strong&gt;Hyperparameters&lt;/strong&gt;: parameters of prior distribution&lt;br&gt;
　 先驗分配的參數（超參數／超母數）&lt;br&gt;
(2) &lt;strong&gt;Hyperdisrtibution&lt;/strong&gt;: distribution of hyperparameters&lt;br&gt;
　 超參數的分配&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如：我們需要建模學校的學生測驗成績&lt;/p&gt;</description>
    </item>
    <item>
      <title>Expectation maximization algorithm</title>
      <link>http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sun, 12 Jan 2025 11:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;&lt;strong&gt;這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Expectation-maximization algorithm&lt;br&gt;
又翻譯為「最大期望值演算法」&lt;/li&gt;
&lt;li&gt;經過兩個步驟交替進行計算：&lt;br&gt;
第一步是計算期望值（E）：利用對隱藏變量的現有估計值，計算其最大概似估計值&lt;br&gt;
第二步是最大化（M）：最大化在E步上求得的最大概似值來計算參數的值
M步上找到的參數估計值被用於下一個E步計算中，這個過程不斷交替進行&lt;br&gt;
&lt;strong&gt;引自維基百科&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>cocktail webcrawler</title>
      <link>http://localhost:1313/posts/%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/</link>
      <pubDate>Fri, 10 Jan 2025 13:30:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/</guid>
      <description>&lt;p&gt;&lt;strong&gt;這是給我自己的一份教學，以免日子久了忘記怎麼爬蟲，同時也是一篇學習紀錄&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;擁有一個可以寫code的環境，網路上很多安裝環境的教學&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;我是用anoconda的vs code寫code的&lt;/strong&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import requests as req
# 向客戶端要求網址  
from bs4 import BeautifulSoup as B
# 索取網址內容  
import pandas as pd
# 最後將結果輸出為CSV檔
import time
# 避免爬蟲時被抓到是爬蟲，所以移用這個延長每次爬蟲時間，假裝自己是人類
import random
# 延遲隨機時間用的
builder_url = &amp;#39;https://www.theeducatedbarfly.com/cocktail-builder/&amp;#39;
# 我是用 **cocktail builder** 這個網站來爬蟲我要的酒單  
header = {&amp;#39;User-Agent&amp;#39;: &amp;#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36&amp;#39;}
# 起初在爬的時候有發現跑不出資料，所以新增header來假裝我是人類，網路上也有很多如何在瀏覽器找header的教學
resp = req.get(builder_url, headers=header)
# 建立抓取url的回應變數
cocktail_name_list = []
# 建立空清單，將抓取到的資料先放進來，以便最後做成dataframe
if resp.status_code == 200:
# 確定你的url是可以連線的
    soup = B(resp.text, &amp;#39;html.parser&amp;#39;)
    # 建立爬蟲的頭頭，後面的html.parser是每次建立都要有，好像是用來解析html的功能
    for cocktail_name in soup.find_all(&amp;#39;div&amp;#39;, class_=&amp;#34;wpupg-item-title wpupg-block-text-bold&amp;#34;):
    # 打開網頁按下F2，按下ctrl+shift+c進入選取網頁元素模式，點選任一調酒，會指引到該調酒的block
    # 你會發現所有的調酒名稱都在&amp;#39;div&amp;#39;, class_=&amp;#34;wpupg-item-title wpupg-block-text-bold&amp;#34;這個標籤底下，所以我們利用find_all遍歷該標籤
        name = cocktail_name.text.strip()
        # 調酒名稱都在&amp;#39;div&amp;#39;, class_=&amp;#34;wpupg-item-title wpupg-block-text-bold&amp;#34;的標籤的text內容中，strip()是去掉text前後多餘的空格
        if name:
        # 確定該標籤有內容才抓，沒有就不抓
            cocktail_name_list.append(name)
            # 抓到修飾後的text並放入剛剛建立的list
    time.sleep(random.uniform(1,3))
    # 每次抓完一個就等待 1~3 秒

cocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[&amp;#39;Name&amp;#39;])
# 將抓完後的清list建立成一個Dataframe，以Name當作該欄位的名稱

cocktail_data = []
# 這是最後的調酒名稱+該調酒的材料的空清單

for name in cocktail_name_df[&amp;#39;Name&amp;#39;]:
    urls = f&amp;#39;https://www.theeducatedbarfly.com/{name.replace(&amp;#34; &amp;#34;, &amp;#34;-&amp;#34;).lower()}/&amp;#39;
    # 建立每個調酒的url，點進去隨便一個調酒，你會發現網址是https://www.theeducatedbarfly.com/xxx-xxx/
    # xxx是該調酒的名稱，只是我們剛剛的調酒名稱list有大寫而且中間是空格不是-，所以用replace將空格替換成-，且轉為小寫
    resp = req.get(urls, headers=header)
    if resp.status_code == 200:
        soup = B(resp.text, &amp;#39;html.parser&amp;#39;)
        # 查找所有具有指定 class 的 &amp;lt;span&amp;gt; 元素
        ingredients = soup.find_all(&amp;#39;span&amp;#39;, class_=&amp;#39;wprm-recipe-ingredient-name&amp;#39;)
        ingredient_name_list = []
        for ingredient in ingredients:
        # 提取&amp;lt;a&amp;gt;標籤的文字內容
            ingredient_name = ingredient.find(&amp;#39;a&amp;#39;)
            if ingredient_name:  
            # 確保&amp;lt;a&amp;gt;存在
                ingredient_name_list.append(ingredient_name.text.strip())

        cocktail_data.append({&amp;#39;Cocktail Name&amp;#39;: name, &amp;#39;Ingredients&amp;#39;: &amp;#34;, &amp;#34;.join(ingredient_name_list)})
        # 最後就是將剛剛抓到的Name跟這個Inredients清單中每個素材加入剛剛建立的加入剛剛建立的cocktail_data清單中
    time.sleep(random.uniform(1,3))

cocktail_df = pd.DataFrame(cocktail_data)
# 最後的最後將list轉為Dataframe並用pd.to_csv輸出成csv檔，結束這回合
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;以上是我提醒自己的爬蟲教學&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;有任何疑問歡迎提出&lt;/strong&gt;&lt;br&gt;
&lt;del&gt;雖然我還沒有建立留言板就是了&lt;/del&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Least square estimator of β in linear regression</title>
      <link>http://localhost:1313/posts/lseofbeta/</link>
      <pubDate>Fri, 04 Oct 2024 16:02:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/lseofbeta/</guid>
      <description>&lt;h3 id=&#34;assume&#34;&gt;Assume&lt;/h3&gt;
&lt;p&gt;$$ Y_i = \beta_o + \beta_1X_i+\varepsilon_i $$
, for given $n$ observerd data $(x_i, Y_i)$, $\forall i=1～n$&lt;/p&gt;
&lt;p&gt;Note that:
$$ Y_i \mid X_i=x_i ~ N(\beta_o+\beta_1X_1, \sigma^2) $$&lt;br&gt;
$\therefore$
$$ E_{Y \mid X}[Y_i \mid X_i =x_i]=\beta_o+\beta_1X_1$$
In vector notation:
$$ Y_i = x^T_i\beta + \varepsilon_i $$
where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ x_i=(1, X_1, &amp;hellip;, X_n)^T $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And for $ Y = (Y_1, &amp;hellip;, Y_n)^T $, We have:
$$
Y = X\beta + \varepsilon
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>My 2nd post</title>
      <link>http://localhost:1313/posts/02/</link>
      <pubDate>Tue, 17 Sep 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/02/</guid>
      <description>&lt;p&gt;瞭解到HUGO有三種語法&lt;/p&gt;
&lt;p&gt;分別是：JSON、TOML、YAML&lt;/p&gt;
&lt;p&gt;因為TOML的內容格式類似PYTHON的，而我本人也比較習慣PYTHON的語法&lt;/p&gt;
&lt;p&gt;所以採用TOML的語法，並將全部的語法改為跟TOML的&lt;/p&gt;</description>
    </item>
    <item>
      <title>My 1st post</title>
      <link>http://localhost:1313/posts/01/</link>
      <pubDate>Mon, 16 Sep 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/01/</guid>
      <description>&lt;p&gt;開學了!&lt;/p&gt;
&lt;h1 id=&#34;這是我的第一行&#34;&gt;這是我的第一行&lt;/h1&gt;
&lt;h2 id=&#34;這是我的第二行&#34;&gt;這是我的第二行&lt;/h2&gt;
&lt;h3 id=&#34;這是我的第三行&#34;&gt;這是我的第三行&lt;/h3&gt;
&lt;h4 id=&#34;這是我的第四行&#34;&gt;這是我的第四行&lt;/h4&gt;
&lt;h5 id=&#34;這是我的第五行&#34;&gt;這是我的第五行&lt;/h5&gt;
&lt;h6 id=&#34;這個文字大小最多到第六行這樣的大小&#34;&gt;這個文字大小最多到第六行這樣的大小&lt;/h6&gt;
&lt;p&gt;&lt;em&gt;這是斜體字&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這是粗體字&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;這是斜體中的粗體&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;這是不分行的數學語法 $a \alpha b \beta \Sigma \sigma $&lt;/p&gt;
&lt;p&gt;這是分行的數學語法 $$a \alpha b \beta \Sigma \sigma $$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;這是編號1號&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;這是編號2號&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;這是項目編號
&lt;ul&gt;
&lt;li&gt;這是第一次縮排的項目編號
&lt;ul&gt;
&lt;li&gt;這是第二次縮牌的項目編號
&lt;ul&gt;
&lt;li&gt;我不知道還有沒有第三次縮排的項目編號
&lt;ul&gt;
&lt;li&gt;看來第二次縮排以後都是一樣的項目編號&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;這是第一列第一行&lt;/th&gt;
          &lt;th&gt;這是第一列第二行&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;這是第二列第一行&lt;/td&gt;
          &lt;td&gt;這是第二列第二行&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;這是第三列第一行&lt;/td&gt;
          &lt;td&gt;這是第三列第二行&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/posts/suportvectormachine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/suportvectormachine/</guid>
      <description></description>
    </item>
  </channel>
</rss>
