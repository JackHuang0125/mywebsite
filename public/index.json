[{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/decisionclassificationtrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD AdaBoosting, a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.\n","permalink":"http://localhost:1313/posts/adaboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eAdaBoosting, a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.\u003c/p\u003e","title":"AdaBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\nç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eéš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003cbr\u003e\nç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/pca/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm\nåˆç¿»è­¯ç‚ºã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ ","permalink":"http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm\u003cbr\u003e\nåˆç¿»è­¯ç‚ºã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/lseofbeta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/02/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/01/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"","permalink":"http://localhost:1313/posts/gradientboost/","summary":"","title":""},{"content":"","permalink":"http://localhost:1313/posts/suportvectormachine/","summary":"","title":""}]