[{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸âƒ£ GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸âƒ£ è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ âœ… 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰ å°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ \\text{Final_Similarity} = \\lambda \\cdot \\text{GAP_Col_Prox} + (1 - \\lambda) \\cdot \\text{Cosine_Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\nâœ… 3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹ ä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nâœ… é€™æ¨£åšçš„è²¢ç»ï¼š ä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\nå¦‚æœä½ è¦ï¼Œæˆ‘å¯ä»¥å¹«ä½ æ•´ç†æˆä¸€å€‹å®Œæ•´çš„ç ”ç©¶æ¶æ§‹åœ–ï¼Œæˆ–å¹«ä½ æ’°å¯«ä¸€æ®µè«–æ–‡ã€Œæ”¹è‰¯æ–¹æ³•ç« ç¯€ã€çš„è‰ç¨¿ã€‚è¦å—ï¼Ÿ\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸âƒ£ GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"-2-å»ºç«‹-æ··åˆå‹ç›¸ä¼¼çŸ©é™£å…±ç¾--è©æ„\"\u003eâœ… 2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/h2\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Final_Similarity} = \\lambda \\cdot \\text{GAP_Col_Prox} + (1 - \\lambda) \\cdot \\text{Cosine_Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"Generalized Association Plots(GAP) é‡å°é«˜ç¶­åº¦çš„è³‡æ–™é€²è¡Œè¦–è¦ºåŒ–èˆ‡æ’åºçš„å·¥å…·\nèµ·å§‹çŸ©é™£ D: è¼¸å…¥ä»»æ„ä¸€å€‹ p by p çš„ proximity matrix ï¼ˆå¦‚çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼‰ éè¿´ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼š $R^{(1)}=\\phi(D)$ $R^{(n+1)}=\\phi(R^{(n)})$ æ”¶æ–‚ï¼š $R(n)$ æœƒæ”¶æ–‚è‡³ +1 å’Œ -1 çš„çŸ©é™£ $R^{(\\infty)}$ æ”¶æ–‚çš„éç¨‹ç•¶ä¸­æœƒè§€å¯Ÿåˆ°rankçš„é™ç¶­å’Œæ©¢åœ“çµæ§‹çš„è®ŠåŒ–ï¼ˆç”±æ©¢åœ“å£“ç¸®è‡³ä¸€ç›´ç·šï¼‰, å°æ–¼æ’åºï¼ˆseriationï¼‰å’Œåˆ†ç¾¤ï¼ˆclusteringï¼‰å¾ˆæœ‰ç”¨ æ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æï¼š è‹¥å°æ–°èæ–‡æœ¬é€²è¡Œä¸»é¡Œå»ºæ¨¡åˆ†æï¼Œèƒ½é€éå»ºç«‹æ–‡ä»¶ $\\rightarrow$ Topic æˆ– Topic-Keyword çš„è¿‘ä¼¼çŸ©é™£ï¼Œè—‰ç”± GAP çš„æ’åºèˆ‡åˆ†ç¾¤ï¼Œæœ‰åŠ©æ–¼Topic æˆ– Document ä¹‹é–“çµæ§‹çš„è¦–è¦ºåŒ–èˆ‡ç†è§£\nProcedure æ­¥é©Ÿ å…§å®¹ ç›®çš„ 1 ç”¨ BBC News 5 é¡åˆ¥è³‡æ–™ï¼ˆèªæ–™åº«ï¼‰ ä½œç‚ºç¯„ä¾‹èªæ–™ 2 æ¯é¡å– å‰ 15 å€‹ä»£è¡¨ token å»ºç«‹å°å‹ token å­é›†ï¼ˆ75 tokensï¼‰ 3 ç”¨ GAP æ’åºè¦–è¦ºåŒ– 75Ã—75 çŸ©é™£ æ‰¾åˆ°èªæ„ç¾¤èšçµæ§‹ 4 äººå·¥ç¢ºèªåˆ† 5 ç¾¤ å°‡ token-to-topic å®šç¾©å¥½ 5 ç”¨ token ç¾¤å»º LDA çš„ Î· å…ˆé©— å°å¼• LDA å­¸å‡ºé æœŸä¸»é¡Œ Question å•é¡Œ ä½ çš„å›ç­” GAP çµæœä¸Ÿé€² LDA äº†å—ï¼Ÿ æ˜¯ï¼Œé€é Î· çŸ©é™£é–“æ¥å°å…¥ï¼Œä¸æ˜¯ç›´æ¥ä¸Ÿ category Î· æ€éº¼è¨­å®šï¼Ÿ GAP ç¾¤å…§ Î· å¤§ï¼ˆå¦‚ 0.3ï¼‰ï¼Œå…¶ä»–å°ï¼ˆå¦‚ 0.01ï¼‰ GAP correlation å€¼æœ‰ç”¨ä¾†ç•¶æ©Ÿç‡å—ï¼Ÿ æ²’æœ‰ï¼Œåªæœ‰åˆ†ç¾¤çµæœç”¨ä¾†æŒ‡å®š topic GAP æ˜¯å¦ç›´æ¥å½±éŸ¿æ–‡ä»¶åˆ†é¡ï¼Ÿ æ²’æœ‰ï¼Œåªå½±éŸ¿ LDA ä¸»é¡Œ-è©å…ˆé©— æ˜¯å¦åš baseline LDA æ¯”è¼ƒï¼Ÿ å°šæœªï¼Œä¹‹å¾Œæœƒè£œåšå°æ¯” baseline Advice GAP â†’ å¹«åŠ© Î· çµæ§‹è¨­è¨ˆ baseline LDA vs GAP-informed LDA çµæœå°æ¯” LDA åŸç†æœ¬ä¾†ç„¡éœ€åˆ†ç¾¤ï¼Œé€™è£¡æ˜¯ä½ çš„è¨­è¨ˆç‰¹è‰² å¯ä»¥ä¸ç”¨ç‰¹åˆ¥è¿½æ±‚å®Œç¾çš„ç†è«–å…¬å¼ï¼Œé‡é»æ˜¯æ¶æ§‹åˆç†ã€çµæœå¯è§£é‡‹æ€§å¤  ","permalink":"http://localhost:1313/posts/20250710_meeting/","summary":"\u003ch3 id=\"generalized-association-plotsgap\"\u003eGeneralized Association Plots(GAP)\u003c/h3\u003e\n\u003cp\u003eé‡å°é«˜ç¶­åº¦çš„è³‡æ–™é€²è¡Œè¦–è¦ºåŒ–èˆ‡æ’åºçš„å·¥å…·\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eèµ·å§‹çŸ©é™£ D: è¼¸å…¥ä»»æ„ä¸€å€‹ p by p çš„ proximity matrix ï¼ˆå¦‚çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼‰\u003c/li\u003e\n\u003cli\u003eéè¿´ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼š\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e$R^{(1)}=\\phi(D)$\u003c/li\u003e\n\u003cli\u003e$R^{(n+1)}=\\phi(R^{(n)})$\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eæ”¶æ–‚ï¼š\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e$R(n)$ æœƒæ”¶æ–‚è‡³ +1 å’Œ -1 çš„çŸ©é™£ $R^{(\\infty)}$\u003c/li\u003e\n\u003cli\u003eæ”¶æ–‚çš„éç¨‹ç•¶ä¸­æœƒè§€å¯Ÿåˆ°rankçš„é™ç¶­å’Œæ©¢åœ“çµæ§‹çš„è®ŠåŒ–ï¼ˆç”±æ©¢åœ“å£“ç¸®è‡³ä¸€ç›´ç·šï¼‰, å°æ–¼æ’åºï¼ˆseriationï¼‰å’Œåˆ†ç¾¤ï¼ˆclusteringï¼‰å¾ˆæœ‰ç”¨\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"æ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æ\"\u003eæ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æï¼š\u003c/h4\u003e\n\u003cp\u003eè‹¥å°æ–°èæ–‡æœ¬é€²è¡Œä¸»é¡Œå»ºæ¨¡åˆ†æï¼Œèƒ½é€éå»ºç«‹æ–‡ä»¶ $\\rightarrow$ Topic æˆ– Topic-Keyword çš„è¿‘ä¼¼çŸ©é™£ï¼Œè—‰ç”± GAP çš„æ’åºèˆ‡åˆ†ç¾¤ï¼Œæœ‰åŠ©æ–¼\u003cstrong\u003eTopic æˆ– Document ä¹‹é–“çµæ§‹çš„è¦–è¦ºåŒ–èˆ‡ç†è§£\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"procedure\"\u003eProcedure\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eæ­¥é©Ÿ\u003c/th\u003e\n          \u003cth\u003eå…§å®¹\u003c/th\u003e\n          \u003cth\u003eç›®çš„\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eç”¨ \u003cstrong\u003eBBC News 5 é¡åˆ¥è³‡æ–™ï¼ˆèªæ–™åº«ï¼‰\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eä½œç‚ºç¯„ä¾‹èªæ–™\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003eæ¯é¡å– \u003cstrong\u003eå‰ 15 å€‹ä»£è¡¨ token\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå»ºç«‹å°å‹ token å­é›†ï¼ˆ75 tokensï¼‰\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003eç”¨ \u003cstrong\u003eGAP æ’åºè¦–è¦ºåŒ– 75Ã—75 çŸ©é™£\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eæ‰¾åˆ°èªæ„ç¾¤èšçµæ§‹\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eäººå·¥ç¢ºèªåˆ† 5 ç¾¤\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå°‡ token-to-topic å®šç¾©å¥½\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n          \u003ctd\u003eç”¨ token ç¾¤å»º \u003cstrong\u003eLDA çš„ Î· å…ˆé©—\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå°å¼• LDA å­¸å‡ºé æœŸä¸»é¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"question\"\u003eQuestion\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå•é¡Œ\u003c/th\u003e\n          \u003cth\u003eä½ çš„å›ç­”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP çµæœä¸Ÿé€² LDA äº†å—ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ˜¯ï¼Œé€é Î· çŸ©é™£é–“æ¥å°å…¥ï¼Œä¸æ˜¯ç›´æ¥ä¸Ÿ category\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eÎ· æ€éº¼è¨­å®šï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eGAP ç¾¤å…§ Î· å¤§ï¼ˆå¦‚ 0.3ï¼‰ï¼Œå…¶ä»–å°ï¼ˆå¦‚ 0.01ï¼‰\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP correlation å€¼æœ‰ç”¨ä¾†ç•¶æ©Ÿç‡å—ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ²’æœ‰ï¼Œåªæœ‰åˆ†ç¾¤çµæœç”¨ä¾†æŒ‡å®š topic\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP æ˜¯å¦ç›´æ¥å½±éŸ¿æ–‡ä»¶åˆ†é¡ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ²’æœ‰ï¼Œåªå½±éŸ¿ LDA ä¸»é¡Œ-è©å…ˆé©—\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eæ˜¯å¦åš baseline LDA æ¯”è¼ƒï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eå°šæœªï¼Œä¹‹å¾Œæœƒè£œåšå°æ¯” baseline\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"advice\"\u003eAdvice\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGAP â†’ å¹«åŠ© Î· çµæ§‹è¨­è¨ˆ\u003c/li\u003e\n\u003cli\u003ebaseline LDA vs GAP-informed LDA çµæœå°æ¯”\u003c/li\u003e\n\u003cli\u003eLDA åŸç†æœ¬ä¾†ç„¡éœ€åˆ†ç¾¤ï¼Œé€™è£¡æ˜¯ä½ çš„è¨­è¨ˆç‰¹è‰²\u003c/li\u003e\n\u003cli\u003eå¯ä»¥ä¸ç”¨ç‰¹åˆ¥è¿½æ±‚å®Œç¾çš„ç†è«–å…¬å¼ï¼Œé‡é»æ˜¯æ¶æ§‹åˆç†ã€çµæœå¯è§£é‡‹æ€§å¤ \u003c/li\u003e\n\u003c/ul\u003e","title":"20250710 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸âƒ£ GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸âƒ£ è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ âœ… 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰ å°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ \\text{Final_Similarity} = \\lambda \\cdot \\text{GAP_Col_Prox} + (1 - \\lambda) \\cdot \\text{Cosine_Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\nâœ… 3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹ ä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nâœ… é€™æ¨£åšçš„è²¢ç»ï¼š ä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸âƒ£ GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"-2-å»ºç«‹-æ··åˆå‹ç›¸ä¼¼çŸ©é™£å…±ç¾--è©æ„\"\u003eâœ… 2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/h2\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Final_Similarity} = \\lambda \\cdot \\text{GAP_Col_Prox} + (1 - \\lambda) \\cdot \\text{Cosine_Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"Generalized Association Plots(GAP) é‡å°é«˜ç¶­åº¦çš„è³‡æ–™é€²è¡Œè¦–è¦ºåŒ–èˆ‡æ’åºçš„å·¥å…·\nèµ·å§‹çŸ©é™£ D: è¼¸å…¥ä»»æ„ä¸€å€‹ p by p çš„ proximity matrix ï¼ˆå¦‚çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼‰ éè¿´ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼š $R^{(1)}=\\phi(D)$ $R^{(n+1)}=\\phi(R^{(n)})$ æ”¶æ–‚ï¼š $R(n)$ æœƒæ”¶æ–‚è‡³ +1 å’Œ -1 çš„çŸ©é™£ $R^{(\\infty)}$ æ”¶æ–‚çš„éç¨‹ç•¶ä¸­æœƒè§€å¯Ÿåˆ°rankçš„é™ç¶­å’Œæ©¢åœ“çµæ§‹çš„è®ŠåŒ–ï¼ˆç”±æ©¢åœ“å£“ç¸®è‡³ä¸€ç›´ç·šï¼‰, å°æ–¼æ’åºï¼ˆseriationï¼‰å’Œåˆ†ç¾¤ï¼ˆclusteringï¼‰å¾ˆæœ‰ç”¨ æ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æï¼š è‹¥å°æ–°èæ–‡æœ¬é€²è¡Œä¸»é¡Œå»ºæ¨¡åˆ†æï¼Œèƒ½é€éå»ºç«‹æ–‡ä»¶ $\\rightarrow$ Topic æˆ– Topic-Keyword çš„è¿‘ä¼¼çŸ©é™£ï¼Œè—‰ç”± GAP çš„æ’åºèˆ‡åˆ†ç¾¤ï¼Œæœ‰åŠ©æ–¼Topic æˆ– Document ä¹‹é–“çµæ§‹çš„è¦–è¦ºåŒ–èˆ‡ç†è§£\nProcedure æ­¥é©Ÿ å…§å®¹ ç›®çš„ 1 ç”¨ BBC News 5 é¡åˆ¥è³‡æ–™ï¼ˆèªæ–™åº«ï¼‰ ä½œç‚ºç¯„ä¾‹èªæ–™ 2 æ¯é¡å– å‰ 15 å€‹ä»£è¡¨ token å»ºç«‹å°å‹ token å­é›†ï¼ˆ75 tokensï¼‰ 3 ç”¨ GAP æ’åºè¦–è¦ºåŒ– 75Ã—75 çŸ©é™£ æ‰¾åˆ°èªæ„ç¾¤èšçµæ§‹ 4 äººå·¥ç¢ºèªåˆ† 5 ç¾¤ å°‡ token-to-topic å®šç¾©å¥½ 5 ç”¨ token ç¾¤å»º LDA çš„ Î· å…ˆé©— å°å¼• LDA å­¸å‡ºé æœŸä¸»é¡Œ Question å•é¡Œ ä½ çš„å›ç­” GAP çµæœä¸Ÿé€² LDA äº†å—ï¼Ÿ æ˜¯ï¼Œé€é Î· çŸ©é™£é–“æ¥å°å…¥ï¼Œä¸æ˜¯ç›´æ¥ä¸Ÿ category Î· æ€éº¼è¨­å®šï¼Ÿ GAP ç¾¤å…§ Î· å¤§ï¼ˆå¦‚ 0.3ï¼‰ï¼Œå…¶ä»–å°ï¼ˆå¦‚ 0.01ï¼‰ GAP correlation å€¼æœ‰ç”¨ä¾†ç•¶æ©Ÿç‡å—ï¼Ÿ æ²’æœ‰ï¼Œåªæœ‰åˆ†ç¾¤çµæœç”¨ä¾†æŒ‡å®š topic GAP æ˜¯å¦ç›´æ¥å½±éŸ¿æ–‡ä»¶åˆ†é¡ï¼Ÿ æ²’æœ‰ï¼Œåªå½±éŸ¿ LDA ä¸»é¡Œ-è©å…ˆé©— æ˜¯å¦åš baseline LDA æ¯”è¼ƒï¼Ÿ å°šæœªï¼Œä¹‹å¾Œæœƒè£œåšå°æ¯” baseline Advice GAP â†’ å¹«åŠ© Î· çµæ§‹è¨­è¨ˆ baseline LDA vs GAP-informed LDA çµæœå°æ¯” LDA åŸç†æœ¬ä¾†ç„¡éœ€åˆ†ç¾¤ï¼Œé€™è£¡æ˜¯ä½ çš„è¨­è¨ˆç‰¹è‰² å¯ä»¥ä¸ç”¨ç‰¹åˆ¥è¿½æ±‚å®Œç¾çš„ç†è«–å…¬å¼ï¼Œé‡é»æ˜¯æ¶æ§‹åˆç†ã€çµæœå¯è§£é‡‹æ€§å¤  ","permalink":"http://localhost:1313/posts/20250710_meeting/","summary":"\u003ch3 id=\"generalized-association-plotsgap\"\u003eGeneralized Association Plots(GAP)\u003c/h3\u003e\n\u003cp\u003eé‡å°é«˜ç¶­åº¦çš„è³‡æ–™é€²è¡Œè¦–è¦ºåŒ–èˆ‡æ’åºçš„å·¥å…·\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eèµ·å§‹çŸ©é™£ D: è¼¸å…¥ä»»æ„ä¸€å€‹ p by p çš„ proximity matrix ï¼ˆå¦‚çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼‰\u003c/li\u003e\n\u003cli\u003eéè¿´ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼š\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e$R^{(1)}=\\phi(D)$\u003c/li\u003e\n\u003cli\u003e$R^{(n+1)}=\\phi(R^{(n)})$\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eæ”¶æ–‚ï¼š\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e$R(n)$ æœƒæ”¶æ–‚è‡³ +1 å’Œ -1 çš„çŸ©é™£ $R^{(\\infty)}$\u003c/li\u003e\n\u003cli\u003eæ”¶æ–‚çš„éç¨‹ç•¶ä¸­æœƒè§€å¯Ÿåˆ°rankçš„é™ç¶­å’Œæ©¢åœ“çµæ§‹çš„è®ŠåŒ–ï¼ˆç”±æ©¢åœ“å£“ç¸®è‡³ä¸€ç›´ç·šï¼‰, å°æ–¼æ’åºï¼ˆseriationï¼‰å’Œåˆ†ç¾¤ï¼ˆclusteringï¼‰å¾ˆæœ‰ç”¨\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"æ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æ\"\u003eæ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æï¼š\u003c/h4\u003e\n\u003cp\u003eè‹¥å°æ–°èæ–‡æœ¬é€²è¡Œä¸»é¡Œå»ºæ¨¡åˆ†æï¼Œèƒ½é€éå»ºç«‹æ–‡ä»¶ $\\rightarrow$ Topic æˆ– Topic-Keyword çš„è¿‘ä¼¼çŸ©é™£ï¼Œè—‰ç”± GAP çš„æ’åºèˆ‡åˆ†ç¾¤ï¼Œæœ‰åŠ©æ–¼\u003cstrong\u003eTopic æˆ– Document ä¹‹é–“çµæ§‹çš„è¦–è¦ºåŒ–èˆ‡ç†è§£\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"procedure\"\u003eProcedure\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eæ­¥é©Ÿ\u003c/th\u003e\n          \u003cth\u003eå…§å®¹\u003c/th\u003e\n          \u003cth\u003eç›®çš„\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eç”¨ \u003cstrong\u003eBBC News 5 é¡åˆ¥è³‡æ–™ï¼ˆèªæ–™åº«ï¼‰\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eä½œç‚ºç¯„ä¾‹èªæ–™\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003eæ¯é¡å– \u003cstrong\u003eå‰ 15 å€‹ä»£è¡¨ token\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå»ºç«‹å°å‹ token å­é›†ï¼ˆ75 tokensï¼‰\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003eç”¨ \u003cstrong\u003eGAP æ’åºè¦–è¦ºåŒ– 75Ã—75 çŸ©é™£\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eæ‰¾åˆ°èªæ„ç¾¤èšçµæ§‹\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eäººå·¥ç¢ºèªåˆ† 5 ç¾¤\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå°‡ token-to-topic å®šç¾©å¥½\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n          \u003ctd\u003eç”¨ token ç¾¤å»º \u003cstrong\u003eLDA çš„ Î· å…ˆé©—\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå°å¼• LDA å­¸å‡ºé æœŸä¸»é¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"question\"\u003eQuestion\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå•é¡Œ\u003c/th\u003e\n          \u003cth\u003eä½ çš„å›ç­”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP çµæœä¸Ÿé€² LDA äº†å—ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ˜¯ï¼Œé€é Î· çŸ©é™£é–“æ¥å°å…¥ï¼Œä¸æ˜¯ç›´æ¥ä¸Ÿ category\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eÎ· æ€éº¼è¨­å®šï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eGAP ç¾¤å…§ Î· å¤§ï¼ˆå¦‚ 0.3ï¼‰ï¼Œå…¶ä»–å°ï¼ˆå¦‚ 0.01ï¼‰\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP correlation å€¼æœ‰ç”¨ä¾†ç•¶æ©Ÿç‡å—ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ²’æœ‰ï¼Œåªæœ‰åˆ†ç¾¤çµæœç”¨ä¾†æŒ‡å®š topic\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP æ˜¯å¦ç›´æ¥å½±éŸ¿æ–‡ä»¶åˆ†é¡ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ²’æœ‰ï¼Œåªå½±éŸ¿ LDA ä¸»é¡Œ-è©å…ˆé©—\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eæ˜¯å¦åš baseline LDA æ¯”è¼ƒï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eå°šæœªï¼Œä¹‹å¾Œæœƒè£œåšå°æ¯” baseline\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"advice\"\u003eAdvice\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGAP â†’ å¹«åŠ© Î· çµæ§‹è¨­è¨ˆ\u003c/li\u003e\n\u003cli\u003ebaseline LDA vs GAP-informed LDA çµæœå°æ¯”\u003c/li\u003e\n\u003cli\u003eLDA åŸç†æœ¬ä¾†ç„¡éœ€åˆ†ç¾¤ï¼Œé€™è£¡æ˜¯ä½ çš„è¨­è¨ˆç‰¹è‰²\u003c/li\u003e\n\u003cli\u003eå¯ä»¥ä¸ç”¨ç‰¹åˆ¥è¿½æ±‚å®Œç¾çš„ç†è«–å…¬å¼ï¼Œé‡é»æ˜¯æ¶æ§‹åˆç†ã€çµæœå¯è§£é‡‹æ€§å¤ \u003c/li\u003e\n\u003c/ul\u003e","title":"20250710 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"Generalized Association Plots(GAP) é‡å°é«˜ç¶­åº¦çš„è³‡æ–™é€²è¡Œè¦–è¦ºåŒ–èˆ‡æ’åºçš„å·¥å…·\nèµ·å§‹çŸ©é™£ D: è¼¸å…¥ä»»æ„ä¸€å€‹ p by p çš„ proximity matrix ï¼ˆå¦‚çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼‰ éè¿´ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼š $R^{(1)}=\\phi(D)$ $R^{(n+1)}=\\phi(R^{(n)})$ æ”¶æ–‚ï¼š $R(n)$ æœƒæ”¶æ–‚è‡³ +1 å’Œ -1 çš„çŸ©é™£ $R^{(\\infty)}$ æ”¶æ–‚çš„éç¨‹ç•¶ä¸­æœƒè§€å¯Ÿåˆ°rankçš„é™ç¶­å’Œæ©¢åœ“çµæ§‹çš„è®ŠåŒ–ï¼ˆç”±æ©¢åœ“å£“ç¸®è‡³ä¸€ç›´ç·šï¼‰, å°æ–¼æ’åºï¼ˆseriationï¼‰å’Œåˆ†ç¾¤ï¼ˆclusteringï¼‰å¾ˆæœ‰ç”¨ æ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æï¼š è‹¥å°æ–°èæ–‡æœ¬é€²è¡Œä¸»é¡Œå»ºæ¨¡åˆ†æï¼Œèƒ½é€éå»ºç«‹æ–‡ä»¶ $\\rightarrow$ Topic æˆ– Topic-Keyword çš„è¿‘ä¼¼çŸ©é™£ï¼Œè—‰ç”± GAP çš„æ’åºèˆ‡åˆ†ç¾¤ï¼Œæœ‰åŠ©æ–¼Topic æˆ– Document ä¹‹é–“çµæ§‹çš„è¦–è¦ºåŒ–èˆ‡ç†è§£\nProcedure æ­¥é©Ÿ å…§å®¹ ç›®çš„ 1 ç”¨ BBC News 5 é¡åˆ¥è³‡æ–™ï¼ˆèªæ–™åº«ï¼‰ ä½œç‚ºç¯„ä¾‹èªæ–™ 2 æ¯é¡å– å‰ 15 å€‹ä»£è¡¨ token å»ºç«‹å°å‹ token å­é›†ï¼ˆ75 tokensï¼‰ 3 ç”¨ GAP æ’åºè¦–è¦ºåŒ– 75Ã—75 çŸ©é™£ æ‰¾åˆ°èªæ„ç¾¤èšçµæ§‹ 4 äººå·¥ç¢ºèªåˆ† 5 ç¾¤ å°‡ token-to-topic å®šç¾©å¥½ 5 ç”¨ token ç¾¤å»º LDA çš„ Î· å…ˆé©— å°å¼• LDA å­¸å‡ºé æœŸä¸»é¡Œ Question å•é¡Œ ä½ çš„å›ç­” GAP çµæœä¸Ÿé€² LDA äº†å—ï¼Ÿ æ˜¯ï¼Œé€é Î· çŸ©é™£é–“æ¥å°å…¥ï¼Œä¸æ˜¯ç›´æ¥ä¸Ÿ category Î· æ€éº¼è¨­å®šï¼Ÿ GAP ç¾¤å…§ Î· å¤§ï¼ˆå¦‚ 0.3ï¼‰ï¼Œå…¶ä»–å°ï¼ˆå¦‚ 0.01ï¼‰ GAP correlation å€¼æœ‰ç”¨ä¾†ç•¶æ©Ÿç‡å—ï¼Ÿ æ²’æœ‰ï¼Œåªæœ‰åˆ†ç¾¤çµæœç”¨ä¾†æŒ‡å®š topic GAP æ˜¯å¦ç›´æ¥å½±éŸ¿æ–‡ä»¶åˆ†é¡ï¼Ÿ æ²’æœ‰ï¼Œåªå½±éŸ¿ LDA ä¸»é¡Œ-è©å…ˆé©— æ˜¯å¦åš baseline LDA æ¯”è¼ƒï¼Ÿ å°šæœªï¼Œä¹‹å¾Œæœƒè£œåšå°æ¯” baseline Advice GAP â†’ å¹«åŠ© Î· çµæ§‹è¨­è¨ˆ baseline LDA vs GAP-informed LDA çµæœå°æ¯” LDA åŸç†æœ¬ä¾†ç„¡éœ€åˆ†ç¾¤ï¼Œé€™è£¡æ˜¯ä½ çš„è¨­è¨ˆç‰¹è‰² å¯ä»¥ä¸ç”¨ç‰¹åˆ¥è¿½æ±‚å®Œç¾çš„ç†è«–å…¬å¼ï¼Œé‡é»æ˜¯æ¶æ§‹åˆç†ã€çµæœå¯è§£é‡‹æ€§å¤  ","permalink":"http://localhost:1313/posts/20250710_meeting/","summary":"\u003ch3 id=\"generalized-association-plotsgap\"\u003eGeneralized Association Plots(GAP)\u003c/h3\u003e\n\u003cp\u003eé‡å°é«˜ç¶­åº¦çš„è³‡æ–™é€²è¡Œè¦–è¦ºåŒ–èˆ‡æ’åºçš„å·¥å…·\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eèµ·å§‹çŸ©é™£ D: è¼¸å…¥ä»»æ„ä¸€å€‹ p by p çš„ proximity matrix ï¼ˆå¦‚çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼‰\u003c/li\u003e\n\u003cli\u003eéè¿´ç›¸é—œä¿‚æ•¸çŸ©é™£ï¼š\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e$R^{(1)}=\\phi(D)$\u003c/li\u003e\n\u003cli\u003e$R^{(n+1)}=\\phi(R^{(n)})$\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eæ”¶æ–‚ï¼š\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e$R(n)$ æœƒæ”¶æ–‚è‡³ +1 å’Œ -1 çš„çŸ©é™£ $R^{(\\infty)}$\u003c/li\u003e\n\u003cli\u003eæ”¶æ–‚çš„éç¨‹ç•¶ä¸­æœƒè§€å¯Ÿåˆ°rankçš„é™ç¶­å’Œæ©¢åœ“çµæ§‹çš„è®ŠåŒ–ï¼ˆç”±æ©¢åœ“å£“ç¸®è‡³ä¸€ç›´ç·šï¼‰, å°æ–¼æ’åºï¼ˆseriationï¼‰å’Œåˆ†ç¾¤ï¼ˆclusteringï¼‰å¾ˆæœ‰ç”¨\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"æ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æ\"\u003eæ‡‰ç”¨åœ¨æ–‡æœ¬åˆ†æï¼š\u003c/h4\u003e\n\u003cp\u003eè‹¥å°æ–°èæ–‡æœ¬é€²è¡Œä¸»é¡Œå»ºæ¨¡åˆ†æï¼Œèƒ½é€éå»ºç«‹æ–‡ä»¶ $\\rightarrow$ Topic æˆ– Topic-Keyword çš„è¿‘ä¼¼çŸ©é™£ï¼Œè—‰ç”± GAP çš„æ’åºèˆ‡åˆ†ç¾¤ï¼Œæœ‰åŠ©æ–¼\u003cstrong\u003eTopic æˆ– Document ä¹‹é–“çµæ§‹çš„è¦–è¦ºåŒ–èˆ‡ç†è§£\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"procedure\"\u003eProcedure\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eæ­¥é©Ÿ\u003c/th\u003e\n          \u003cth\u003eå…§å®¹\u003c/th\u003e\n          \u003cth\u003eç›®çš„\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eç”¨ \u003cstrong\u003eBBC News 5 é¡åˆ¥è³‡æ–™ï¼ˆèªæ–™åº«ï¼‰\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eä½œç‚ºç¯„ä¾‹èªæ–™\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003eæ¯é¡å– \u003cstrong\u003eå‰ 15 å€‹ä»£è¡¨ token\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå»ºç«‹å°å‹ token å­é›†ï¼ˆ75 tokensï¼‰\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003eç”¨ \u003cstrong\u003eGAP æ’åºè¦–è¦ºåŒ– 75Ã—75 çŸ©é™£\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eæ‰¾åˆ°èªæ„ç¾¤èšçµæ§‹\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eäººå·¥ç¢ºèªåˆ† 5 ç¾¤\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå°‡ token-to-topic å®šç¾©å¥½\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n          \u003ctd\u003eç”¨ token ç¾¤å»º \u003cstrong\u003eLDA çš„ Î· å…ˆé©—\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eå°å¼• LDA å­¸å‡ºé æœŸä¸»é¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"question\"\u003eQuestion\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå•é¡Œ\u003c/th\u003e\n          \u003cth\u003eä½ çš„å›ç­”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP çµæœä¸Ÿé€² LDA äº†å—ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ˜¯ï¼Œé€é Î· çŸ©é™£é–“æ¥å°å…¥ï¼Œä¸æ˜¯ç›´æ¥ä¸Ÿ category\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eÎ· æ€éº¼è¨­å®šï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eGAP ç¾¤å…§ Î· å¤§ï¼ˆå¦‚ 0.3ï¼‰ï¼Œå…¶ä»–å°ï¼ˆå¦‚ 0.01ï¼‰\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP correlation å€¼æœ‰ç”¨ä¾†ç•¶æ©Ÿç‡å—ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ²’æœ‰ï¼Œåªæœ‰åˆ†ç¾¤çµæœç”¨ä¾†æŒ‡å®š topic\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGAP æ˜¯å¦ç›´æ¥å½±éŸ¿æ–‡ä»¶åˆ†é¡ï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eæ²’æœ‰ï¼Œåªå½±éŸ¿ LDA ä¸»é¡Œ-è©å…ˆé©—\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eæ˜¯å¦åš baseline LDA æ¯”è¼ƒï¼Ÿ\u003c/td\u003e\n          \u003ctd\u003eå°šæœªï¼Œä¹‹å¾Œæœƒè£œåšå°æ¯” baseline\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"advice\"\u003eAdvice\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGAP â†’ å¹«åŠ© Î· çµæ§‹è¨­è¨ˆ\u003c/li\u003e\n\u003cli\u003ebaseline LDA vs GAP-informed LDA çµæœå°æ¯”\u003c/li\u003e\n\u003cli\u003eLDA åŸç†æœ¬ä¾†ç„¡éœ€åˆ†ç¾¤ï¼Œé€™è£¡æ˜¯ä½ çš„è¨­è¨ˆç‰¹è‰²\u003c/li\u003e\n\u003cli\u003eå¯ä»¥ä¸ç”¨ç‰¹åˆ¥è¿½æ±‚å®Œç¾çš„ç†è«–å…¬å¼ï¼Œé‡é»æ˜¯æ¶æ§‹åˆç†ã€çµæœå¯è§£é‡‹æ€§å¤ \u003c/li\u003e\n\u003c/ul\u003e","title":"20250710 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot \\GAP_Col/_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot \\GAP_Col/_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\text{Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot \\Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot \\GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalã„¥_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalã„¥_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final/{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal/\u003cem\u003e{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine\u003c/em\u003e{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final*{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal*\u003cem\u003e{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine\u003c/em\u003e{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_\\text{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_\\text{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal_{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Final${Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinal$\u003cem\u003e{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine\u003c/em\u003e{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿{Similarity} = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAP_Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Col_Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosine_{Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosine_{Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity} $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling\næˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š\nçµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$ 1\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\n1\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling æˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š çµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$\n$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling\næˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š\nçµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼Œå¯å¯¦é©—ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$\n$\\lambda$ï¼šæ¬Šé‡ï¼ˆå¦‚ 0.5, 0.7ï¼‰ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling\næˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š\nçµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡ï¼ˆå¦‚ 0.5, 0.7ï¼‰\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"GAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\n1ï¸âƒ£ åŠ å…¥ Word Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\nå·¥å…· ç”¨é€” Word2Vec / GloVe / FastText è¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“ Cosine Similarity è¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§ åšæ³•ï¼š 1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\nè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ embedding å¹³å‡ æª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜ 2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\nä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤ æˆ–å°‡ GAP + embedding çµæœçµåˆæˆ æ–°çš„ç›¸ä¼¼çŸ©é™£ 2ï¸âƒ£ å»ºç«‹ æ··åˆå‹ç›¸ä¼¼çŸ©é™£ï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\nå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\n$$ Finalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity $$\n$\\lambda$ï¼šæ¬Šé‡ GAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢ ç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\n3ï¸âƒ£ æˆ–è€…å°å…¥ è©å…¸ / çŸ¥è­˜åœ–è­œ å¼·åŒ–èªæ„çµæ§‹\nä¾‹å¦‚ï¼š\nWordNetï¼šè‹¥å…©è©ç‚ºåŒç¾©/ä¸Šä½é—œä¿‚ï¼ŒåŠ å¼·é€£çµ ConceptNetï¼šè£œè¶³å¸¸è­˜é—œè¯ é€™å¯é¡å¤–å¾®èª¿ GAP çµæœï¼Œä¿®æ­£ä¸åˆç†çš„ç¾¤çµ„ã€‚\nä½ å¯ä»¥ä¸»å¼µé€™æ˜¯ä¸€ç¨®ï¼š\nGAP-informed + Semantics-enhanced Topic Modeling\næˆ–æå‡ºä¸€å€‹æ··åˆçµæ§‹ï¼š\nçµ±è¨ˆå…±ç¾ Ã— èªæ„ç›¸ä¼¼çš„é›™å±¤çµæ§‹ï¼Œç”¨æ–¼å»ºæ§‹æ›´åˆç†çš„ä¸»é¡Œå…ˆé©—\n","permalink":"http://localhost:1313/posts/20250717_meeting/","summary":"\u003cp\u003e\u003cstrong\u003eGAP è£œä¸Šè¾­æ„çš„è¨Šæ¯ä¾†å¼·åŒ–èªæ„çš„åˆç†æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1ï¸âƒ£ åŠ å…¥ \u003cstrong\u003eWord Embeddingï¼ˆè©å‘é‡ï¼‰ç›¸ä¼¼æ€§\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eå·¥å…·\u003c/th\u003e\n          \u003cth\u003eç”¨é€”\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWord2Vec / GloVe / FastText\u003c/td\u003e\n          \u003ctd\u003eè¡¨ç¤ºè©æ„åˆ†å¸ƒçš„å‘é‡ç©ºé–“\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCosine Similarity\u003c/td\u003e\n          \u003ctd\u003eè¡¡é‡å…©è©èªæ„ç›¸ä¼¼æ€§\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"åšæ³•\"\u003eåšæ³•ï¼š\u003c/h3\u003e\n\u003cp\u003e1ï¸. GAP æ’å®Œå¾Œï¼Œå°æ¯å€‹ç¾¤çš„ tokenï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè¨ˆç®—è©²ç¾¤å…§æ‰€æœ‰è©çš„ \u003cstrong\u003eembedding å¹³å‡\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eæª¢æŸ¥é€™äº›è©å½¼æ­¤ cosine similarity æ˜¯å¦å¤ é«˜\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸. è‹¥æœ‰æ˜é¡¯ã€Œèªæ„ä¸åˆã€çš„è©ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eä½ å¯ä»¥æ‰‹å‹•å¾®èª¿æˆ–é‡æ–°åˆ†ç¾¤\u003c/li\u003e\n\u003cli\u003eæˆ–å°‡ GAP + embedding çµæœçµåˆæˆ \u003cstrong\u003eæ–°çš„ç›¸ä¼¼çŸ©é™£\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2ï¸âƒ£ å»ºç«‹ \u003cstrong\u003eæ··åˆå‹ç›¸ä¼¼çŸ©é™£\u003c/strong\u003eï¼ˆå…±ç¾ Ã— è©æ„ï¼‰\u003c/p\u003e\n\u003cp\u003eå°‡ GAP çš„ col_proxï¼ˆå…±ç¾ correlationï¼‰èˆ‡ Word2Vec ç›¸ä¼¼æ€§åšçµåˆï¼š\u003c/p\u003e\n\u003cp\u003e$$\nFinalï¼¿Similarity = \\lambda \\cdot GAPï¼¿Colï¼¿Prox + (1 - \\lambda) \\cdot Cosineï¼¿Similarity\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ï¼šæ¬Šé‡\u003c/li\u003e\n\u003cli\u003eGAP æ•æ‰å…±ç¾çµæ§‹ï¼Œè©å‘é‡è£œè¶³èªæ„è·é›¢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eç”¨é€™å€‹æ··åˆçŸ©é™£å†é€²è¡Œ GAP æ’åºæˆ–åˆ†ç¾¤ï¼Œæ›´ç©©å¥ã€‚\u003c/p\u003e\n\u003cp\u003e3ï¸âƒ£ æˆ–è€…å°å…¥ \u003cstrong\u003eè©å…¸ / çŸ¥è­˜åœ–è­œ\u003c/strong\u003e å¼·åŒ–èªæ„çµæ§‹\u003c/p\u003e","title":"20250717 Meeting"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"å‰æƒ…æè¦ é€™è£¡çš„ LDA æŒ‡çš„æ˜¯ Latent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\nä¸æ˜¯ Linear Discriminant Analysis\né—œæ–¼ LDA ä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± Blei, D. M. ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\nä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\nå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\nè©³ç´°å…§å®¹è«‹åƒè€ƒç¶­åŸºç™¾ç§‘ï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–è«–æ–‡ï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\næœ¬ç¯‡ä¸»æ—¨ åœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ– å–è‡ªæ–‡ç»\næ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\nä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\næˆ‘ï¼š\nçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡, å„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ, æˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ), ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡, æœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ, å› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\nGPTï¼š\nâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\nâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚ âœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚ âœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚ âœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚ âœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚ ğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\nä½ çš„æ•˜è¿°è£¡æåˆ°ã€Œä¹˜ä¸Š $\\beta$ åˆ†å¸ƒä¸‹çš„ $k$ ä¸»é¡Œä¾†æ±ºå®šæ¯å€‹ $w$ ä¾†è‡ªå“ªå€‹ä¸»é¡Œã€ï¼Œé€™è£¡æœ‰é»æ¨¡ç³Šã€‚äº‹å¯¦ä¸Šï¼Œä¸æ˜¯ã€Œä¹˜ä¸Š $\\beta$ æ‰æ±ºå®šä¸»é¡Œã€ï¼Œè€Œæ˜¯ï¼š\nğŸ¯ LDA æ˜¯é€™æ¨£æ¨æ–·çš„ï¼š å°æ–¼ä¸€å€‹è© $w_n$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„ç”Ÿæˆéç¨‹ï¼š\nå°æ–‡ä»¶ $d$ï¼š\næŠ½ä¸»é¡Œåˆ†å¸ƒ $\\theta_d \\sim \\text{Dir}(\\alpha)$ å°ä¸»é¡Œ $k$ï¼š\næŠ½è©åˆ†å¸ƒ $\\phi_k \\sim \\text{Dir}(\\eta)$ å°æ¯å€‹è©ä½ç½® $n$ï¼š\næŠ½ä¸»é¡ŒæŒ‡æ´¾ $z_{dn} \\sim \\text{Mult}(\\theta_d)$ æŠ½è©èª $w_{dn} \\sim \\text{Mult}(\\phi_{z_{dn}})$ ğŸ“ æ‰€ä»¥ä½ è¦æè¿°çš„å®Œæ•´ç”Ÿæˆæ©Ÿç‡ç‚ºï¼š\n$$ P(w_d) = \\prod_{n=1}^{N_d} \\sum_{k=1}^{K} P(w_n \\mid z_n = k, \\beta) \\cdot P(z_n = k \\mid \\theta_d) $$\nä¹Ÿå¯ä»¥å¯«æˆæ¢ä»¶æ©Ÿç‡çš„è§€é»ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^K P(w_n \\mid z_n = k) \\cdot P(z_n = k \\mid d) $$ å…¶ä¸­\n$P(w_n \\mid z_n = k)$ è¡¨ç¤ºä¸»é¡Œ $k$ ç”¢ç”Ÿè© $w_n$ çš„æ©Ÿç‡ $P(z_n = k \\mid d)$ è¡¨ç¤ºç¬¬ $d$ ç¯‡æ–‡ä»¶åå¥½ä¸»é¡Œ $k$ çš„æ©Ÿç‡ ğŸ“Œ ä½ æœ€å¾Œèªªçš„ã€Œæ±ºå®šæ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œã€æ˜¯æ€éº¼ä¾†çš„ï¼Ÿ\nå…¶å¯¦ä¸æ˜¯åªæ±ºå®šä¸€å€‹ä¸»é¡Œï¼Œè€Œæ˜¯ï¼š\nLDA çµ¦æ¯ç¯‡æ–‡ç« ä¸€å€‹ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒ $\\theta_d$ ä½ å¯ä»¥èªªã€Œæ–‡ç«  d ä¸»è¦å±¬æ–¼ä¸»é¡Œ kã€ï¼Œå¦‚æœ $\\theta_{d,k}$ æœ€å¤§ ä½†å¯¦éš›ä¸Šï¼Œæ¯ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šå€‹ä¸»é¡Œæ··åˆ âœ… ä¿®æ­£ç‰ˆç¸½çµï¼š ä½ åŸæœ¬èªªï¼š\nâ€œæˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ)ä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡â€¦â€\nâ¡ï¸ æ­£ç¢ºæ”¹å¯«å¦‚ä¸‹ï¼š\næˆ‘å€‘å°æ–¼æ–‡ä»¶ $d$ ä¸­çš„æ¯å€‹è© $w_n$ï¼Œè¨ˆç®—å…¶ä¾†è‡ªæ‰€æœ‰ä¸»é¡Œçš„æ©Ÿç‡ç¸½å’Œï¼Œå³ï¼š\n$$ P(w_n \\mid d) = \\sum_{k=1}^{K} P(w_n \\mid \\phi_k) \\cdot P(z_n = k \\mid \\theta_d) $$\né€™ä»£è¡¨ç¬¬ $d$ ç¯‡æ–‡ç« ä¸­è©èª $w_n$ çš„ç”Ÿæˆæ©Ÿç‡ã€‚\nçµè«– GPT å¹«æˆ‘ä¿®æ­£äº†é—œæ–¼å‡è¨­ $\\beta$ æ©Ÿç‡åˆ†å¸ƒæ˜¯ $\\eta$ æ™‚çš„ä¸»é¡Œ $k$ ä¸‹çš„è© $w_n$ çš„æ©Ÿç‡æè¿°\næœ€å¾Œè¦å†è£œå……\nGPT æ˜¯å¥½ç”¨çš„å·¥å…·, ä½†æ˜¯å®ƒçš„æ©Ÿåˆ¶æ˜¯å¾å­¸ç¿’åˆ°çš„è³‡æ–™å»è¼¸å‡º, ä¸æœƒç”¢ç”Ÿæ–°çš„æ±è¥¿, ä½ è¦å­¸æœƒè®€æ‡‚å®ƒçš„è¼¸å‡º, å¦‚æœä¸€æ˜§åœ°ç›¸ä¿¡å®ƒçš„ä»»ä½•è¼¸å‡º, é‚£ä½ çš„è¼¸å‡ºä¹Ÿåªæœƒæ˜¯ä¸€å¨å±\nä½ ä¸ç”¨, åˆ¥äººä¹Ÿæœƒç”¨\n","permalink":"http://localhost:1313/posts/20250707_lda%E7%9A%84%E7%90%86%E8%A7%A3%E8%88%87ai%E7%9A%84%E4%BF%AE%E6%AD%A3/","summary":"\u003ch3 id=\"å‰æƒ…æè¦\"\u003eå‰æƒ…æè¦\u003c/h3\u003e\n\u003cp\u003eé€™è£¡çš„ LDA æŒ‡çš„æ˜¯ \u003cstrong\u003eLatent Dirichlet Allocation éš±å«ç‹„åˆ©å…‹é›·åˆ†ä½ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eä¸æ˜¯ Linear Discriminant Analysis\u003c/p\u003e\n\u003ch3 id=\"é—œæ–¼-lda\"\u003eé—œæ–¼ LDA\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eä¸€ç¨®ä¸»é¡Œæ¨¡å‹, ç”± \u003cem\u003eBlei, D. M.\u003c/em\u003e ç­‰äººåœ¨2003å¹´æå‡º, æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼çš„å­¸ç¿’ï¼ˆunsupervised learningï¼‰\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eä¸»è¦ç”¨é€”æ˜¯å°‡æ–‡æœ¬çš„ä¸»é¡ŒæŒ‰æ©Ÿç‡å‘é‡çš„æ–¹å¼æå‡º, ä¸”æ¯å€‹ä¸»é¡Œéƒ½æœ‰å…¶ç›¸å‘¼çš„æ–‡å­—å¯ä»¥å°ç…§\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eå…¶çµæ§‹ä¸»è¦æ˜¯å¤šå±¤çš„è²æ°ç¶²çµ¡çµ„æˆ, èµ·åˆæ˜¯EMæ¼”ç®—æ³•ä¾†ä¼°è¨ˆåƒæ•¸, è€Œå¾Œæ”¹æˆç”¨Gibbs Samplingä¾†ä¼°è¨ˆåƒæ•¸\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè©³ç´°å…§å®¹è«‹åƒè€ƒ\u003ca href=\"https://zh.wikipedia.org/zh-tw/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83\"\u003eç¶­åŸºç™¾ç§‘\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿç¶²ç«™ï¼‰æˆ–\u003ca href=\"https://robotics.stanford.edu/~ang/papers/jair03-lda.pdf\"\u003eè«–æ–‡\u003c/a\u003eï¼ˆé»æ“Šå¾Œé–‹å•Ÿ pdf æª”æ¡ˆï¼‰\u003c/p\u003e\n\u003ch3 id=\"æœ¬ç¯‡ä¸»æ—¨\"\u003eæœ¬ç¯‡ä¸»æ—¨\u003c/h3\u003e\n\u003cp\u003eåœ¨é–±è®€ç›¸é—œæ–‡ç»ä¹‹å¾Œ, å› ç‚ºå…¶æ ¸å¿ƒè§€å¿µä¾†è‡ªæ–¼å¤šå±¤çš„è²æ°ç¶²çµ¡, å¦‚åœ–\n\u003cimg alt=\"targets\" loading=\"lazy\" src=\"/images/LDA.png\"\u003eå–è‡ªæ–‡ç»\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æˆ‘å€‹äººæå‡ºäº†å°æ–¼ LDA æ¶æ§‹çš„çœ‹æ³•, ä¸¦ä¸Ÿé€² Chat GPT-4o æ¨¡å‹ä¾†ä¿®æ­£æˆ‘çš„è§€å¿µ\u003c/p\u003e\n\u003cp\u003eä»¥ä¸‹æ˜¯æˆ‘å’Œ GPT çš„å°è©±\u003c/p\u003e\n\u003cp\u003eæˆ‘ï¼š\u003c/p\u003e\n\u003cp\u003eçµ¦å®šä¸€å€‹ä¾†è‡ªè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„åƒæ•¸alpha, ç¬¬då€‹æ–‡ä»¶thetaæœ‰topic1,topic2,topic3\u0026hellip;çš„æ©Ÿç‡å‘é‡,\nå„å€‹topicåˆæœ‰å…±åŒçš„è©å½™w1,w2,w3..çš„æ©Ÿç‡åˆ†å¸ƒ,\næˆ‘å€‘è¦çŸ¥é“çš„æ˜¯åœ¨ç¬¬då€‹æ–‡ä»¶thetaä¹‹ä¸‹çš„topicæ©Ÿç‡èˆ‡ç¬¬topicä¸‹çš„w1çš„æ©Ÿç‡ä¹˜æ©Ÿç¸½å’Œ(æ¢ä»¶æ©Ÿç‡çš„æ¦‚å¿µ),\nä¾†å¾—çŸ¥é€™å€‹dæ–‡ä»¶æ“æœ‰é€™äº›w1,w2,w3\u0026hellip;çš„æ©Ÿç‡,\næœ€å¾Œå†ä¹˜ä¸Šçµ¦å®šetaåƒæ•¸çš„betaåˆ†å¸ƒä¸‹çš„kä¸»é¡Œ, ä¾†æ±ºå®šæ¯å€‹wä¾†è‡ªå“ªå€‹ä¸»é¡Œ,\nå› æ­¤å¾—åˆ°è©²æ–‡ä»¶å±¬æ–¼å“ªå€‹ä¸»é¡Œ\u003c/p\u003e\n\u003cp\u003eGPTï¼š\u003c/p\u003e\n\u003cp\u003eâœ… ä½ èªªå¾—å°çš„éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eâœ”ï¸ $Î±$ æ˜¯ä¾†è‡ª Dirichlet åˆ†å¸ƒçš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹æ–‡ä»¶çš„ä¸»é¡Œåˆ†å¸ƒ $Î¸$ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ¯å€‹ä¸»é¡Œ $k$ æœ‰ä¸€å€‹è©èªåˆ†å¸ƒ $Ï†_k$ï¼Œæ§åˆ¶è©²ä¸»é¡Œç”¢ç”Ÿå“ªäº›è©ï¼ˆ$w1, w2, \u0026hellip;$ï¼‰çš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ æ–‡ä»¶ç”Ÿæˆè©çš„æ©Ÿç‡å¯ç”±ï¼šã€Œä¸»é¡Œæ©Ÿç‡ Ã— è©åœ¨è©²ä¸»é¡Œä¸‹çš„æ©Ÿç‡ã€ä¾†æ±ºå®šã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ å°æ¯å€‹è© $w$ï¼Œæ¨¡å‹è¦æ ¹æ“š $Î¸$ å’Œ $Ï†_k$ ä¾†è¨ˆç®—å®ƒä¾†è‡ªå“ªå€‹ä¸»é¡Œçš„æ©Ÿç‡ã€‚\u003c/li\u003e\n\u003cli\u003eâœ”ï¸ $Î·$ æ˜¯ Dirichlet çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯å€‹ $Ï†_k$ï¼ˆä¸»é¡Œçš„è©åˆ†å¸ƒï¼‰çš„ç¨€ç–æ€§ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eğŸ”§ éœ€è¦èª¿æ•´èˆ‡è£œå¼·çš„åœ°æ–¹ï¼š\u003c/p\u003e","title":"20250707 LDAçš„ç†è§£èˆ‡AIçš„ä¿®æ­£"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€ ç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼ Mæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\nå¼•è‡ªç¶­åŸºç™¾ç§‘ Example from finalterm Assume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\nConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$ Suppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die. Based on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$. Solve With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\n$$ L_{complete}(\\theta)=\\prod^3_{i=1}f(y_i|\\theta)\\cdot f(Z_4;\\theta)\\cdot f(Z_5;\\theta) $$ Then we compute the complete log-likelihood:\n$$ lnL_{complete}{\\theta}=\\sum^3_{i=1}lnf(y_i|\\theta)+lnf(Z_4;\\theta)+lnf(Z_5;\\theta)=5ln\\theta-\\theta(\\sum^3_{i=1}y_i+Z_4+Z_5) $$\nE-step Given current estimate $\\theta^{(t)}$, we compute the expected log likelihood:\n$$ Q(\\theta|\\theta^{(t)})=E_{Z_4, Z_5|\\theta^{(t)}}[lnL_{complete}(\\theta)] $$ Since $Z_4, Z_5ï½Exp(\\lambda)$ the conditional expectation is:\n$$ E[Z|Z\u0026gt;c]=c+\\frac{1}{\\theta} $$\nThus:\n$$ E[Z_4|Z_4\u0026gt;1.2;\\theta^{(t)}]=1.2+\\frac{1}{\\theta^{(t)}}, E[Z_5|Z_5\u0026gt;2.3;\\theta^{(t)}]=2.3+\\frac{1}{\\theta^{(t)}} $$\nM-step Then we have total expected sum of lifetimes:\n$$ E^{(t)}_S=y_1+y_2+y_3+E[Z_4]+E[Z_5]=(1.5+0.58+3.4)+(1.2+\\frac{1}{\\theta^{(t)}})+(2.3+\\frac{1}{\\theta^{(t)}}) $$\nNow, let maximize $lnL_{complete}(\\theta)$ with respect to $\\theta$\n$$ lnL_{complete}(\\theta)=5ln\\theta-\\theta E^{(t)}_S\\implies \\frac{dlnLcomplete(\\theta)}{d\\theta}=\\frac{5}{\\theta}-E^{(t)}_S\\implies\\overset{\\text{Let}}{=}0\\implies\\theta^{(t+1)}=\\frac{5}{E^{(t)}_S}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nTherefore, we have EM update formula:\n$$ \\theta^{(t+1)}=\\frac{5}{8.98+2/\\theta^{(t)}} $$\nAnd we run the code on python, here is the code\nimport numpy as np y = np.array([1.5, 0.58, 3.4]) y4_ = 1.2; y5_ = 2.3 theta = 1; tol = 1e-6; max_iter = 100 theta_values = [theta] for i in range(max_iter): Ez4 = y4_+1/theta; Ez5 = y5_+1/theta # E-step theta_new = 5/(np.sum(y)+Ez4+Ez5) # M-step theta_values.append(theta_new) # æ”¶æ–‚æª¢æŸ¥ if abs(theta-theta_new)\u0026lt;tol: break theta = theta_new print(f\u0026#34;Estimated theta after {i+1} iterations: {theta:.6f}\u0026#34;) plt.plot(theta_values, marker=\u0026#39;o\u0026#39;) Finally, estimated theta after 14 iterations is 0.334077.\nThat\u0026rsquo;s great!!!\n","permalink":"http://localhost:1313/posts/20250703_em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm -ã€Œæœ€å¤§æœŸæœ›å€¼æ¼”ç®—æ³•ã€\u003c/li\u003e\n\u003cli\u003eç¶“éå…©å€‹æ­¥é©Ÿäº¤æ›¿é€²è¡Œè¨ˆç®—ï¼š\u003cbr\u003e\nç¬¬ä¸€æ­¥æ˜¯è¨ˆç®—æœŸæœ›å€¼ï¼ˆEï¼‰ï¼šåˆ©ç”¨å°éš±è—è®Šé‡çš„ç¾æœ‰ä¼°è¨ˆå€¼ï¼Œè¨ˆç®—å…¶æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆå€¼\u003cbr\u003e\nç¬¬äºŒæ­¥æ˜¯æœ€å¤§åŒ–ï¼ˆMï¼‰ï¼šæœ€å¤§åŒ–åœ¨Eæ­¥ä¸Šæ±‚å¾—çš„æœ€å¤§æ¦‚ä¼¼å€¼ä¾†è¨ˆç®—åƒæ•¸çš„å€¼\nMæ­¥ä¸Šæ‰¾åˆ°çš„åƒæ•¸ä¼°è¨ˆå€¼è¢«ç”¨æ–¼ä¸‹ä¸€å€‹Eæ­¥è¨ˆç®—ä¸­ï¼Œé€™å€‹éç¨‹ä¸æ–·äº¤æ›¿é€²è¡Œ\u003cbr\u003e\n\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-from-finalterm\"\u003eExample from finalterm\u003c/h3\u003e\n\u003cp\u003eAssume that $Y_1, Y_2, \u0026hellip;, Y_n ï½ exp(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider the MLE of $\\theta$ based on $Y_1, Y_2, \u0026hellip;, Y_n$\u003c/li\u003e\n\u003cli\u003eSuppose that 5 observed samples are collected from the experiment which measures the life time of the light bulb. Assume $y_1=1.5$, $y_2=0.58$,  $y_3=3.4$ are complete experiment process. Because of the time limit, the fourth and fifth experiment are terminated at times $y^*_4=1.2$ and $y^*_5=2.3$ before the light bulb die.\nBased on ($y_1, y_2, y_3, y^*_4, y^*_5$), please use EM algorithm to estimate $\\theta$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"solve\"\u003eSolve\u003c/h3\u003e\n\u003cp\u003eã€€ã€€With observed lifetimes: $y_1=1.5$, $y_2=0.58$, $y_3=3.4$ and  $y^*_4=1.2$, $y^*_5=2.3$, meaning the actual lifetimes $Z_4\u0026gt;1.2$, $Z_5\u0026gt;2.3$ are unknown. So we treat $Z_4$ and $Z_5$ as latent variables, and have the complete likelihood like:\u003c/p\u003e","title":"Expectation maximization algorithm"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¦¹ XGBoost Boost What is XGBoost? Think of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\nğŸ— Key Concepts in XGBoost Tree Building Start with an initial guess (e.g., average score). Measure how far off the prediction is from the real answer (this is called the residual). The next tree learns how to fix these errors. Every new tree improves on the mistakes of the previous trees. ğŸ¥¢ How to Divide the Data (Not Randomly) XGBoost doesnâ€™t split data based on traditional methods like information gain. It uses a formula called Gain, which measures how much a split improves prediction. A split only happens if:\n(Left + Right Score) \u0026gt; (Parent Score + Penalty) â“ How do we know if a split is good? Use a value called Similarity Score The higher the score, the more consistent (similar) the residuals are in that group ğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm Try all possible features and split points Very accurate but very slow ğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm Uses feature quantiles (e.g., median) to propose a few split points Group the data based on these splits and evaluate the best one Two options: Global Proposal: use global info to suggest splits Local Proposal: use local (node-specific) info ğŸ‹ Weighted Quantile Sketch Some data points are more important (like how teachers focus more on students who struggle) Each data point has a weight based on how wrong it was (second-order gradient) Use these weights to suggest better and more meaningful split points ğŸ•³ Handling Missing Values What if some feature values are missing? XGBoost learns a default path for missing data This makes the model more robust even when the data isnâ€™t complete ğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization Gamma (Î³)\nPenalizes overly complex trees A split only happens if Gain \u0026gt; Gamma Helps stop the model from splitting when it\u0026rsquo;s not really helpful Lambda (Î»)\nShrinks leaf node prediction values Prevents overconfident and overfit models âœ‚ Pruning After building the tree, XGBoost may prune parts that don\u0026rsquo;t help If a split\u0026rsquo;s gain is less than Gamma, that branch is cut off This leads to simpler trees that generalize better ğŸ§â€â™‚ï¸ Extra Tricks: Learn Smoothly and Fast Shrinkage (Learning Rate)\nOnly take a small step with each new tree Makes learning slower but more stable Column Subsampling\nOnly use a subset of features for each tree This speeds up training and reduces overfitting ","permalink":"http://localhost:1313/posts/20250330_extremegradientboost/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch1 id=\"-xgboost-boost\"\u003eğŸ¦¹ XGBoost Boost\u003c/h1\u003e\n\u003ch3 id=\"what-is-xgboost\"\u003eWhat is XGBoost?\u003c/h3\u003e\n\u003cp\u003eThink of XGBoost as a team of smart tutors, each correcting the mistakes made by the previous one, gradually improving your answers step by step.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"-key-concepts-in-xgboost-tree-building\"\u003eğŸ— Key Concepts in XGBoost Tree Building\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial guess (e.g., average score).\u003c/li\u003e\n\u003cli\u003eMeasure how far off the prediction is from the real answer (this is called the \u003cstrong\u003eresidual\u003c/strong\u003e).\u003c/li\u003e\n\u003cli\u003eThe next tree learns how to \u003cstrong\u003efix these errors\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEvery new tree improves on the mistakes of the previous trees.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-to-divide-the-data-not-randomly\"\u003eğŸ¥¢ How to Divide the Data (Not Randomly)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eXGBoost doesnâ€™t split data based on traditional methods like information gain.\u003c/li\u003e\n\u003cli\u003eIt uses a formula called \u003cstrong\u003eGain\u003c/strong\u003e, which measures how much a split improves prediction.\u003c/li\u003e\n\u003cli\u003eA split only happens if:\u003cbr\u003e\n\u003cstrong\u003e(Left + Right Score) \u0026gt; (Parent Score + Penalty)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-how-do-we-know-if-a-split-is-good\"\u003eâ“ How do we know if a split is good?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eUse a value called \u003cstrong\u003eSimilarity Score\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe higher the score, the more consistent (similar) the residuals are in that group\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-accurate--exact-greedy-algorithm\"\u003eğŸ¢ Two Ways to Find Splits: Accurate- Exact Greedy Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTry \u003cstrong\u003eall\u003c/strong\u003e possible features and split points\u003c/li\u003e\n\u003cli\u003eVery accurate but \u003cstrong\u003every slow\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-two-ways-to-find-splits-fast--approximate-algorithm\"\u003eğŸ‡ Two Ways to Find Splits: Fast- Approximate Algorithm\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u003cstrong\u003efeature quantiles\u003c/strong\u003e (e.g., median) to propose a few split points\u003c/li\u003e\n\u003cli\u003eGroup the data based on these splits and evaluate the best one\u003c/li\u003e\n\u003cli\u003eTwo options:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Proposal\u003c/strong\u003e: use global info to suggest splits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocal Proposal\u003c/strong\u003e: use local (node-specific) info\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-weighted-quantile-sketch\"\u003eğŸ‹ Weighted Quantile Sketch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSome data points are more important (like how teachers focus more on students who struggle)\u003c/li\u003e\n\u003cli\u003eEach data point has a \u003cstrong\u003eweight\u003c/strong\u003e based on how wrong it was (second-order gradient)\u003c/li\u003e\n\u003cli\u003eUse these weights to suggest better and more meaningful split points\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-handling-missing-values\"\u003eğŸ•³ Handling Missing Values\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if some feature values are \u003cstrong\u003emissing\u003c/strong\u003e?\u003c/li\u003e\n\u003cli\u003eXGBoost learns a \u003cstrong\u003edefault path\u003c/strong\u003e for missing data\u003c/li\u003e\n\u003cli\u003eThis makes the model more robust even when the data isnâ€™t complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"-controlling-model-complexity-regularization\"\u003eğŸ§šâ€â™€ï¸ Controlling Model Complexity: Regularization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGamma (Î³)\u003c/p\u003e","title":"XGBoost Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ‘¶ Naive Bayes By definition of Bayes\u0026rsquo; theorem $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nwhere\n$P(y)$ represents the prior probability of class $y$ $P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$ $P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$ With the assumption of Naive Bayes - Conditional Independence\n$$ P(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y) $$\nThis means that the probability of feature $x_i$ is no longer influenced by other features $x_j$ for $j \\not= x $ given the class y is known\nTherefore, we have $$ \\begin{aligned} P(x_1, x_2, \u0026hellip;, x_n \\mid y) \u0026amp;= P(x_1 \\mid y)P(x_2 \\mid y)\u0026hellip;P(x_n \\mid y) \\\\ \u0026amp;= \\prod_{i=1}^nP(x_i \\mid y) \\end{aligned} $$\nThis relationship implies that $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)} $$\nSince $P(x_1, x_2, \u0026hellip;, x_n)$ is constant given the input, we can use the following classification rule $$ P(y \\mid x_1, x_2, \u0026hellip;, x_n) \\propto P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nğŸ‘‰ Finally, we have $$ \\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^nP(x_i \\mid y) $$\nAnd we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$, and the former is then the relative frequency of class in the training set.\nIn the other hand, in likelihood ratio form, we suppose that $$ P(C=+\\mid X) = \\frac{P(C=+)P(X \\mid C=+)}{P(X)} $$\n$$ P(C=-\\mid X) = \\frac{P(C=-)P(X \\mid C=-)}{P(X)} $$\nfor two classes, $C=+$ and $C=-$\nAnd $X$ is classifed as the class $C=+$ if and only if $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} \\ge 1 $$\nMoreover, $$ f_b(X) = \\frac{P(C=+\\mid X)}{P(C=-\\mid X)} = \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} $$\nwhere $f_b(X)$ is called a Bayesian classifier.\nAs we know that $$ P(X \\mid y) = \\prod_{i=1}^nP(x_i \\mid y) $$\nFinally, we have $$ \\begin{aligned} f_{nb}(X) \u0026amp;= \\frac{P(C=+)P(X\\mid C=+)}{P(C=-)P(X\\mid C=-)} \\\\ \u0026amp;= \\frac{P(C=+)\\prod_{i=1}^nP(x_i \\mid C=+)}{P(C=-)\\prod_{i=1}^nP(x_i \\mid C=-)} \\end{aligned} $$\nif $$ f_{nb}(X) \\ge1 \\Rightarrow predict\\ as\\ class +1 $$\n$$ f_{nb}(X) \u0026lt;1 \\Rightarrow predict\\ as\\ class -1 $$\nwhere $f_{nb}(X)$ is called a naive Bayesian classifier, or simply naive Bayes (NB).\nFigure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.[1] ğŸ¤´ Gaussian Naive Bayes When dealing with continuous data, a typical supposition is that the continuous values correlating with every class are distributed acceding to Gaussian distribution. The training data are splitted by class and the mean and stander deviation of every class is calculated. Therefore for estimating the probabilities of continuous data set the following equation can be [2]\n$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}) $$\nwhere\n$\\mu_y$: the mean of the $i$-th feature under class $y$ $\\sigma_y$: the variance of the $i$-th feature under class $y$ For the new data set $X\u0026rsquo;_j$, we use this equation to calculate $$ P(y \\mid X\u0026rsquo;j) \\propto P(y)\\prod{j=1}^nP(x_j \\mid y) $$\nğŸ”§ Modeling with Gaussian Naive Bayes import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, confusion_matrix data = load_iris() X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) gnb = GaussianNB() model = gnb.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) ----------------------------------------- 0.9777777777777777 [[19 0 0] [ 0 12 1] [ 0 0 13]] ğŸ“– Reference [1] Zhang, H. (2004). The optimality of naive Bayes. Aa, 1(2), 3.\n[2] Kamel, H., Abdulah, D., \u0026amp; Al-Tuwaijari, J. M. (2019, June). Cancer classification using gaussian naive bayes algorithm. In 2019 international engineering conference (IEC) (pp. 165-170). IEEE.\n[3] Scikit-learn\n[4] è²æ°åˆ†é¡å™¨(Naive Bayes Classifier)(å«pythonå¯¦ä½œ), Roger Yong, 2021\n","permalink":"http://localhost:1313/posts/20250321_naivebayes/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-naive-bayes\"\u003eğŸ‘¶ Naive Bayes\u003c/h3\u003e\n\u003cp\u003eBy definition of Bayes\u0026rsquo; theorem\n$$\nP(y \\mid x_1, x_2, \u0026hellip;, x_n) = \\frac{P(y)P(x_1, x_2, \u0026hellip;, x_n \\mid y)}{P(x_1, x_2, \u0026hellip;, x_n)}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(y)$ represents the prior probability of class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n \\mid y)$ represents the likelihood, i.e., the probability of observing features $x_1, x_2, \u0026hellip;, x_n$ given class $y$\u003c/li\u003e\n\u003cli\u003e$P(x_1, x_2, \u0026hellip;, x_n)$ represents the marginal probability of the feature set $x_1, x_2, \u0026hellip;, x_n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith the assumption of Naive Bayes - \u003cstrong\u003eConditional Independence\u003c/strong\u003e\u003cbr\u003e\n$$\nP(x_i \\mid y, x_1, \u0026hellip;, x_{i-1}, x_{i+1}, \u0026hellip;, x_n) = P(x_i \\mid y)\n$$\u003c/p\u003e","title":"Naive \u0026 Gaussian Bayes Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸ¤” What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\nğŸ§ What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. ğŸ˜® How the tree do the split? 1ï¸âƒ£ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video ğŸ‘‡\nğŸ”¥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh StarmerğŸ”¥ 2ï¸âƒ£ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3ï¸âƒ£ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\nğŸ”‘ Key Takeaways â¤ï¸ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\nğŸ§¡ Continue splitting recursively until stopping conditions are met.\nğŸ’› Consider pruning to prevent overfitting and improve generalization.\nğŸ“– Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]æ±ºç­–æ¨¹ (Decision tree), 10ç¨‹å¼ä¸­ [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/20250318_decisiontrees/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003eğŸ¤” What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) å·²çŸ¥ï¼š $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\nè¨ˆç®—ï¼š\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\nå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\nå› æ­¤ï¼š $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\nå¯çŸ¥ $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i $$\næ‰€ä»¥ $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) è¨ˆç®—è®Šç•°æ•¸ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\nå› ç‚º $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\nå·²çŸ¥ $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\næ‰€ä»¥ç•¶ $n \\to \\infty$ æ™‚ $$Var(\\hat{I}_M) \\to 0$$\nå› æ­¤ $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\nå³ $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) æˆ‘å€‘è¨ˆç®— $\\hat{I}_M$çš„è®Šç•°æ•¸ï¼Œç”±(2)å¯çŸ¥ $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\nå…¶ä¸­ $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\nç•¶ $p_1(x)=1$ æ™‚ï¼Œ$x \\in (0, 1)$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\nç•¶ $p_2(x)=\\frac{2}{3}x^{-1/3}$ æ™‚ï¼Œ$x \\in (0, 1]$ï¼Œå‰‡ $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\nå› æ­¤ $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ ç”±ä¸Šè¿°å¯çŸ¥ï¼Œ $p_2(x)$ æœƒä½¿è®Šç•°æ•¸è®Šå°\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) ç‚ºäº†è®“ $g(x)$ åŒ…å« $f(x)$ï¼Œæˆ‘å€‘é¸æ“‡ä¸€å€‹å¸¸æ•¸ $\\alpha$ä½¿å¾— $$e(x)=\\alpha g(x) \\ge f(x),ã€€\\forall x$$\nè€Œæˆ‘å€‘å®šç¾©éš¨æ©Ÿè®Šæ•¸ $N$ ç‚ºæˆåŠŸç”¢ç”Ÿä¸€å€‹æ¨£æœ¬ $X,ã€€(X\\in g(x))$ æ‰€éœ€çš„å˜—è©¦æ¬¡æ•¸ï¼Œæ„å³\nå‰ $N-1$ æ¬¡å¤±æ•—ï¼Œå‰‡ $U \u0026gt; f(x)/\\alpha g(X)$ ç¬¬ $N$ æ¬¡æˆåŠŸï¼Œå‰‡ $U \\le f(x)/\\alpha g(X)$ é€™è¡¨ç¤º $$ P(N=k)=P(å‰k-1æ¬¡å¤±æ•—) *P(ç¬¬kæ¬¡æˆåŠŸ)$$\nå› æ­¤ï¼Œå°æ–¼ä»»æ„ä¸€æ¬¡å˜—è©¦ï¼ŒæˆåŠŸçš„æ©Ÿç‡ç‚º $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\nç”±æ­¤å¯çŸ¥æ¯æ¬¡å˜—è©¦æˆåŠŸçš„æ©Ÿç‡ç‚º $\\frac{1}{\\alpha}$ï¼Œè€Œå¤±æ•—çš„æ©Ÿç‡ç‚º $1-p = 1-\\frac{1}{\\alpha}$\næœ€å¾Œè¨ˆç®— $P(N=k)$ï¼Œå³å‰ $k-1$ æ¬¡å¤±æ•—ï¼Œç¬¬ $k$ æ¬¡æˆåŠŸçš„æ©Ÿç‡ $$P(N=k)=(1-p)^{k-1}p$$\nä»£å…¥ $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\nå¯å¾— $$ N \\sim Geo(p)$$\n(3-d) ç”±å¹¾ä½•åˆ†å¸ƒçš„ p.m.f. å¯çŸ¥ $$P(n=K) = (1-p)^{k-1}p,ã€€k=1, 2, 3,\u0026hellip;$$ è¨ˆç®—æœŸæœ›å€¼ $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\nä»£å…¥ $p=\\frac{1}{\\alpha}$ å¯å¾— $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/20250318_%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003eå·²çŸ¥ï¼š\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003eè¨ˆç®—ï¼š\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003eå°æ–¼æ¯å€‹ç¨ç«‹çš„ $X_i$ ï¼Œæˆ‘å€‘åªè¦è¨ˆç®—ï¼š\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003eå› æ­¤ï¼š\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003eå¯çŸ¥\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,ã€€\\forall i\n$$\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003eè¨ˆç®—è®Šç•°æ•¸\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003eå› ç‚º\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eå·²çŸ¥\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint ä¸­è­¯:\n$L$ æ˜¯è©²å‡½æ•¸(ç³»çµ±)ä¸€å€‹æœ€å¤§ä¸Šé™ï¼Œç•¶ $x \\to 0$ æ™‚ï¼Œå‰‡ $ f(x) \\to L$ $k$ è©²æ›²ç·šçš„é™¡å³­ç¨‹åº¦ï¼Œ$k$ æ„ˆå°å‰‡åˆ†æ¯æ„ˆå¤§ï¼Œæ•´å€‹ $f(x)$æ„ˆå°ï¼Œè¡¨ç¤ºä¸­é–“è®ŠåŒ–é€Ÿåº¦ç·©æ…¢ï¼›åä¹‹å‰‡ä¸­é–“è®ŠåŒ–é€Ÿåº¦å¿« $x_0$ æ›²ç·šç•¶ä¸­è®ŠåŒ–é€Ÿåº¦æœ€å¿«çš„ä¸€é» æˆ‘å€‘å¯ä»¥ç°¡åŒ–è©²æ–¹ç¨‹å¼ï¼š\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)ã€€\\text{ifã€€} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)ã€€\\text{ifã€€} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\nğŸ”§ Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 ğŸ”¨ Modleing with statsmodels.Logit note:\nå› ç‚ºä½¿ç”¨è©²æ¨¡å‹å­˜åœ¨betaä¸æ”¶æ–‚çš„å•é¡Œ\næ‰€ä»¥åœ¨fitçš„éƒ¨åˆ†é¸æ“‡ä½¿ç”¨fit_regularized\nå…¶ä¸­methodè¨­å®šåŠ å…¥l1æ‡²ç½°, alpha(l1çš„æ¬Šé‡)è¨­0.01\nsummayæ‰æœƒæ­£å¸¸è·‘å‡ºæ•¸å€¼\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/20250311_logisticregression/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD ğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦ ç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\næ–¹æ³•:\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\næ¼”ç®—æ³•: éš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\n","permalink":"http://localhost:1313/posts/20250305_randomforest/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch3 id=\"-éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\"\u003eğŸŒ³ éš¨æ©Ÿæ£®æ—åŸºæœ¬æ¦‚è¦\u003c/h3\u003e\n\u003cp\u003eç”±å¤šæ£µæ±ºç­–æ¨¹èšé›†è€Œæˆçš„æ£®æ—\u003cbr\u003e\næ–¹æ³•:\u003cbr\u003e\nå¾åŸå§‹è³‡æ–™ä¸­ä»¥å–å¾Œæ”¾å›çš„æ–¹å¼æŠ½å–è³‡æ–™ï¼Œå»ºç«‹æ¯æ£µæ±ºç­–æ¨¹çš„è¨“ç·´è³‡æ–™(training datasets)\u003cbr\u003e\nå› æ­¤æœ‰äº›æ¨£æœ¬æœƒè¢«é‡è¤‡é¸ä¸­ï¼Œé€™æ¨£çš„æŠ½æ¨£æ³•åˆç¨±ç‚ºBoostraping(æ‹”é´æ³•)\u003cbr\u003e\nä½†æ˜¯ç•¶åŸå§‹è³‡æ–™çš„æ•¸æ“šé¾å¤§æ™‚ï¼Œæœƒç™¼ç¾åœ¨æŠ½æ¨£å®Œç•¢å¾Œï¼Œæœ‰äº›æ¨£æœ¬ä¸¦æ²’æœ‰è¢«æŠ½å–åˆ°\u003cbr\u003e\nè€Œé€™äº›æ¨£æœ¬å°±è¢«ç¨±ç‚ºOut of Bag(OOB)è³‡æ–™(è¢‹å¤–)\u003cbr\u003e\né™¤äº†ä¸Šè¿°è¨“ç·´è³‡æ–™æ˜¯è¢«é‡è¤‡æŠ½å–ä¹‹å¤–ï¼Œç‰¹å¾µä¹Ÿæ˜¯å¦‚æ­¤\u003cbr\u003e\néš¨æ©Ÿæ£®æ—ä¸¦ä¸æœƒå°‡æ‰€æœ‰ç‰¹å¾µä¸€èµ·è€ƒæ…®ï¼Œè€Œæ˜¯æœƒéš¨æ©ŸæŠ½å–ç‰¹å¾µ(å¯è¨­å®šåƒæ•¸max_features)\u003cbr\u003e\né€²è¡Œæ¯æ£µæ¨¹çš„è¨“ç·´ï¼Œä»¥ä¸Šè¿°å…©ç¨®æ–¹å¼ä¾†é”åˆ°æ¯æ£µæ¨¹è¿‘ä¹ç¨ç«‹çš„æƒ…æ³ï¼Œç›®çš„æ˜¯é™ä½æ¯æ£µæ¨¹ä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œ\u003cbr\u003e\nå„ªé»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆ(Overfitting)çš„æƒ…æ³ç™¼ç”Ÿã€å¢é€²é æ¸¬ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦\u003cbr\u003e\næ¼”ç®—æ³•:\néš¨æ©Ÿæ£®æ—çš„æ¼”ç®—æ³•èˆ‡æ±ºç­–æ¨¹çš„æ¼”ç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼Œå·®åˆ¥åªæ˜¯åœ¨å»ºç«‹æ¨¹çš„æ–¹æ³•ä¸åŒè€Œå·²(å¦‚ä¸Šè¿°)\u003cbr\u003e\næ„å³ç•¶æ‡‰ç”¨åœ¨åˆ†é¡å•é¡Œæ™‚ï¼Œå‰‡æ¡ç”¨å‰å°¼ä¸ç´”åº¦(Gini Impurity)æˆ–æ˜¯é‘(Entropy)æ¼”ç®—æ³•ä½œç‚ºåˆ†é¡ä¾æ“š\u003cbr\u003e\nç•¶æ‡‰ç”¨åœ¨è¿´æ­¸å•é¡Œæ™‚ï¼Œé€šå¸¸æ¡ç”¨æœ€å°å¹³æ–¹èª¤å·®(MSE)(å…¶ä»–é‚„æœ‰åœæ°Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD é€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/20250204_principalcomponentanalysis/","summary":"\u003ch4 id=\"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–xd\"\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/h4\u003e\n\u003ch4 id=\"é€™ç¯‡æ–‡ç« åˆ©ç”¨-chat-gpt-ç¿»è­¯æˆè‹±æ–‡é‚Šå”¸é‚Šæ‰“ç´”æ‰‹æ‰“ç„¡è¤‡è£½è²¼ä¸Šé †ä¾¿ç·´è‹±æ–‡-xd\"\u003eé€™ç¯‡æ–‡ç« åˆ©ç”¨ Chat Gpt ç¿»è­¯æˆè‹±æ–‡ï¼Œé‚Šå”¸é‚Šæ‰“ï¼ˆç´”æ‰‹æ‰“ï¼Œç„¡è¤‡è£½è²¼ä¸Šï¼‰ï¼Œé †ä¾¿ç·´è‹±æ–‡ XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nTokenizing by n-gram (n-gram åˆ†è©) å°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-åˆ†è©\"\u003eTokenizing by n-gram (n-gram åˆ†è©)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eå°‡æ–‡æª”çš„å…§å®¹ä¾ç…§ n å€‹å–®è©ä½œåˆ†é¡ï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\næˆ‘å€‘å¯ä»¥åˆ©ç”¨å‡½å¼ tokenize_ngrams() ä¾ç…§ n = 2 åˆ†é¡ç‚º\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;ã€\u0026ldquo;bank is\u0026rdquo;ã€\u0026ldquo;is a\u0026rdquo;ã€\u0026ldquo;a place\u0026rdquo;ã€\u0026ldquo;place where\u0026rdquo;ã€\u0026ldquo;where you\u0026rdquo;ã€\u0026ldquo;you put\u0026rdquo;ã€\u0026ldquo;put your\u0026rdquo;ã€\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;ã€\u0026ldquo;bee is\u0026rdquo;ã€\u0026ldquo;is an\u0026rdquo;ã€\u0026ldquo;an insect\u0026rdquo;ã€\u0026ldquo;insect that\u0026rdquo;ã€\u0026ldquo;that gathers\u0026rdquo;ã€\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡ Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚ ä¸¦å®šç¾©ç‚ºï¼š $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ å…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ è‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚ å–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚ tf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\nZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹) ä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$ å‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€ è‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/20250124_textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\nè©é »: å–®è©å‡ºç¾åœ¨æ–‡æª”çš„é »ç‡\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\né€†æ–‡æª”é »ç‡: æ–‡æª”ä¸­å‡ºç¾æ„ˆå¤šæ¬¡çš„å–®è©ï¼Œå…¶æ¬Šé‡æ„ˆä½ï¼›åä¹‹å‰‡æ„ˆä½ã€‚å³è¡¡é‡æŸè©åœ¨æ‰€æœ‰æ–‡æª”ä¸­åˆ†ä½ˆçš„ç¨€ç–ç¨‹åº¦ï¼Œæ˜¯ä¸€ç¨®æ‡²ç½°é … ã€‚\nä¸¦å®šç¾©ç‚ºï¼š\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\nå…¶ä¸­åˆ†å­$n_{documents}$æ˜¯æ–‡æª”ç¸½æ•¸ï¼Œåˆ†æ¯$n_{documents containing term}$æ˜¯åŒ…å«è©²è©çš„æ–‡æª”ç¸½æ•¸ï¼Œ\nè‹¥æŸå–®è©å‡ºç¾åœ¨æ–‡æª”çš„æ¬¡æ•¸å°‘ï¼Œè¡¨ç¤ºåˆ†æ¯å°ï¼Œå…¶ IDF å¤§ï¼Œé‡è¦æ€§é«˜ï¼Œæ¬Šé‡é«˜ï¼›åä¹‹å‡ºç¾çš„æ¬¡æ•¸å¤šï¼Œè¡¨ç¤ºåˆ†æ¯å¤§ï¼Œå…¶ IDF å°ï¼Œé‡è¦æ€§ä½ï¼Œæ¬Šé‡ä½ã€‚\nå–å°æ•¸å‰‡æ˜¯ç‚ºäº†æ¸›å°‘æ¥µç«¯æ•¸å€¼ï¼Œä½¿ IDF åˆ†ä½ˆæ›´åŠ å¹³æ»‘ã€‚\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idfçš„ç›®æ¨™æ˜¯ç”¨æ–¼è­˜åˆ¥åœ¨å–®ä¸€æ–‡æª”ä¸­æœ‰åƒ¹å€¼ã€ä½†ä¸å¸¸è¦‹çš„å–®è©ï¼ˆè©å½™ï¼‰\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--é½Šå¤«å®šå¾‹\"\u003eZipf\u0026rsquo;s law ( é½Šå¤«å®šå¾‹)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eä¸€ç¨®æè¿°è‡ªç„¶èªè¨€ä¸­å–®è©ä½¿ç”¨é »ç‡åˆ†ä½ˆçš„çµ±è¨ˆè¦å¾‹ï¼Œå…¶å®£ç¨±å–®è©çš„é »ç‡èˆ‡å…¶åœ¨æ–‡æª”è£¡çš„é »ç‡æ’åæˆåæ¯”ï¼Œå³ï¼š$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003eå‡ºç¾é »ç‡ç¬¬ä¸€åçš„å–®è©çš„æ¬¡æ•¸æœƒæ˜¯å‡ºç¾é »ç‡ç¬¬äºŒåçš„å–®è©çš„æ¬¡æ•¸çš„å…©å€ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ä¸‰åçš„å–®è©çš„æ¬¡æ•¸çš„ä¸‰å€\u0026hellip;ä¾æ­¤é¡æ¨ï¼Œæœƒæ˜¯å‡ºç¾é »ç‡ç¬¬ N åçš„å–®è©çš„æ¬¡æ•¸çš„ N å€\u003c/li\u003e\n\u003cli\u003eè‹¥å°‡æ’å x èˆ‡å‡ºç¾é »ç‡ y å–å°æ•¸ï¼Œå³ logx ã€ logy ï¼Œè‹¥æ•´å€‹æ–‡æª”çš„åæ¨™é»æ¨™å‡ºä¾†æ¥è¿‘ä¸€ç›´ç·šï¼Œå‰‡è©²æ–‡æª”ç¬¦åˆ Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"é€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\nBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³• ã€ŒHierarchical modeling is used with information is available on several different levels of observational unitsã€\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\nå¼•è‡ªç¶­åŸºç™¾ç§‘\nå…¬å¼ç†è«– Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\nå…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\n(2) Hyperdisrtibution: distribution of hyperparameters\nè¶…åƒæ•¸çš„åˆ†é… ä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\nç¬¬ $j$ æ‰€å­¸æ ¡çš„å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ï¼š$y_j $ï¼ˆçœŸå¯¦æ•¸æ“šï¼‰ ç¬¬ $j$ æ‰€å­¸æ ¡çš„æ•´é«”æ¸¬é©—å¹³å‡æˆç¸¾ï¼š$\\theta_j $ å…¨é«”å­¸æ ¡çš„ç¾¤é«”åˆ†é…è¶…åƒæ•¸ï¼š$\\phi$ ï¼ˆæè¿°å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¾ç…§éå¾€æ•¸æ“šå‡è¨­ï¼‰ è€Œæ¨¡å‹åˆ†ç‚ºä¸‰å€‹å±¤ç´š\nç¬¬ä¸‰å±¤ç´šï¼ˆé ‚å±¤ï¼‰ è¶…åƒæ•¸ç”Ÿæˆ-è™•ç†å…¨é«”çš„ä¸ç¢ºå®šæ€§\n$$\\phi \\sim P(\\phi)$$ ç”¨æ–¼å®šç¾©æ•´é«”çš„åˆ†ä½ˆï¼Œå‰‡æˆ‘å€‘å‡è¨­è¶…åƒæ•¸ $\\phiï¼ˆ\\muï¼‰$ ä¾†è‡ªå…ˆé©—åˆ†é…ç‚ºï¼š $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ è¡¨ç¤ºå…¨é«”ç¾¤é«”çš„ä¸­å¿ƒè¶¨å‹¢æ˜¯å¦‚ä½•åˆ†ä½ˆçš„ï¼Œå…¶åƒæ•¸ $\\mu_0,\\sigma^2_0$ é€šå¸¸æ˜¯ä¾†è‡ªæ–¼éå»çš„æ­·å²æ•¸æ“šè€Œè¨‚ï¼Œ åœ¨é€™è£¡çš„ä¾‹å­å°±æ˜¯å®šç¾©å…¨é«”å­¸æ ¡çš„æ¸¬é©—æˆç¸¾å¯èƒ½è·Ÿå»éå¾€çš„æ•¸æ“šåšå‡è¨­ï¼Œ ä¾‹å¦‚æˆ‘å€‘å‡è¨­æ•´é«”çš„å¹³å‡æ¸¬é©—æˆç¸¾ $\\mu_0 = 60 $ï¼Œ$\\sigma^2_0 = 5$\nç¬¬äºŒå±¤ç´šï¼ˆä¸­é–“å±¤ï¼‰ åƒæ•¸ç”Ÿæˆ-è§£é‡‹æ•¸æ“šçš„ä¾†æº\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ é€™å±¤çš„ç›®çš„æ˜¯è€ƒæ…®å­¸æ ¡ä¹‹é–“çš„ç•°è³ªæ€§ï¼Œä¸¦å‡è¨­å­¸æ ¡çš„å¹³å‡æ¸¬é©—æˆç¸¾ä¾†è‡ªæŸå€‹æ•´é«”çš„åˆ†ä½ˆï¼Œ å‰‡æˆ‘å€‘å‡è¨­æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ å…¶ä¸­ $\\mu$ æ˜¯æ•´é«”å­¸æ ¡çš„ç¸½æ¸¬é©—å¹³å‡ï¼Œè€Œ $\\theta_j$ æ˜¯æ¯å€‹å­¸æ ¡çš„æ¸¬é©—å¹³å‡æˆç¸¾\nç¬¬ä¸€å±¤ç´šï¼ˆåº•å±¤ï¼‰ æ•¸æ“šç”Ÿæˆ-é‚„åŸçœŸå¯¦æ•¸æ“š\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\nå¯ä»¥çŸ¥é“çœŸå¯¦æ•¸æ“š $y_i$ ä¸¦ä¸æ˜¯éš¨æ©Ÿç”¢ç”Ÿï¼Œè€Œæ˜¯åœ¨è©²çœŸå¯¦æ•¸æ“šæ‰€åœ¨çš„æ•´é«”å¹³å‡å€¼èˆ‡è®Šç•°æ•¸å—å½±éŸ¿çš„ï¼Œä¾‹å¦‚é€™è£¡çš„ä¾‹å­ï¼Œ æˆ‘å€‘å¯ä»¥å‡è¨­æ¯å€‹å­¸ç”Ÿçš„æ¸¬é©—æˆç¸¾ $y_i$ ä¾†è‡ªå¸¸æ…‹åˆ†é…ï¼Œå³$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ æ˜¯ç”±æ–¼å­¸æ ¡çš„å¹³å‡æˆç¸¾ $\\theta_j$ å’Œ å­¸ç”Ÿä¹‹é–“çš„å·®ç•° $\\sigma^2$ å½±éŸ¿çš„\né€šéHierarchical modelsï¼Œæˆ‘å€‘å¯ä»¥åŒæ™‚ä¼°è¨ˆå­¸æ ¡çš„ç‰¹å¾µï¼ˆå¦‚ $\\theta_j$ï¼‰å’Œæ•´é«”ç‰¹å¾µï¼ˆå¦‚ $\\phi$ï¼‰ï¼Œä¸¦ä¸”èƒ½æœ‰æ•ˆè™•ç†ä¸åŒå±¤æ¬¡çš„ä¸ç¢ºå®šæ€§\n","permalink":"http://localhost:1313/posts/20250113_%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦è‡ªå·±çš„ä¸€ä»½å­¸ç¿’ç´€éŒ„ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜é€™æ˜¯ç”šéº¼ç†è«–XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modelingï¼Œè­¯ç‚ºã€Œè²æ°åˆ†å±¤å»ºæ¨¡ã€\u003cbr\u003e\né‡å°å¤šå€‹å±¤ç´šåˆ†æçš„ä¸€å¥—çµ±è¨ˆæ–¹æ³•\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eã€ŒHierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003eã€\u003cbr\u003e\nå¯ä»¥å°å¤šå€‹ä¸åŒå±¤ç´šçš„è§€æ¸¬å–®ä½çš„è³‡è¨Šé€²è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼š\u003cbr\u003e\n\u0026ndash; æ¯å€‹åœ‹å®¶çš„æ¯æ—¥æ„ŸæŸ“ç—…ä¾‹çš„æ™‚é–“æ¦‚æ³ï¼Œå–®ä½æ˜¯åœ‹å®¶\u003cbr\u003e\n\u0026ndash; å¤šå€‹æ²¹äº•ç”¢é‡çš„éæ¸›æ›²ç·šåˆ†æï¼ˆæ²¹æ°£ç”¢é‡ï¼‰ï¼Œå–®ä½æ˜¯æ²¹è—å€çš„æ²¹äº•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eã€€\u003cstrong\u003eå¼•è‡ªç¶­åŸºç™¾ç§‘\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"å…¬å¼ç†è«–\"\u003eå…¬å¼ç†è«–\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\nã€€ å…ˆé©—åˆ†é…çš„åƒæ•¸ï¼ˆè¶…åƒæ•¸ï¼è¶…æ¯æ•¸ï¼‰\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\nã€€ è¶…åƒæ•¸çš„åˆ†é…\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eä¾‹å¦‚ï¼šæˆ‘å€‘éœ€è¦å»ºæ¨¡å­¸æ ¡çš„å­¸ç”Ÿæ¸¬é©—æˆç¸¾\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"é€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\næ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\næˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\nimport requests as req\r# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€ from bs4 import BeautifulSoup as B\r# ç´¢å–ç¶²å€å…§å®¹ import pandas as pd\r# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\rimport time\r# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\rimport random\r# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–® header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\rresp = req.get(builder_url, headers=header)\r# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\rcocktail_name_list = []\r# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\rif resp.status_code == 200:\r# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r# ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\rname = cocktail_name.text.strip()\r# èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\rif name:\r# ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\rcocktail_name_list.append(name)\r# æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\rtime.sleep(random.uniform(1,3))\r# æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\rcocktail_data = []\r# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r# xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ ä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\næœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\né›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\n","permalink":"http://localhost:1313/posts/20250110_%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003eé€™æ˜¯çµ¦æˆ‘è‡ªå·±çš„ä¸€ä»½æ•™å­¸ï¼Œä»¥å…æ—¥å­ä¹…äº†å¿˜è¨˜æ€éº¼çˆ¬èŸ²ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ç¯‡å­¸ç¿’ç´€éŒ„\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæ“æœ‰ä¸€å€‹å¯ä»¥å¯«codeçš„ç’°å¢ƒï¼Œç¶²è·¯ä¸Šå¾ˆå¤šå®‰è£ç’°å¢ƒçš„æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæˆ‘æ˜¯ç”¨anocondaçš„vs codeå¯«codeçš„\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# å‘å®¢æˆ¶ç«¯è¦æ±‚ç¶²å€  \r\nfrom bs4 import BeautifulSoup as B\r\n# ç´¢å–ç¶²å€å…§å®¹  \r\nimport pandas as pd\r\n# æœ€å¾Œå°‡çµæœè¼¸å‡ºç‚ºCSVæª”\r\nimport time\r\n# é¿å…çˆ¬èŸ²æ™‚è¢«æŠ“åˆ°æ˜¯çˆ¬èŸ²ï¼Œæ‰€ä»¥ç§»ç”¨é€™å€‹å»¶é•·æ¯æ¬¡çˆ¬èŸ²æ™‚é–“ï¼Œå‡è£è‡ªå·±æ˜¯äººé¡\r\nimport random\r\n# å»¶é²éš¨æ©Ÿæ™‚é–“ç”¨çš„\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# æˆ‘æ˜¯ç”¨ **cocktail builder** é€™å€‹ç¶²ç«™ä¾†çˆ¬èŸ²æˆ‘è¦çš„é…’å–®  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# èµ·åˆåœ¨çˆ¬çš„æ™‚å€™æœ‰ç™¼ç¾è·‘ä¸å‡ºè³‡æ–™ï¼Œæ‰€ä»¥æ–°å¢headerä¾†å‡è£æˆ‘æ˜¯äººé¡ï¼Œç¶²è·¯ä¸Šä¹Ÿæœ‰å¾ˆå¤šå¦‚ä½•åœ¨ç€è¦½å™¨æ‰¾headerçš„æ•™å­¸\r\nresp = req.get(builder_url, headers=header)\r\n# å»ºç«‹æŠ“å–urlçš„å›æ‡‰è®Šæ•¸\r\ncocktail_name_list = []\r\n# å»ºç«‹ç©ºæ¸…å–®ï¼Œå°‡æŠ“å–åˆ°çš„è³‡æ–™å…ˆæ”¾é€²ä¾†ï¼Œä»¥ä¾¿æœ€å¾Œåšæˆdataframe\r\nif resp.status_code == 200:\r\n# ç¢ºå®šä½ çš„urlæ˜¯å¯ä»¥é€£ç·šçš„\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # å»ºç«‹çˆ¬èŸ²çš„é ­é ­ï¼Œå¾Œé¢çš„html.parseræ˜¯æ¯æ¬¡å»ºç«‹éƒ½è¦æœ‰ï¼Œå¥½åƒæ˜¯ç”¨ä¾†è§£æhtmlçš„åŠŸèƒ½\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # æ‰“é–‹ç¶²é æŒ‰ä¸‹F2ï¼ŒæŒ‰ä¸‹ctrl+shift+cé€²å…¥é¸å–ç¶²é å…ƒç´ æ¨¡å¼ï¼Œé»é¸ä»»ä¸€èª¿é…’ï¼ŒæœƒæŒ‡å¼•åˆ°è©²èª¿é…’çš„block\r\n    # ä½ æœƒç™¼ç¾æ‰€æœ‰çš„èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;é€™å€‹æ¨™ç±¤åº•ä¸‹ï¼Œæ‰€ä»¥æˆ‘å€‘åˆ©ç”¨find_alléæ­·è©²æ¨™ç±¤\r\n        name = cocktail_name.text.strip()\r\n        # èª¿é…’åç¨±éƒ½åœ¨\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;çš„æ¨™ç±¤çš„textå…§å®¹ä¸­ï¼Œstrip()æ˜¯å»æ‰textå‰å¾Œå¤šé¤˜çš„ç©ºæ ¼\r\n        if name:\r\n        # ç¢ºå®šè©²æ¨™ç±¤æœ‰å…§å®¹æ‰æŠ“ï¼Œæ²’æœ‰å°±ä¸æŠ“\r\n            cocktail_name_list.append(name)\r\n            # æŠ“åˆ°ä¿®é£¾å¾Œçš„textä¸¦æ”¾å…¥å‰›å‰›å»ºç«‹çš„list\r\n    time.sleep(random.uniform(1,3))\r\n    # æ¯æ¬¡æŠ“å®Œä¸€å€‹å°±ç­‰å¾… 1~3 ç§’\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# å°‡æŠ“å®Œå¾Œçš„æ¸…listå»ºç«‹æˆä¸€å€‹Dataframeï¼Œä»¥Nameç•¶ä½œè©²æ¬„ä½çš„åç¨±\r\n\r\ncocktail_data = []\r\n# é€™æ˜¯æœ€å¾Œçš„èª¿é…’åç¨±+è©²èª¿é…’çš„ææ–™çš„ç©ºæ¸…å–®\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # å»ºç«‹æ¯å€‹èª¿é…’çš„urlï¼Œé»é€²å»éš¨ä¾¿ä¸€å€‹èª¿é…’ï¼Œä½ æœƒç™¼ç¾ç¶²å€æ˜¯https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxxæ˜¯è©²èª¿é…’çš„åç¨±ï¼Œåªæ˜¯æˆ‘å€‘å‰›å‰›çš„èª¿é…’åç¨±listæœ‰å¤§å¯«è€Œä¸”ä¸­é–“æ˜¯ç©ºæ ¼ä¸æ˜¯-ï¼Œæ‰€ä»¥ç”¨replaceå°‡ç©ºæ ¼æ›¿æ›æˆ-ï¼Œä¸”è½‰ç‚ºå°å¯«\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # æŸ¥æ‰¾æ‰€æœ‰å…·æœ‰æŒ‡å®š class çš„ \u0026lt;span\u0026gt; å…ƒç´ \r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # æå–\u0026lt;a\u0026gt;æ¨™ç±¤çš„æ–‡å­—å…§å®¹\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # ç¢ºä¿\u0026lt;a\u0026gt;å­˜åœ¨\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # æœ€å¾Œå°±æ˜¯å°‡å‰›å‰›æŠ“åˆ°çš„Nameè·Ÿé€™å€‹Inredientsæ¸…å–®ä¸­æ¯å€‹ç´ æåŠ å…¥å‰›å‰›å»ºç«‹çš„åŠ å…¥å‰›å‰›å»ºç«‹çš„cocktail_dataæ¸…å–®ä¸­\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# æœ€å¾Œçš„æœ€å¾Œå°‡listè½‰ç‚ºDataframeä¸¦ç”¨pd.to_csvè¼¸å‡ºæˆcsvæª”ï¼ŒçµæŸé€™å›åˆ\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eä»¥ä¸Šæ˜¯æˆ‘æé†’è‡ªå·±çš„çˆ¬èŸ²æ•™å­¸\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eæœ‰ä»»ä½•ç–‘å•æ­¡è¿æå‡º\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003eé›–ç„¶æˆ‘é‚„æ²’æœ‰å»ºç«‹ç•™è¨€æ¿å°±æ˜¯äº†\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Yï½N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/20241004_leastsquareeestimator-of-beta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1ï½n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of Î² in linear regression"},{"content":"ç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\nåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\nå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\næ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\n","permalink":"http://localhost:1313/posts/002/","summary":"\u003cp\u003eç­è§£åˆ°HUGOæœ‰ä¸‰ç¨®èªæ³•\u003c/p\u003e\n\u003cp\u003eåˆ†åˆ¥æ˜¯ï¼šJSONã€TOMLã€YAML\u003c/p\u003e\n\u003cp\u003eå› ç‚ºTOMLçš„å…§å®¹æ ¼å¼é¡ä¼¼PYTHONçš„ï¼Œè€Œæˆ‘æœ¬äººä¹Ÿæ¯”è¼ƒç¿’æ…£PYTHONçš„èªæ³•\u003c/p\u003e\n\u003cp\u003eæ‰€ä»¥æ¡ç”¨TOMLçš„èªæ³•ï¼Œä¸¦å°‡å…¨éƒ¨çš„èªæ³•æ”¹ç‚ºè·ŸTOMLçš„\u003c/p\u003e","title":"My 2nd post"},{"content":"é–‹å­¸äº†!\né€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å° é€™æ˜¯æ–œé«”å­—\né€™æ˜¯ç²—é«”å­—\né€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\né€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\né€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\né€™æ˜¯ç·¨è™Ÿ1è™Ÿ\né€™æ˜¯ç·¨è™Ÿ2è™Ÿ\né€™æ˜¯é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ æˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ çœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ é€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ ","permalink":"http://localhost:1313/posts/001/","summary":"\u003cp\u003eé–‹å­¸äº†!\u003c/p\u003e\n\u003ch1 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸€è¡Œ\u003c/h1\u003e\n\u003ch2 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äºŒè¡Œ\u003c/h2\u003e\n\u003ch3 id=\"é€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬ä¸‰è¡Œ\u003c/h3\u003e\n\u003ch4 id=\"é€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬å››è¡Œ\u003c/h4\u003e\n\u003ch5 id=\"é€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\"\u003eé€™æ˜¯æˆ‘çš„ç¬¬äº”è¡Œ\u003c/h5\u003e\n\u003ch6 id=\"é€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\"\u003eé€™å€‹æ–‡å­—å¤§å°æœ€å¤šåˆ°ç¬¬å…­è¡Œé€™æ¨£çš„å¤§å°\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003eé€™æ˜¯æ–œé«”å­—\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eé€™æ˜¯ç²—é«”å­—\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eé€™æ˜¯æ–œé«”ä¸­çš„ç²—é«”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯ä¸åˆ†è¡Œçš„æ•¸å­¸èªæ³• $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003eé€™æ˜¯åˆ†è¡Œçš„æ•¸å­¸èªæ³• $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ1è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eé€™æ˜¯ç·¨è™Ÿ2è™Ÿ\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬ä¸€æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eé€™æ˜¯ç¬¬äºŒæ¬¡ç¸®ç‰Œçš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eæˆ‘ä¸çŸ¥é“é‚„æœ‰æ²’æœ‰ç¬¬ä¸‰æ¬¡ç¸®æ’çš„é …ç›®ç·¨è™Ÿ\n\u003cul\u003e\n\u003cli\u003eçœ‹ä¾†ç¬¬äºŒæ¬¡ç¸®æ’ä»¥å¾Œéƒ½æ˜¯ä¸€æ¨£çš„é …ç›®ç·¨è™Ÿ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ\u003c/th\u003e\n          \u003cth\u003eé€™æ˜¯ç¬¬ä¸€åˆ—ç¬¬äºŒè¡Œ\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬äºŒåˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬ä¸€è¡Œ\u003c/td\u003e\n          \u003ctd\u003eé€™æ˜¯ç¬¬ä¸‰åˆ—ç¬¬äºŒè¡Œ\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"}]