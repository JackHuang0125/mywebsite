[{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 🤔 What is decision tree? Decision tree is a system that relies on evaluating conditions as True or False to make decisions, such as in classification or regression.\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call it a classification tree; On the other hand, when the tree performs regression to predict a numerical value, we call it a regression tree.\n🧐 What are the components of a decision tree? Root node: It is the starting point for decision-making. Internal nodes: They are between the root and leaf nodes. Each node represents a decision based on a specific feature. Leaf Nodes: It is the final points of the tree that provide a output(class label in classification or number value in regression). Branches: Each time a splitting occurs, new branches are created. Splitting: The process of dividing a node into two or more sub-nodes based on a feature. Pruning: A technique used to remove unnecessary nodes to simplify the tree and prevent overfitting. 😮 How the tree do the split? 1️⃣ Check all the features and choose the best feature to be the root node. Both numerical and categorical features follow the same process - calculating the Gini impurity to determine the optimal split. Gini impurity is defined as: $$ Gini \\space Index(Impurity) = 1- \\sum^N_ip^2_i$$\nwhere\n$p_i$ represents the proportion of instances belonging to class $i$. $N$ is the total number of classes in the node. For each feature, possible split points are considered, and the feature with the minimum Gini impurity is chosen as the root node. Howeve, when evaluating split nodes, we do not only consider the Gini impurity of the result groups, but also their size. This is done using the weighted Gini impurity, which accounted for the proportin of instances in each child node. The weighted Gini impurity is defined as: $$ Gini_{split} = \\frac{N_L}{N}Gini_{L}+\\frac{N_R}{N}Gini_{R} $$ where\n$N_L$ and $N_R$ are the number of instances in the left and right child nodes. $N$ is the total number of instances in the parent node. $Gini_{L}$ and $Gini_{R}$ are the Gini impurity values for the left and right nodes.\nAlso, there are different ways to calculate Gini impurity for them . If you want to learn more. I recommend watching this video 👇\n🔥Decision and Classification Trees, Clearly Explained!!!, StatQuest with Josh Starmer🔥 2️⃣ After making sure the root node, we repeat the process to select internal nodes from other features by recalculating Gini impurity until one of the internal nodes can no longer be split, meaning its Gini impurity equals 0.\nThe splitting process continues until one of the following stopping conditions is met:\nGini impurity = 0, meaning all instances in a node belong to the same class. A predefined maximum depth is reached, to prevent overfitting. The number of instances in a node falls below a minimum threshold, ensuring statistical reliability. 3️⃣ Overfitting also happens when using decision tree because the tree will split again and again until one of the following stopping conditions is met like I mentioned earlier. Therefore, to avoid overfitting, decision trees may undergo pruning after training. Pruning removes unnecessary branches that do not significantly improve classification accuracy.\n🔑 Key Takeaways ❤️ Choose the root node by calculating Gini impurity for all features and selecting the split with the lowest weighted impurity.\n🧡 Continue splitting recursively until stopping conditions are met.\n💛 Consider pruning to prevent overfitting and improve generalization.\n📖 Reference [1] Scikit-learn\n[2] Decision and Classification Trees, StatQuest with Josh Starmer\n[3] [Day 12]決策樹 (Decision tree), 10程式中 [4] Wikipedia-Decision tree learning [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984\n","permalink":"http://localhost:1313/posts/decisionclassificationtrees/","summary":"\u003ch4 id=\"這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd\"\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/h4\u003e\n\u003ch3 id=\"-what-is-decision-tree\"\u003e🤔 What is decision tree?\u003c/h3\u003e\n\u003cp\u003eDecision tree is a system that relies on evaluating conditions as \u003cem\u003eTrue\u003c/em\u003e or \u003cem\u003eFalse\u003c/em\u003e to make decisions, such as in classification or regression.\u003cbr\u003e\nWhen the tree needs to classify something into class A or class B, or even into multiple classes (which is called multi-class classification), we call\nit a \u003cstrong\u003eclassification tree\u003c/strong\u003e; On the other hand, when the tree performs regression to predict a numerical value, we call it a \u003cstrong\u003eregression tree\u003c/strong\u003e.\u003c/p\u003e","title":"Decision \u0026 Classification Tree Learning"},{"content":"This is homework (1) 已知： $$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\n計算：\n$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\n對於每個獨立的 $X_i$ ，我們只要計算： $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\n因此： $$ E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I $$\n可知 $$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,　\\forall i $$\n所以 $$ E(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I $$\n(2) 計算變異數 $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\n因為 $$ \\begin{aligned} Var(\\widehat{I}_M) \u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right) = \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\ \u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) \\end{aligned} $$\n已知 $$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\n所以當 $n \\to \\infty$ 時 $$Var(\\hat{I}_M) \\to 0$$\n因此 $$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]\\to 0$$\n即 $$\\hat{I}_M \\overset{L^2}{\\to} 0$$\n(3-a) 我們計算 $\\hat{I}_M$的變異數，由(2)可知 $$ Var(\\hat{I}_M)=\\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right) $$\n其中 $$ \\tag{1} E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]=\\int^1_0\\frac{f(x)^2}{p(x)}dx $$\n$$ \\tag{2} I = E\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(X)}{p(X)}p(x)dx $$\n$$ \\Rightarrow I= \\int^1_0(x^{-1/3}+\\frac{x}{10})dx \\approx 1.55 $$\n當 $p_1(x)=1$ 時，$x \\in (0, 1)$，則 $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_1(X)}\\right)^2\\right] \u0026amp;=\\int^1_0f(x)^2dx \\\\ \u0026amp;=\\int^1_0(x^{-1/3}+\\frac{x}{10})^2dx \\\\ \u0026amp;\\approx 3.1233 \\end{aligned} $$\n因此 $$ Var(\\hat{I}_M)=\\frac{1}{n}(3.1233-1.55^2)=\\frac{0.7208}{n} $$\n當 $p_2(x)=\\frac{2}{3}x^{-1/3}$ 時，$x \\in (0, 1]$，則 $$ \\begin{aligned} E\\left[\\left(\\frac{f(X)}{p_2(X)}\\right)^2\\right] \u0026amp;=\\int^1_0\\frac{f(x)^2}{p_2(x)}dx \\\\ \u0026amp;=\\int^1_0\\frac{(x^{-1/3}+\\frac{x}{10})^2}{\\frac{2}{3}x^{-1/3}}dx \\\\ \u0026amp;= 2.4045 \\end{aligned} $$\n因此 $$ Var(\\hat{I}_M)=\\frac{1}{n}(2.4045-1.55^2)=\\frac{0.002}{n} $$ 由上述可知， $p_2(x)$ 會使變異數變小\nimport numpy as np\rdef f(x):\rreturn x**(-1/3) + x/10\rdef p1(n):\rreturn np.random.uniform(0, 1, n)\rdef p2(n):\ru = np.random.uniform(0, 1, n)\rreturn u**3\rdef monte_carlo_int(n, sample_f, p_f):\rX = sample_f(n)\rweights = f(X)/p_f(X)\rreturn np.mean(weights)\rn_samples = 5000\rn_test = 1000\restimate_p1 = np.zeros(n_test)\restimate_p2 = np.zeros(n_test)\rfor i in range(n_test):\restimate_p1[i] = monte_carlo_int(n_samples, p1, lambda x:1)\restimate_p2[i] = monte_carlo_int(n_samples, p2, lambda x: (2/3)*x**(-1/3))\rvar_p1 = np.var(estimate_p1, ddof=1)\rvar_p2 = np.var(estimate_p2, ddof=1)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p1(x) is: {var_p1: .6f}\u0026#39;)\rprint(f\u0026#39;The estimator variance by Monte-Carlo of p2(x) is: {var_p2: .6f}\u0026#39;) The estimator variance by Monte-Carlo of p1(x) is: 0.000139\rThe estimator variance by Monte-Carlo of p2(x) is: 0.000000 (3-c) 為了讓 $g(x)$ 包含 $f(x)$，我們選擇一個常數 $\\alpha$使得 $$e(x)=\\alpha g(x) \\ge f(x),　\\forall x$$\n而我們定義隨機變數 $N$ 為成功產生一個樣本 $X,　(X\\in g(x))$ 所需的嘗試次數，意即\n前 $N-1$ 次失敗，則 $U \u0026gt; f(x)/\\alpha g(X)$ 第 $N$ 次成功，則 $U \\le f(x)/\\alpha g(X)$ 這表示 $$ P(N=k)=P(前k-1次失敗) *P(第k次成功)$$\n因此，對於任意一次嘗試，成功的機率為 $$ p = \\int^{\\infty}_0g(x)\\frac{f(x)}{\\alpha g(x)}dx = \\frac{1}{\\alpha}\\int^{\\infty}_0f(x)dx =\\frac{1}{\\alpha} $$\n由此可知每次嘗試成功的機率為 $\\frac{1}{\\alpha}$，而失敗的機率為 $1-p = 1-\\frac{1}{\\alpha}$\n最後計算 $P(N=k)$，即前 $k-1$ 次失敗，第 $k$ 次成功的機率 $$P(N=k)=(1-p)^{k-1}p$$\n代入 $\\frac{1}{\\alpha}$ $$P(N=k)=\\left(1-\\frac{1}{\\alpha}\\right)^{k-1}\\frac{1}{\\alpha}$$\n可得 $$ N \\sim Geo(p)$$\n(3-d) 由幾何分布的 p.m.f. 可知 $$P(n=K) = (1-p)^{k-1}p,　k=1, 2, 3,\u0026hellip;$$ 計算期望值 $$ \\begin{aligned} E(N) \u0026amp;= \\sum^\\infty_{k=1}k (1-p)^{k-1}p = p\\sum^\\infty_{k=1}k (1-p)^{k-1} \\\\ \u0026amp;= p*\\frac{1}{p^2}= \\frac{1}{p} \\end{aligned} $$\n代入 $p=\\frac{1}{\\alpha}$ 可得 $$E(N)=\\alpha$$\n","permalink":"http://localhost:1313/posts/%E7%B5%B1%E8%A8%88%E8%A8%88%E7%AE%970320/","summary":"\u003ch4 id=\"this-is-homework\"\u003eThis is homework\u003c/h4\u003e\n\u003ch1 id=\"1\"\u003e(1)\u003c/h1\u003e\n\u003cp\u003e已知：\n$$X_1, X_2, \u0026hellip;, X_n \\overset{\\text{iid}}{\\sim}p(x)$$\u003c/p\u003e\n\u003cp\u003e計算：\u003c/p\u003e\n\u003cp\u003e$$ E( \\hat{I}_M)=E\\left[\\frac{1}{n} \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right]=\\frac{1}{n}E\\left[ \\sum^n_{i=1} \\frac{f(X_i)}{p(X_i)} \\right] $$\u003c/p\u003e\n\u003cp\u003e對於每個獨立的 $X_i$ ，我們只要計算：\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right]$$\u003c/p\u003e\n\u003cp\u003e因此：\n$$\nE\\left[\\frac{f(X)}{p(X)} \\right] = \\int^b_a\\frac{f(x)}{p(x)}p(x)dx =\\int^b_af(x)dx = I\n$$\u003c/p\u003e\n\u003cp\u003e可知\n$$E\\left[\\frac{f(X_i)}{p(X_i)} \\right] =I,　\\forall i\n$$\u003c/p\u003e\n\u003cp\u003e所以\n$$\nE(\\hat{I}_M) =\\frac{1}{n}\\sum^n_{i=1}I=I\n$$\u003c/p\u003e\n\u003ch1 id=\"2\"\u003e(2)\u003c/h1\u003e\n\u003cp\u003e計算變異數\n$$Var(\\hat{I}_M)=E\\left[(\\hat{I}_M-I)^2\\right]$$\u003c/p\u003e\n\u003cp\u003e因為\n$$\n\\begin{aligned}\nVar(\\widehat{I}_M)\n\u0026amp;= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(X_i)}{p(X_i)}\\right)\n= \\frac{1}{n}Var\\left(\\frac{f(X)}{p(X)}\\right) \\\\\n\u0026amp;= \\frac{1}{n}\\left(E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right]-I^2\\right)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003e已知\n$$E\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] \u0026lt; \\infty$$\u003c/p\u003e","title":"Statistical Computing HW_0320"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD Logistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier.\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\nAnd what is logistic function? Let talk about it. Here comes from Wikipedia:\nA logistic function or a logistic curve is a commond S-shaped curve (sigmoid curve) with the equation: $$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$ where:\n$L$ is the supremum of the values of the function $k$ is the logistic growth rate, the steepness of the curve $x_0$ is the $x$ value of the function\u0026rsquo;s midpoint 中譯:\n$L$ 是該函數(系統)一個最大上限，當 $x \\to 0$ 時，則 $ f(x) \\to L$ $k$ 該曲線的陡峭程度，$k$ 愈小則分母愈大，整個 $f(x)$愈小，表示中間變化速度緩慢；反之則中間變化速度快 $x_0$ 曲線當中變化速度最快的一點 我們可以簡化該方程式：\nThe standard logistic function, where $L=1, k=1, x_0=0$, has the euqation: $$ f(x) = \\frac{1}{1+e^{-x}}$$\nand is also called sigmoid function, and it looks like:\nWe can know that:\nWhen $x=0, f(0)= \\frac{1}{2}$ When $x=\\infty, f(\\infty)= 1$ When $x=-\\infty, f(-\\infty)=0$ Therefore, we use this function because the logistic function ranges between 0 and 1 and is relatively simple among smooth functions\nBut how does logistic regression find the best estimators for making predictions? Here is the process 1. Target We suppose that: $$ f(X) = P(Y=1 \\mid X) = \\frac{1}{1+e^{-(W^TX)}} $$ and we should know that:\n$$ P(Y\\mid X) = f(X)　\\text{if　} Y=1 $$\n$$ P(Y\\mid X) = 1 - f(X)　\\text{if　} Y=0 $$\n2. How to Logistic regression uses the likelihood function to estimate parameters, allowing the model to make more precise predictions. So we define likelihood function: $$ L(W, b) = \\Pi^n_{i=1}P\\left(y_i \\mid x_i \\right) $$\n3. Mathematical Then we plug $P(Y\\mid X)$ into the formula, so we have: $$ L(W, b) = \\Pi^n_{i=1}\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)^{y_i}\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)^{1- y_i} $$ and we take the log of likelihood function(log-likelihood): $$ lnL(W, b) = \\Sigma^n_{i=1}\\left[y_iln\\left(\\frac{1}{1+e^{-(W^TX)}}\\right)\\right] + (1- y_i)ln\\left(1-\\frac{1}{1+e^{-(W^TX)}}\\right)$$\nthen we use calculus and gradient descent to find the optimal parameter W. Finally, we ensure that the likelihood function reaches its maximum, which corresponds to the MLE solution.\nIn my opinion It is easy to see that using linear regression for predicting or classifying binary classes can be highly influenced by outliers.\nA small change in the data can cause a data point to switch from Class 1 to Class 2 unpredictably, which is unreasonable. To address this issue and improve the regression model for binary classification, we use a logistic function that better fits the binary class data.\n🔧 Modeling with sklearn.LogisticRegression import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt coloumns_name = [\u0026#39;Length\u0026#39;, \u0026#39;Left\u0026#39;, \u0026#39;Right\u0026#39;, \u0026#39;Bottom\u0026#39;, \u0026#39;Top\u0026#39;, \u0026#39;Diagonal\u0026#39;] data = pd.read_csv(\u0026#39;bank2.dat\u0026#39;, delim_whitespace=True, header=None, names=coloumns_name) data.head() -------------------------------------------------- Length Left Right Bottom Top Diagonal Class 0 214.8 131.0 131.1 9.0 9.7 141.0 Genuine 1 214.6 129.7 129.7 8.1 9.5 141.7 Genuine 2 214.8 129.7 129.7 8.7 9.6 142.2 Genuine 3 214.8 129.7 129.6 7.5 10.4 142.0 Genuine 4 215.0 129.6 129.7 10.4 7.7 141.8 Genuine data.info() -------------------------------------------------- \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Length 200 non-null float64 1 Left 200 non-null float64 2 Right 200 non-null float64 3 Bottom 200 non-null float64 4 Top 200 non-null float64 5 Diagonal 200 non-null float64 6 Class 200 non-null object dtypes: float64(6), object(1) memory usage: 11.1+ KB data.isna().sum() -------------------------------------------------- Length 0 Left 0 Right 0 Bottom 0 Top 0 Diagonal 0 Class 0 dtype: int64 data.describe().T -------------------------------------------------- count mean std min 25% 50% 75% max Length 200.0 214.8960 0.376554 213.8 214.6 214.90 215.100 216.3 Left 200.0 130.1215 0.361026 129.0 129.9 130.20 130.400 131.0 Right 200.0 129.9565 0.404072 129.0 129.7 130.00 130.225 131.1 Bottom 200.0 9.4175 1.444603 7.2 8.2 9.10 10.600 12.7 Top 200.0 10.6505 0.802947 7.7 10.1 10.60 11.200 12.3 Diagonal 200.0 140.4835 1.152266 137.8 139.5 140.45 141.500 142.4 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, accuracy_score, classification_report X = data.drop(columns=[\u0026#39;Class\u0026#39;]) y = data[\u0026#39;Class\u0026#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) lr = LogisticRegression(random_state=42, penalty=None) model = lr.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) y_proba = model.predict_proba(X_test_scaled) print(confusion_matrix(y_test, y_pred)) print(accuracy_score(y_test, y_pred)) -------------------------------------------------- [[19 0] [ 1 20]] 0.975 print(\u0026#34;\\nModel Accuracy:\u0026#34;, model.score(X_test_scaled, y_test)) print(classification_report(y_test, y_pred)) Model Accuracy: 0.975 precision recall f1-score support Counterfeit 0.95 1.00 0.97 19 Genuine 1.00 0.95 0.98 21 accuracy 0.97 40 macro avg 0.97 0.98 0.97 40 weighted avg 0.98 0.97 0.98 40 🔨 Modleing with statsmodels.Logit note:\n因為使用該模型存在beta不收斂的問題\n所以在fit的部分選擇使用fit_regularized\n其中method設定加入l1懲罰, alpha(l1的權重)設0.01\nsummay才會正常跑出數值\nconstant = sm.add_constant(X) model = sm.Logit(y, constant) result = model.fit_regularized(method=\u0026#39;l1\u0026#39;, alpha=0.01) print(result.summary()) -------------------------------------------------- Optimization terminated successfully (Exit mode 0) Current function value: 0.002174041962487562 Iterations: 131 Function evaluations: 136 Gradient evaluations: 131 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 200 Model: Logit Df Residuals: 193 Method: MLE Df Model: 6 Date: Thu, 20 Mar 2025 Pseudo R-squ.: 0.9994 Time: 16:39:59 Log-Likelihood: -0.083416 converged: True LL-Null: -138.63 Covariance Type: nonrobust LLR p-value: 6.587e-57 ============================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 7.851e-18 1.11e+04 7.07e-22 1.000 -2.18e+04 2.18e+04 Length -4.8724 46.740 -0.104 0.917 -96.481 86.737 Left 6.129e-16 83.355 7.35e-18 1.000 -163.372 163.372 Right -6.8e-16 80.137 -8.49e-18 1.000 -157.066 157.066 Bottom -11.9135 14.936 -0.798 0.425 -41.187 17.360 Top -9.3913 15.731 -0.597 0.551 -40.224 21.441 Diagonal 8.9620 10.880 0.824 0.410 -12.362 30.286 ============================================================================== Possibly complete quasi-separation: A fraction 0.95 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 7 parameters Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers warnings.warn(message, ConvergenceWarning) c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == \u0026#39;size\u0026#39; will still work. warnings.warn(msg, ConvergenceWarning) ","permalink":"http://localhost:1313/posts/logisticregression/","summary":"\u003ch4 id=\"這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd\"\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/h4\u003e\n\u003cp\u003eLogistic Function (aka logit, MaxEnt) classifier, which means that it is also known as logit regression,\nmaximum-entropy classification(MaxEnt) or the log-linear classifier.\u003cbr\u003e\nIn this model, the probabilities from the outcome of predictions is using a logistic function.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"and-what-is-logistic-function\"\u003eAnd what is logistic function?\u003c/h3\u003e\n\u003ch3 id=\"let-talk-about-it\"\u003eLet talk about it.\u003c/h3\u003e\n\u003cp\u003eHere comes from \u003cstrong\u003eWikipedia\u003c/strong\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA logistic function or a logistic curve is a commond S-shaped curve (\u003cstrong\u003esigmoid curve\u003c/strong\u003e) with the equation:\n$$ f(x) = \\frac{L}{1+e^{-k(x-x_o)}}$$\nwhere:\u003c/p\u003e","title":"Logistic Regression Learning"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD AdaBoosting, a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.\n","permalink":"http://localhost:1313/posts/adaboost/","summary":"\u003ch4 id=\"這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd\"\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/h4\u003e\n\u003cp\u003eAdaBoosting, a popluar boosting algorithm, introduced in 1995 by Freund and Schapire.\u003c/p\u003e","title":"AdaBoost Learning"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 隨機森林基本概要\n由多棵決策樹聚集而成的森林\n方法:\n從原始資料中以取後放回的方式抽取資料，建立每棵決策樹的訓練資料(training datasets)\n因此有些樣本會被重複選中，這樣的抽樣法又稱為Boostraping(拔靴法)\n但是當原始資料的數據龐大時，會發現在抽樣完畢後，有些樣本並沒有被抽取到\n而這些樣本就被稱為Out of Bag(OOB)資料(袋外)\n除了上述訓練資料是被重複抽取之外，特徵也是如此\n隨機森林並不會將所有特徵一起考慮，而是會隨機抽取特徵(可設定參數max_features)\n進行每棵樹的訓練，以上述兩種方式來達到每棵樹近乎獨立的情況，目的是降低每棵樹之間的高度相關性，\n優點是提高模型的泛化能力，防止過擬合(Overfitting)的情況發生、增進預測穩定性與準確度\n演算法: 隨機森林的演算法與決策樹的演算法的核心概念是一樣的，差別只是在建立樹的方法不同而已(如上述)\n意即當應用在分類問題時，則採用吉尼不純度(Gini Impurity)或是鏑(Entropy)演算法作為分類依據\n當應用在迴歸問題時，通常採用最小平方誤差(MSE)(其他還有卜氏Possion)\n","permalink":"http://localhost:1313/posts/randomforest/","summary":"\u003ch4 id=\"這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd\"\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/h4\u003e\n\u003cp\u003e隨機森林基本概要\u003cbr\u003e\n由多棵決策樹聚集而成的森林\u003cbr\u003e\n方法:\u003cbr\u003e\n從原始資料中以取後放回的方式抽取資料，建立每棵決策樹的訓練資料(training datasets)\u003cbr\u003e\n因此有些樣本會被重複選中，這樣的抽樣法又稱為Boostraping(拔靴法)\u003cbr\u003e\n但是當原始資料的數據龐大時，會發現在抽樣完畢後，有些樣本並沒有被抽取到\u003cbr\u003e\n而這些樣本就被稱為Out of Bag(OOB)資料(袋外)\u003cbr\u003e\n除了上述訓練資料是被重複抽取之外，特徵也是如此\u003cbr\u003e\n隨機森林並不會將所有特徵一起考慮，而是會隨機抽取特徵(可設定參數max_features)\u003cbr\u003e\n進行每棵樹的訓練，以上述兩種方式來達到每棵樹近乎獨立的情況，目的是降低每棵樹之間的高度相關性，\u003cbr\u003e\n優點是提高模型的泛化能力，防止過擬合(Overfitting)的情況發生、增進預測穩定性與準確度\u003cbr\u003e\n演算法:\n隨機森林的演算法與決策樹的演算法的核心概念是一樣的，差別只是在建立樹的方法不同而已(如上述)\u003cbr\u003e\n意即當應用在分類問題時，則採用吉尼不純度(Gini Impurity)或是鏑(Entropy)演算法作為分類依據\u003cbr\u003e\n當應用在迴歸問題時，通常採用最小平方誤差(MSE)(其他還有卜氏Possion)\u003c/p\u003e","title":"RandomForest Learning"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD 這篇文章利用 Chat Gpt 翻譯成英文，邊唸邊打（純手打，無複製貼上），順便練英文 XD We can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically follow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula: $$ \\left[ \\lambda_\\alpha \\left( 1 - 1.96 \\sqrt{\\frac{2}{n-1}} \\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}} \\right) \\right] $$ where:\n$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue $n$ denotes the sample size. By caculating the 95% confidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain. This approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original information as possible.\nIn PCA, it\u0026rsquo;s typically assumed that variables are continuous and follow a normal distribution. However, the presence of outliers or extreme values can violate these assumptions, potentially affecting the analysis results. To moderate these effects, data trasformation can be considered to make the results independent of measurement scales and monotonic transformations. A commone method is to replace each observation with its rank within the variable. In rank transformation, Spearman\u0026rsquo;s rank corrlelation coefficient is often used to measure the monotonic relationship between two variables. The calculation formula is: $$ r_s\\left(j, j'\\right) = 1 - \\frac{6\\sum_{i=1}^n \\left(x_{ij} - x_{ij'}\\right)^2}{n(n^2-1)} $$ where:\n$r_s\\left(j, j^\u0026rsquo;\\right)$ denotes the Spearman correlation coefficient between variables $j$ and $j'$ $x_{ij}$ and $x_{ij^\u0026rsquo;}$ represent the ranks of the $i$-th observation in variables $j$ and $j'$ $n$ is the total number of observations Spearman rank transformation offers several keys advantages:\nReducing the impact of outliers Preserving the \u0026lsquo;relative relationships\u0026rsquo; between variables while ignoring data units Capturing non-linear relationships Relationship between PCA and clustering ayalysis PCA for dimensionality reduction enhances clustering effectiveness\nChallenge in high-dimensional data \u0026amp; Solution Through PCA\nClustering algorithms can be adversely affected by the \u0026lsquo;Curse of Dimensionality\u0026rsquo; when dealing with high-dimensional datasets, by applying PCA for dimensionality reduction, we can project data into a lower-dimentional space(e.g. 2D), facilitating more stable and interpretable clustering results.\nTo discover clusters of data points that share similar characteristics, common clustering techniques include:\nHierachical clustering: Generates a dendrogram to represent nested groupings of data points. K-means clustering: Partitions data points into k clusters based on proximity to cluster centroids. Applications of PCA and Clustering:\nMarket Segmentation: Classifying consumers based on purchasing behavior to tailor marketing strategies. Medical Data Analysis: Identifying patient subgroups to enhance personalized treatment plans. Socioeconomic Studies: Analyzing patterns of economic development across different urban areas. Gene Expression Analysis: Detecting groups of genes with similar expression profiles to understand biological processes. In PCA, the inclusion of weights can influence the calculation of means, covariances, and correlations. Unweighted analyses focus on describing the sample, whereas weighted analyses aim to infer characteristics of the overall population. Therefore, when assigning weights, it\u0026rsquo;s crucial to avoid excessive variability and consider them carefully to encure the stability and reliability of the estimates.\nWe need to express the response variable in terms of the original variables. Each principal component is written as a linear combination of the explanatory variables: $$y = b_o + b_1\\Psi_1 + \\cdots + b_p\\Psi_p = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\cdots $$ Selecting prinicpal components with eigenvalues significantly greater than 0 as new explanatory variables can enhance the model\u0026rsquo;s stability and interpretability. This approach ensures that each retained component accounts for a substantial protion of the data\u0026rsquo;s variance, leading to more reliable and meaningful results.\nWhen we lack a variable matrix X and only have an inter-individual distance matrix D, we can still perform PCA using inner product matrix transformation, similar to the concept of Multidimensional Scaling(MDS).\nIn what situations do we only have distance between individuals?\nIn many real-world scenarious, data is not presented as a clear variable matrix X but exits in the form of a distance matrix. Here are some examples:\n1. Similarity/Dissimilarity Matrices\n- Genetic Research: The similarity between DNA sequences can be converted into Euclidean or Jaccard distances.\n- Text Mining: The similarity between documents can be calculated using Cosine Similarity or Edit Distance.\n2. Social Network Analysis\n- The number of mutual friends can define a similarity matrix, which can then be converted into distances.\n- Message transmission time can represent the \u0026lsquo;distance\u0026rsquo; between individuals.\n3. Geospatial \u0026amp; Transportation Data\n- Urban Planning: Distances between cities can be used in PCA to help identify regional clusters.\n- Applications like Google Maps: Analyzes similarities between cities using actual route distances.\n4. Recommendation Systems\n- In e-commerce or music recommendation systems, user behavior can be measured by \u0026lsquo;distance\u0026rsquo;.\n- The more similar the products purchased by two users, the shorted the \u0026lsquo;distance\u0026rsquo; between them.\nWhen we have only the inter-individual matrix D, we can transform it into an inner product matrix W using the following formula: $$w_{il} = \\frac{1}{2} \\left( d^2_{i\\cdot} + d^2_{l\\cdot} - d^2_{\\cdot\\cdot} - d^2{(i, l)} \\right)$$ where:\n$d^2{(i, l)}$ represents the squared distance between individuals $i$ and $l$ $d^2_{i\\cdot}$ and $d^2_{l\\cdot}$ are the average squared distances of individuals $i$ and $l$ to all other individuals $d^2_{\\cdot\\cdot}$ is the overall average of these squared distances After obtaining the inner product matrix W, we can perform PCA by applying eigenvalue decomposition or SVD to W for dimensionality reduction.\nAnd here is the different between eigenvalue decomposition and SVD\nCondition Eigen Decomposition SVD Matrix Type Only for square matrices Can be applied to any matrix shape Applicable To Inner product matrix $W$, covariance matrix $C$ Original data matrix $X$ Data Size Best for $p \\ll n$ Best for $p \\gg n$ Results Obtains principal component directions( variable correlations ) Obtains both individual projections and principal components Computational Efficiency More computationally expensive( requires computing $C$ ) More efficient, suitable for high-dimensional data In practical applications, libraries like scikit-learn implement PCA using SVD, as it is more numerically stable and computaionally efficient.\nConditional PCA\nBy incorporating matrix $Z$ to control for certain variables, we can analyze the variability in the data using the residual matrix $E$. The matrix $Z$ represents grouping information, such as: controlling for time effects, eliminating the influence of income, analyzing results based on PCA, or grouping based on categories. The residual matrix $E$ allows us to assess the variability of variables after accounting for specific influencing factors( represented by matrix $Z$ ). The formula for the residual matrix $E$ is: $$ \\frac{1}{n} E^T E = \\frac{1}{n} \\left( X^TX - X^TZ(X^TX)^{-1}Z^TX \\right) = V(X|Z) $$ This method calculates the after removing the effects of conditional variables. The goal is to eliminate the influence of certain known factors and focus on unexplained variability. This approach is widely applied in fields such as finance, social sciences, bioinformatics, and time series analysis.\nRegardless of the utilized medthod for modeling the variables $X$, we always decompose the covariance matrix into two terms: one term explains the variables $Z$, whose effect we want to elimate; the other term expresses the remaining or residual variability. The conditional analysis involves analyzing this latter term $$ V = V_{explained} + V_{residual} $$\n","permalink":"http://localhost:1313/posts/pca/","summary":"\u003ch4 id=\"這是給自己的一份學習紀錄以免日子久了忘記這是甚麼理論xd\"\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/h4\u003e\n\u003ch4 id=\"這篇文章利用-chat-gpt-翻譯成英文邊唸邊打純手打無複製貼上順便練英文-xd\"\u003e這篇文章利用 Chat Gpt 翻譯成英文，邊唸邊打（純手打，無複製貼上），順便練英文 XD\u003c/h4\u003e\n\u003cp\u003eWe can assume that the data comes from a sample with a normal distribution, in this context, the eigenvalues asyptotically\nfollow a normal distribution. Therefore, we can estimate the 95% confidence interval for each eigenvalue using the following formula:\n$$\n\\left[\n\\lambda_\\alpha \\left(\n1 - 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right); \\lambda_\\alpha \\left(1 + 1.96 \\sqrt{\\frac{2}{n-1}}\n\\right)\n\\right]\n$$\nwhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda_\\alpha$ represents the $\\alpha$-th eigenvalue\u003c/li\u003e\n\u003cli\u003e$n$ denotes the sample size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy caculating the 95%\nconfidence intervals of the eigenvalue, we can assess their stability and determine the appropriate number of pricipal component axes to retain.\nThis approach aids in deciding how many principal components to keep in PCA to reduce data dimensionality while preserving as much of the original\ninformation as possible.\u003c/p\u003e","title":"Principal Component Analysis(PCA) Learning"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\nTokenizing by n-gram (n-gram 分詞) 將文檔的內容依照 n 個單詞作分類，例如：\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\n[2] The Bee is an insect that gathers honey\u0026quot;\n我們可以利用函式 tokenize_ngrams() 依照 n = 2 分類為\n[1] \u0026ldquo;the bank\u0026rdquo;、\u0026ldquo;bank is\u0026rdquo;、\u0026ldquo;is a\u0026rdquo;、\u0026ldquo;a place\u0026rdquo;、\u0026ldquo;place where\u0026rdquo;、\u0026ldquo;where you\u0026rdquo;、\u0026ldquo;you put\u0026rdquo;、\u0026ldquo;put your\u0026rdquo;、\u0026ldquo;your money\u0026rdquo;\n[2] \u0026ldquo;the bee\u0026rdquo;、\u0026ldquo;bee is\u0026rdquo;、\u0026ldquo;is an\u0026rdquo;、\u0026ldquo;an insect\u0026rdquo;、\u0026ldquo;insect that\u0026rdquo;、\u0026ldquo;that gathers\u0026rdquo;、\u0026ldquo;gathers honey\u0026rdquo; ","permalink":"http://localhost:1313/posts/textmining%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"tokenizing-by-n-gram-n-gram-分詞\"\u003eTokenizing by n-gram (n-gram 分詞)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e將文檔的內容依照 n 個單詞作分類，例如：\u003cbr\u003e\n[1] \u0026ldquo;The Bank is a place where you put your money\u0026rdquo;\u003cbr\u003e\n[2] The Bee is an insect that gathers honey\u0026quot;\u003cbr\u003e\n我們可以利用函式 tokenize_ngrams() 依照 n = 2 分類為\u003cbr\u003e\n[1] \u0026ldquo;the bank\u0026rdquo;、\u0026ldquo;bank is\u0026rdquo;、\u0026ldquo;is a\u0026rdquo;、\u0026ldquo;a place\u0026rdquo;、\u0026ldquo;place where\u0026rdquo;、\u0026ldquo;where you\u0026rdquo;、\u0026ldquo;you put\u0026rdquo;、\u0026ldquo;put your\u0026rdquo;、\u0026ldquo;your money\u0026rdquo;\u003cbr\u003e\n[2] \u0026ldquo;the bee\u0026rdquo;、\u0026ldquo;bee is\u0026rdquo;、\u0026ldquo;is an\u0026rdquo;、\u0026ldquo;an insect\u0026rdquo;、\u0026ldquo;insect that\u0026rdquo;、\u0026ldquo;that gathers\u0026rdquo;、\u0026ldquo;gathers honey\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter4"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\nAnalyzing word and document frequency: tf-idf Term Frequency(tf): How frequently a word occurs in a document\n詞頻: 單詞出現在文檔的頻率 Inverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\n逆文檔頻率: 文檔中出現愈多次的單詞，其權重愈低；反之則愈低。即衡量某詞在所有文檔中分佈的稀疏程度，是一種懲罰項 。 並定義為： $$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$ 其中分子$n_{documents}$是文檔總數，分母$n_{documents containing term}$是包含該詞的文檔總數， 若某單詞出現在文檔的次數少，表示分母小，其 IDF 大，重要性高，權重高；反之出現的次數多，表示分母大，其 IDF 小，重要性低，權重低。 取對數則是為了減少極端數值，使 IDF 分佈更加平滑。 tf-idf的目標是用於識別在單一文檔中有價值、但不常見的單詞（詞彙）\nZipf\u0026rsquo;s law ( 齊夫定律) 一種描述自然語言中單詞使用頻率分佈的統計規律，其宣稱單詞的頻率與其在文檔裡的頻率排名成反比，即：$$frequency \\propto \\frac{1}{rank} $$ 出現頻率第一名的單詞的次數會是出現頻率第二名的單詞的次數的兩倍，會是出現頻率第三名的單詞的次數的三倍\u0026hellip;依此類推，會是出現頻率第 N 名的單詞的次數的 N 倍 若將排名 x 與出現頻率 y 取對數，即 logx 、 logy ，若整個文檔的坐標點標出來接近一直線，則該文檔符合 Zipf\u0026rsquo;s law ","permalink":"http://localhost:1313/posts/textmining%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"\u003cp\u003e\u003cstrong\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"analyzing-word-and-document-frequency-tf-idf\"\u003eAnalyzing word and document frequency: tf-idf\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eTerm Frequency(tf): How frequently a word occurs in a document\u003cbr\u003e\n詞頻: 單詞出現在文檔的頻率\u003c/li\u003e\n\u003cli\u003eInverse Document Frequency(idf): use to decrease the weight for commonly used words and increase the weight for words that are not used very much in a collection of documents\u003cbr\u003e\n逆文檔頻率: 文檔中出現愈多次的單詞，其權重愈低；反之則愈低。即衡量某詞在所有文檔中分佈的稀疏程度，是一種懲罰項 。\n並定義為：\n$$idf(term) = ln\\left(\\frac{n_{documents}}{n_{documents containing term}}\\right)$$\n其中分子$n_{documents}$是文檔總數，分母$n_{documents containing term}$是包含該詞的文檔總數，\n若某單詞出現在文檔的次數少，表示分母小，其 IDF 大，重要性高，權重高；反之出現的次數多，表示分母大，其 IDF 小，重要性低，權重低。\n取對數則是為了減少極端數值，使 IDF 分佈更加平滑。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003etf-idf的目標是用於識別在單一文檔中有價值、但不常見的單詞（詞彙）\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"zipfs-law--齊夫定律\"\u003eZipf\u0026rsquo;s law ( 齊夫定律)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e一種描述自然語言中單詞使用頻率分佈的統計規律，其宣稱單詞的頻率與其在文檔裡的頻率排名成反比，即：$$frequency \\propto \\frac{1}{rank} $$\u003c/li\u003e\n\u003cli\u003e出現頻率第一名的單詞的次數會是出現頻率第二名的單詞的次數的兩倍，會是出現頻率第三名的單詞的次數的三倍\u0026hellip;依此類推，會是出現頻率第 N 名的單詞的次數的 N 倍\u003c/li\u003e\n\u003cli\u003e若將排名 x 與出現頻率 y 取對數，即 logx 、 logy ，若整個文檔的坐標點標出來接近一直線，則該文檔符合 Zipf\u0026rsquo;s law\u003c/li\u003e\n\u003c/ol\u003e","title":"Text Mining with R: Chapter3"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\nBayesian hierarchical modeling，譯為「貝氏分層建模」\n針對多個層級分析的一套統計方法 「Hierarchical modeling is used with information is available on several different levels of observational units」\n可以對多個不同層級的觀測單位的資訊進行分析，例如：\n\u0026ndash; 每個國家的每日感染病例的時間概況，單位是國家\n\u0026ndash; 多個油井產量的遞減曲線分析（油氣產量），單位是油藏區的油井\n引自維基百科\n公式理論 Bayes\u0026rsquo; theorem:\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\nusing the basic property of conditional probability, the posterior distribution will yield: $$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$ so, we can say that: $$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$ Hierarchical models:\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\n(1) Hyperparameters: parameters of prior distribution\n先驗分配的參數（超參數／超母數）\n(2) Hyperdisrtibution: distribution of hyperparameters\n超參數的分配 例如：我們需要建模學校的學生測驗成績\n第 $j$ 所學校的學生的測驗成績：$y_j $（真實數據） 第 $j$ 所學校的整體測驗平均成績：$\\theta_j $ 全體學校的群體分配超參數：$\\phi$ （描述學校之間的異質性，依照過往數據假設） 而模型分為三個層級\n第三層級（頂層） 超參數生成-處理全體的不確定性\n$$\\phi \\sim P(\\phi)$$ 用於定義整體的分佈，則我們假設超參數 $\\phi（\\mu）$ 來自先驗分配為： $$\\mu \\sim N(\\mu_0,\\sigma^2_0)$$ 表示全體群體的中心趨勢是如何分佈的，其參數 $\\mu_0,\\sigma^2_0$ 通常是來自於過去的歷史數據而訂， 在這裡的例子就是定義全體學校的測驗成績可能跟去過往的數據做假設， 例如我們假設整體的平均測驗成績 $\\mu_0 = 60 $，$\\sigma^2_0 = 5$\n第二層級（中間層） 參數生成-解釋數據的來源\n$$\\theta_j \\mid \\phi \\sim P(\\theta_j \\mid \\phi)$$ 這層的目的是考慮學校之間的異質性，並假設學校的平均測驗成績來自某個整體的分佈， 則我們假設每個學校的測驗平均成績來自常態分配，即$$\\theta_j \\mid \\phi \\sim N(\\mu,1)$$ 其中 $\\mu$ 是整體學校的總測驗平均，而 $\\theta_j$ 是每個學校的測驗平均成績\n第一層級（底層） 數據生成-還原真實數據\n$$y_i \\mid \\theta_j,\\sigma^2 \\sim P(y_i \\mid \\theta_j,\\sigma^2)$$\n可以知道真實數據 $y_i$ 並不是隨機產生，而是在該真實數據所在的整體平均值與變異數受影響的，例如這裡的例子， 我們可以假設每個學生的測驗成績 $y_i$ 來自常態分配，即$$y_i \\mid \\theta_j,\\sigma^2 \\sim N(\\theta_j,\\sigma^2)$$ 是由於學校的平均成績 $\\theta_j$ 和 學生之間的差異 $\\sigma^2$ 影響的\n通過Hierarchical models，我們可以同時估計學校的特徵（如 $\\theta_j$）和整體特徵（如 $\\phi$），並且能有效處理不同層次的不確定性\n","permalink":"http://localhost:1313/posts/%E8%B2%9D%E6%B0%8F%E5%88%86%E5%B1%A4%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e\u003cstrong\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBayesian hierarchical modeling，譯為「貝氏分層建模」\u003cbr\u003e\n針對多個層級分析的一套統計方法\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003e「Hierarchical modeling is used with information is available on \u003cstrong\u003eseveral different levels\u003c/strong\u003e of observational \u003cstrong\u003eunits\u003c/strong\u003e」\u003cbr\u003e\n可以對多個不同層級的觀測單位的資訊進行分析，例如：\u003cbr\u003e\n\u0026ndash; 每個國家的每日感染病例的時間概況，單位是國家\u003cbr\u003e\n\u0026ndash; 多個油井產量的遞減曲線分析（油氣產量），單位是油藏區的油井\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e　\u003cstrong\u003e引自維基百科\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"公式理論\"\u003e公式理論\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBayes\u0026rsquo; theorem:\u003cbr\u003e\nthe updated probability statements about $\\theta_j$, given the occurence of event $y$,\u003cbr\u003e\nusing the basic property of conditional probability, the posterior distribution will yield:\n$$P(\\theta \\mid y) = \\frac{P(\\theta, y)}{P(y)} = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)}$$\nso, we can say that:\n$$P(\\theta \\mid y) \\propto P(y \\mid \\theta)P(\\theta)$$\u003c/li\u003e\n\u003cli\u003eHierarchical models:\u003cbr\u003e\nBayesian hierarchical modeling makes use of two important concepts in deriving the posterior distribution namely:\u003cbr\u003e\n(1) \u003cstrong\u003eHyperparameters\u003c/strong\u003e: parameters of prior distribution\u003cbr\u003e\n　 先驗分配的參數（超參數／超母數）\u003cbr\u003e\n(2) \u003cstrong\u003eHyperdisrtibution\u003c/strong\u003e: distribution of hyperparameters\u003cbr\u003e\n　 超參數的分配\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e例如：我們需要建模學校的學生測驗成績\u003c/p\u003e","title":"Hirarchical bayesian modeling"},{"content":"這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\nExpectation-maximization algorithm\n又翻譯為「最大期望值演算法」 經過兩個步驟交替進行計算：\n第一步是計算期望值（E）：利用對隱藏變量的現有估計值，計算其最大概似估計值\n第二步是最大化（M）：最大化在E步上求得的最大概似值來計算參數的值 M步上找到的參數估計值被用於下一個E步計算中，這個過程不斷交替進行\n引自維基百科 ","permalink":"http://localhost:1313/posts/em%E6%BC%94%E7%AE%97%E6%B3%95/","summary":"\u003cp\u003e\u003cstrong\u003e這是給自己的一份學習紀錄，以免日子久了忘記這是甚麼理論XD\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExpectation-maximization algorithm\u003cbr\u003e\n又翻譯為「最大期望值演算法」\u003c/li\u003e\n\u003cli\u003e經過兩個步驟交替進行計算：\u003cbr\u003e\n第一步是計算期望值（E）：利用對隱藏變量的現有估計值，計算其最大概似估計值\u003cbr\u003e\n第二步是最大化（M）：最大化在E步上求得的最大概似值來計算參數的值\nM步上找到的參數估計值被用於下一個E步計算中，這個過程不斷交替進行\u003cbr\u003e\n\u003cstrong\u003e引自維基百科\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e","title":"Expectation maximization algorithm"},{"content":"這是給我自己的一份教學，以免日子久了忘記怎麼爬蟲，同時也是一篇學習紀錄\n擁有一個可以寫code的環境，網路上很多安裝環境的教學\n我是用anoconda的vs code寫code的\nimport requests as req\r# 向客戶端要求網址 from bs4 import BeautifulSoup as B\r# 索取網址內容 import pandas as pd\r# 最後將結果輸出為CSV檔\rimport time\r# 避免爬蟲時被抓到是爬蟲，所以移用這個延長每次爬蟲時間，假裝自己是人類\rimport random\r# 延遲隨機時間用的\rbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r# 我是用 **cocktail builder** 這個網站來爬蟲我要的酒單 header = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r# 起初在爬的時候有發現跑不出資料，所以新增header來假裝我是人類，網路上也有很多如何在瀏覽器找header的教學\rresp = req.get(builder_url, headers=header)\r# 建立抓取url的回應變數\rcocktail_name_list = []\r# 建立空清單，將抓取到的資料先放進來，以便最後做成dataframe\rif resp.status_code == 200:\r# 確定你的url是可以連線的\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# 建立爬蟲的頭頭，後面的html.parser是每次建立都要有，好像是用來解析html的功能\rfor cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r# 打開網頁按下F2，按下ctrl+shift+c進入選取網頁元素模式，點選任一調酒，會指引到該調酒的block\r# 你會發現所有的調酒名稱都在\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;這個標籤底下，所以我們利用find_all遍歷該標籤\rname = cocktail_name.text.strip()\r# 調酒名稱都在\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;的標籤的text內容中，strip()是去掉text前後多餘的空格\rif name:\r# 確定該標籤有內容才抓，沒有就不抓\rcocktail_name_list.append(name)\r# 抓到修飾後的text並放入剛剛建立的list\rtime.sleep(random.uniform(1,3))\r# 每次抓完一個就等待 1~3 秒\rcocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r# 將抓完後的清list建立成一個Dataframe，以Name當作該欄位的名稱\rcocktail_data = []\r# 這是最後的調酒名稱+該調酒的材料的空清單\rfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\rurls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r# 建立每個調酒的url，點進去隨便一個調酒，你會發現網址是https://www.theeducatedbarfly.com/xxx-xxx/\r# xxx是該調酒的名稱，只是我們剛剛的調酒名稱list有大寫而且中間是空格不是-，所以用replace將空格替換成-，且轉為小寫\rresp = req.get(urls, headers=header)\rif resp.status_code == 200:\rsoup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r# 查找所有具有指定 class 的 \u0026lt;span\u0026gt; 元素\ringredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\ringredient_name_list = []\rfor ingredient in ingredients:\r# 提取\u0026lt;a\u0026gt;標籤的文字內容\ringredient_name = ingredient.find(\u0026#39;a\u0026#39;)\rif ingredient_name: # 確保\u0026lt;a\u0026gt;存在\ringredient_name_list.append(ingredient_name.text.strip())\rcocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r# 最後就是將剛剛抓到的Name跟這個Inredients清單中每個素材加入剛剛建立的加入剛剛建立的cocktail_data清單中\rtime.sleep(random.uniform(1,3))\rcocktail_df = pd.DataFrame(cocktail_data)\r# 最後的最後將list轉為Dataframe並用pd.to_csv輸出成csv檔，結束這回合 以上是我提醒自己的爬蟲教學\n有任何疑問歡迎提出\n雖然我還沒有建立留言板就是了\n","permalink":"http://localhost:1313/posts/%E9%85%92%E8%AD%9C%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8/","summary":"\u003cp\u003e\u003cstrong\u003e這是給我自己的一份教學，以免日子久了忘記怎麼爬蟲，同時也是一篇學習紀錄\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003e擁有一個可以寫code的環境，網路上很多安裝環境的教學\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003e我是用anoconda的vs code寫code的\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests as req\r\n# 向客戶端要求網址  \r\nfrom bs4 import BeautifulSoup as B\r\n# 索取網址內容  \r\nimport pandas as pd\r\n# 最後將結果輸出為CSV檔\r\nimport time\r\n# 避免爬蟲時被抓到是爬蟲，所以移用這個延長每次爬蟲時間，假裝自己是人類\r\nimport random\r\n# 延遲隨機時間用的\r\nbuilder_url = \u0026#39;https://www.theeducatedbarfly.com/cocktail-builder/\u0026#39;\r\n# 我是用 **cocktail builder** 這個網站來爬蟲我要的酒單  \r\nheader = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\u0026#39;}\r\n# 起初在爬的時候有發現跑不出資料，所以新增header來假裝我是人類，網路上也有很多如何在瀏覽器找header的教學\r\nresp = req.get(builder_url, headers=header)\r\n# 建立抓取url的回應變數\r\ncocktail_name_list = []\r\n# 建立空清單，將抓取到的資料先放進來，以便最後做成dataframe\r\nif resp.status_code == 200:\r\n# 確定你的url是可以連線的\r\n    soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n    # 建立爬蟲的頭頭，後面的html.parser是每次建立都要有，好像是用來解析html的功能\r\n    for cocktail_name in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;):\r\n    # 打開網頁按下F2，按下ctrl+shift+c進入選取網頁元素模式，點選任一調酒，會指引到該調酒的block\r\n    # 你會發現所有的調酒名稱都在\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;這個標籤底下，所以我們利用find_all遍歷該標籤\r\n        name = cocktail_name.text.strip()\r\n        # 調酒名稱都在\u0026#39;div\u0026#39;, class_=\u0026#34;wpupg-item-title wpupg-block-text-bold\u0026#34;的標籤的text內容中，strip()是去掉text前後多餘的空格\r\n        if name:\r\n        # 確定該標籤有內容才抓，沒有就不抓\r\n            cocktail_name_list.append(name)\r\n            # 抓到修飾後的text並放入剛剛建立的list\r\n    time.sleep(random.uniform(1,3))\r\n    # 每次抓完一個就等待 1~3 秒\r\n\r\ncocktail_name_df = pd.DataFrame(cocktail_name_list, columns=[\u0026#39;Name\u0026#39;])\r\n# 將抓完後的清list建立成一個Dataframe，以Name當作該欄位的名稱\r\n\r\ncocktail_data = []\r\n# 這是最後的調酒名稱+該調酒的材料的空清單\r\n\r\nfor name in cocktail_name_df[\u0026#39;Name\u0026#39;]:\r\n    urls = f\u0026#39;https://www.theeducatedbarfly.com/{name.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;).lower()}/\u0026#39;\r\n    # 建立每個調酒的url，點進去隨便一個調酒，你會發現網址是https://www.theeducatedbarfly.com/xxx-xxx/\r\n    # xxx是該調酒的名稱，只是我們剛剛的調酒名稱list有大寫而且中間是空格不是-，所以用replace將空格替換成-，且轉為小寫\r\n    resp = req.get(urls, headers=header)\r\n    if resp.status_code == 200:\r\n        soup = B(resp.text, \u0026#39;html.parser\u0026#39;)\r\n        # 查找所有具有指定 class 的 \u0026lt;span\u0026gt; 元素\r\n        ingredients = soup.find_all(\u0026#39;span\u0026#39;, class_=\u0026#39;wprm-recipe-ingredient-name\u0026#39;)\r\n        ingredient_name_list = []\r\n        for ingredient in ingredients:\r\n        # 提取\u0026lt;a\u0026gt;標籤的文字內容\r\n            ingredient_name = ingredient.find(\u0026#39;a\u0026#39;)\r\n            if ingredient_name:  \r\n            # 確保\u0026lt;a\u0026gt;存在\r\n                ingredient_name_list.append(ingredient_name.text.strip())\r\n\r\n        cocktail_data.append({\u0026#39;Cocktail Name\u0026#39;: name, \u0026#39;Ingredients\u0026#39;: \u0026#34;, \u0026#34;.join(ingredient_name_list)})\r\n        # 最後就是將剛剛抓到的Name跟這個Inredients清單中每個素材加入剛剛建立的加入剛剛建立的cocktail_data清單中\r\n    time.sleep(random.uniform(1,3))\r\n\r\ncocktail_df = pd.DataFrame(cocktail_data)\r\n# 最後的最後將list轉為Dataframe並用pd.to_csv輸出成csv檔，結束這回合\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003e以上是我提醒自己的爬蟲教學\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003e有任何疑問歡迎提出\u003c/strong\u003e\u003cbr\u003e\n\u003cdel\u003e雖然我還沒有建立留言板就是了\u003c/del\u003e\u003c/p\u003e","title":"cocktail webcrawler"},{"content":"Assume $$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$ , for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1～n$\nNote that: $$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\n$\\therefore$ $$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$ In vector notation: $$ Y_i = x^T_i\\beta + \\varepsilon_i $$ where\n$ x_i=(1, X_1, \u0026hellip;, X_n)^T $ And for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have: $$ Y = X\\beta + \\varepsilon $$\nwhere\n$ X = \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix} $\n$ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} $,\n$ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; X_1 \\\\ 1 \u0026amp; X_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; X_n \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}+\\varepsilon $\n$ Y～N(X\\beta, \\sigma^2) $ given a joint p.d.f. for $Y$ given $X$ of the form: $$ f_y(y; \\beta, \\sigma^2)= \\frac{1}{\\sqrt 2\\pi\\sigma^2}e^{\\frac{(y-X\\beta)^T(y-X\\beta)}{-2\\sigma^2}} $$ consider the maximization for $\\beta$ indicates that: $$ min \\left[(y-X\\beta)^T(y-X\\beta)\\right] $$ and thus: $$ \\begin{aligned} (y - X\\beta)^T (y - X\\beta) \u0026amp;= (y^T - (X\\beta)^T)(y - X\\beta) \\\\ \u0026amp;= (y^T - \\beta^T X^T)(y - X\\beta) \\\\ \u0026amp;= y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta \\\\ \u0026amp;= y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \\\\ \\frac{\\partial}{\\partial\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) \u0026amp;= -2yT^X+2X^TX\\beta \\overset{\\text{Let}}{=}0 \\\\ X^TX\\beta \u0026amp;= y^TX \\end{aligned} $$ since $(X^TX)^{-1}$ exists\n$\\therefore$ $$ \\beta = (X^TX)^{-1}X^Ty $$\nNext time, we will talk about $\\beta_0$ and $\\beta_1$ ","permalink":"http://localhost:1313/posts/lseofbeta/","summary":"\u003ch3 id=\"assume\"\u003eAssume\u003c/h3\u003e\n\u003cp\u003e$$ Y_i = \\beta_o + \\beta_1X_i+\\varepsilon_i $$\n, for given $n$ observerd data $(x_i, Y_i)$, $\\forall i=1～n$\u003c/p\u003e\n\u003cp\u003eNote that:\n$$ Y_i \\mid X_i=x_i ~ N(\\beta_o+\\beta_1X_1, \\sigma^2) $$\u003cbr\u003e\n$\\therefore$\n$$ E_{Y \\mid X}[Y_i \\mid X_i =x_i]=\\beta_o+\\beta_1X_1$$\nIn vector notation:\n$$ Y_i = x^T_i\\beta + \\varepsilon_i $$\nwhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ x_i=(1, X_1, \u0026hellip;, X_n)^T $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd for $ Y = (Y_1, \u0026hellip;, Y_n)^T $, We have:\n$$\nY = X\\beta + \\varepsilon\n$$\u003c/p\u003e","title":"Least square estimator of β in linear regression"},{"content":"瞭解到HUGO有三種語法\n分別是：JSON、TOML、YAML\n因為TOML的內容格式類似PYTHON的，而我本人也比較習慣PYTHON的語法\n所以採用TOML的語法，並將全部的語法改為跟TOML的\n","permalink":"http://localhost:1313/posts/02/","summary":"\u003cp\u003e瞭解到HUGO有三種語法\u003c/p\u003e\n\u003cp\u003e分別是：JSON、TOML、YAML\u003c/p\u003e\n\u003cp\u003e因為TOML的內容格式類似PYTHON的，而我本人也比較習慣PYTHON的語法\u003c/p\u003e\n\u003cp\u003e所以採用TOML的語法，並將全部的語法改為跟TOML的\u003c/p\u003e","title":"My 2nd post"},{"content":"開學了!\n這是我的第一行 這是我的第二行 這是我的第三行 這是我的第四行 這是我的第五行 這個文字大小最多到第六行這樣的大小 這是斜體字\n這是粗體字\n這是斜體中的粗體\n這是不分行的數學語法 $a \\alpha b \\beta \\Sigma \\sigma $\n這是分行的數學語法 $$a \\alpha b \\beta \\Sigma \\sigma $$\n這是編號1號\n這是編號2號\n這是項目編號 這是第一次縮排的項目編號 這是第二次縮牌的項目編號 我不知道還有沒有第三次縮排的項目編號 看來第二次縮排以後都是一樣的項目編號 這是第一列第一行 這是第一列第二行 這是第二列第一行 這是第二列第二行 這是第三列第一行 這是第三列第二行 ","permalink":"http://localhost:1313/posts/01/","summary":"\u003cp\u003e開學了!\u003c/p\u003e\n\u003ch1 id=\"這是我的第一行\"\u003e這是我的第一行\u003c/h1\u003e\n\u003ch2 id=\"這是我的第二行\"\u003e這是我的第二行\u003c/h2\u003e\n\u003ch3 id=\"這是我的第三行\"\u003e這是我的第三行\u003c/h3\u003e\n\u003ch4 id=\"這是我的第四行\"\u003e這是我的第四行\u003c/h4\u003e\n\u003ch5 id=\"這是我的第五行\"\u003e這是我的第五行\u003c/h5\u003e\n\u003ch6 id=\"這個文字大小最多到第六行這樣的大小\"\u003e這個文字大小最多到第六行這樣的大小\u003c/h6\u003e\n\u003cp\u003e\u003cem\u003e這是斜體字\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e這是粗體字\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003e這是斜體中的粗體\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e這是不分行的數學語法 $a \\alpha b \\beta \\Sigma \\sigma $\u003c/p\u003e\n\u003cp\u003e這是分行的數學語法 $$a \\alpha b \\beta \\Sigma \\sigma $$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e這是編號1號\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e這是編號2號\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e這是項目編號\n\u003cul\u003e\n\u003cli\u003e這是第一次縮排的項目編號\n\u003cul\u003e\n\u003cli\u003e這是第二次縮牌的項目編號\n\u003cul\u003e\n\u003cli\u003e我不知道還有沒有第三次縮排的項目編號\n\u003cul\u003e\n\u003cli\u003e看來第二次縮排以後都是一樣的項目編號\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e這是第一列第一行\u003c/th\u003e\n          \u003cth\u003e這是第一列第二行\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e這是第二列第一行\u003c/td\u003e\n          \u003ctd\u003e這是第二列第二行\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e這是第三列第一行\u003c/td\u003e\n          \u003ctd\u003e這是第三列第二行\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"My 1st post"},{"content":"","permalink":"http://localhost:1313/posts/gradientboost/","summary":"","title":""},{"content":"","permalink":"http://localhost:1313/posts/suportvectormachine/","summary":"","title":""}]